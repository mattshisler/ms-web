---
title: "Brook's Lemma"
description: "An explanation of Brook's Lemma."
author:
  - name: Matthew Shisler
    affiliation: North Carloina State University - Department of Statistics
    affiliation-url: https://statistics.sciences.ncsu.edu/ 
date: "3/12/2023"
categories: [Stochastic Processes] # self-defined categories
draft: false 
format:
  html: 
    code-fold: true
---


```{r}
#| label: load-packages
#| output: false
#| code-summary: "Code: Load the packages"
```

Here we will explore Brook's Lemma and some of its implications and consequences.
Brook's lemma is particularly useful in the context of spatial statistics.

Consider a discrete Markov Chain $\{X_t, t \in T \}$ with finite state space.

Recall the *Markov Property*. The transition probabilities for $X_t$ depend only on the previous state $X_{t-1}$. One way to describe the Markov property in the context of a time-ordered evolving process is to say that, conditioned on the present, the past and future are independent. 
$$
\Pr(X_t = s_t|X_{t-1}=s_{t-1},\dots,X_0 = s_0) = \Pr(X_t = s_t|X_{t-1}=s_{t-1})
$$
where $s_t$ is the state at stage $t$.

In this setting, we can factor the joint probability mass/density function, denoted $p(\,\cdot\,)$, of $t$ random variables, using the definition of conditional probability and the Markov Property. Given an initial state $X_0$, 

$$
\begin{align*}
p(X_t,X_{t-1},\dots,X_1|X_0) &= p(X_{t}|X_{t-1},\dots,X_1,X_0) \cdot p(X_{t-1},\dots,X_1|X_0)\\
               &= p(X_{t}|X_{t-1},\dots,X_1,X_0)\cdot p(X_{t-1}|X_{t-2},\dots,X_1,X_0)\cdot p(X_{t-2},\dots,X_1|X_0)\\
               &\quad \vdots\\
               &= p(X_{t}|X_{t-1},\dots,X_1,X_0)\cdot p(X_{t-1}|X_{t-2},\dots,X_1,X_0)\cdot \dots \cdot p(X_1|X_0)\\
               &= p(X_{t}|X_{t-1})\cdot p(X_{t-1}|X_{t-2})\cdot \dots \cdot p(X_1|X_0)\\
               &= \prod_{j=1}^t p(X_j|X_{j-1})
\end{align*}
$$
Above, in the first several equalities we apply the definition of conditional probability to write a $k$-variate joint probability density as the product of a univariate conditional and $(k-1)$-variate (joint) marginal probability densities. In the penultimate line, we apply the Markov Property to conclude that each conditional probability depends only on the immediately preceding variate in the Markov chain. The last line puts the expression into product notation.

This approach works in the case of a "natural ordering" or "favored direction" of the Markov Chain. Commonly an ordering is implied by the idea that the stochastic process is evolving over time. The process starts at an initial state $X_0$, then $X_1$ follows $X_0$, $X_2$ follows $X_1$, ..., $X_t$ follows $X_{t-1}$, and so on. Under the Markov property it is not useful to specify a model through conditional probabilities of the form $P(X_{t-1}|X_t)$ (past event conditioned on future event) because by time $t$ the past event at time $t-1$ will have already been "realized" in the stochastic process. That is the state at time $t$ has no influence on the state at time $t-1$ in the system.

Note: conditional probabilities of the form $P(X_{t-1}|X_t)$ might be interesting from the perspective of an observer who perceives the state of the system at time $t$ and wishes to characterize the probability of the previous state.


But this natural ordering is not present in all stochastic processes. The most basic example might be the case where
$$
\Pr(X_t = s_t|X_{t-1}=s_{t-1},\dots,X_0 = s_0) = \Pr(X_t = s_t|X_{t-1}=s_{t-1})
$$







