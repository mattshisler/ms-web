[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matthew Shisler",
    "section": "",
    "text": "Matthew Shisler is a statistics Ph.D. student at North Carolina State University in Raleigh, NC.\nHe is a military officer serving as an Operations Research Analyst for the U.S. Air Force and an Admissions Liaison Officer for the U.S. Air Force Academy."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Matthew Shisler",
    "section": "Education",
    "text": "Education\n North Carolina State University | Raleigh, NC\nPh.D. in Statistics | in progress\n Purdue University | West Lafayette, IN\nM.S. in Industrial Engineering | 2019\n U.S. Air Force Academy | CO Springs, CO\nB.S. in Applied Mathematics | 2017"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Matthew Shisler",
    "section": "Experience",
    "text": "Experience\nUnited States Air Force | various locations\nOperations Research Analyst | 2017 - present\nUnited States Air Force | various locations\nU.S. Air Force Academy Admissions Officer | 2021 - present"
  },
  {
    "objectID": "learn/2023-02-03-Basics of Timeseries AR1 Models/index.html",
    "href": "learn/2023-02-03-Basics of Timeseries AR1 Models/index.html",
    "title": "Basics of Timeseries AR1 Models",
    "section": "",
    "text": "This will be a short description of the basic auto-regressive lag-1 model in time series analysis.\nConsider a simple situation where a researcher makes \\(n\\) measurements \\(y_1,\\dots,y_n\\), on some scientific object of interest and further suppose that we wish to model these observations as independent and identically distributed Gaussian random variables \\(Y_i\\) with unknown mean \\(\\mu\\) and variance \\(\\sigma^2\\),\n\\[\nY_i \\sim \\text{N}(\\mu, \\sigma^2)\n\\]\nEstimating \\(\\mu\\) and \\(\\sigma^\\) is straightforward enough by maximizing the likelihood and applying the usual bias correction:\n\\[\n\\widehat\\mu = \\frac{1}{n}\\sum_{i=1}^n y_i \\quad \\quad \\quad \\widehat\\sigma^2 = \\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\widehat\\mu)^2\n\\]\nIn reality these measurements were most likely collected sequentially and indexed by time \\(t\\), \\(Y_t\\), \\(t = 1, \\dots, n\\). Depending on the nature of the scientific object being measured it is not unreasonable to suspect that a measurement made at time \\(t\\) may be correlated with a measurement made at time \\(t-1\\) and it would be wise for the scientific researcher to account for this fact in their statistical analysis.\nTo illustrate the issue let’s consider an example where we simulate some iid data and some correlated data.\n\n\nCode\nn  <- 1000\nmu <- 0\nsd <- 1\n\ny.iid <- rnorm(n, mu, sd)\n\nplot(y.iid)\n\n\n\n\n\nCode\nplot(density(y.iid))\n\n\n\n\n\n\n\nCode\ny.ts <- arima.sim(list(ar=0.99),n=n)\n\nplot(y.ts)\n\n\n\n\n\nCode\nplot(density(y.ts))"
  },
  {
    "objectID": "learn/2023-02-03-Markov-Chain-Sim/index.html",
    "href": "learn/2023-02-03-Markov-Chain-Sim/index.html",
    "title": "ST746 - Markov Chain Sim",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(viridis)\n\n\nWe want to simulate a simple Markov Chain. The state space is\n\\[\nS = \\{A,B,C\\}\n\\]\nThe transition matrix is\n\\[\nP =\n\\begin{pmatrix}\n. & 0.5 & 0.5\\\\\n0.75 & . & 0.25\\\\\n0.75 & . & 0.25\n\\end{pmatrix}\n\\]\nLet \\(X_n\\) be the state of the chain at time \\(n\\). Starting with \\(X_0 = A\\), we will simulate the chain out to \\(X_6\\).\nLet \\(N_B\\) be the number of visits to state \\(B\\) and \\(N_C\\) be the number of visits to state \\(C\\). We would like to approximate the probability of visiting states \\(B\\) and \\(C\\) an equal number of times in the first six steps.\nWith a little work, we can show that it is not possible for \\(N_B = N_C = 0, 1\\). Also, \\(P(N_B = 3, N_B = 3 | X_0 = A) = P(X_1 \\neq A, \\dots, X_6 \\neq A| X_0 = A)\\). The tricky calculation is \\(P(N_B = 2, N_B = 2 | X_0 = A)\\) and that is the motivation for this small simulation.\n\n\nCode: define the simulation function\nmc.sim <- function(P, init.state = 1, num.iters = 50){\n  \n  num.states <- nrow(P)\n  states     <- numeric(num.iters + 1)\n  states[1]  <- 1\n  \n  for (n in 2:(num.iters+1)){\n    \n    p <- P[states[n-1],]\n    states[n] <- which(rmultinom(1,1,p) == 1)\n    \n  }\n  \n  return(chain = states[2:(num.iters+1)])\n  \n}\n\n\nTest the simulation. We should not see any runs of 2 since there is no probability to stay in the same state. We should not see the first state being 1, since we know we must immediately leave state \\(A\\).\n\n\nCode\nP <- matrix(c(0, 0.5, 0.5,\n              0.75,0,0.25,\n              0.75,0.25,0), byrow = T, nrow = 3)\n\nmc.sim(P, init.state = 1, num.iters = 6)\n\n\n[1] 3 1 2 1 3 1\n\n\nThis looks good. We want to run this simulation many times for the first six steps of the chain.\n\n\nCode\nnum.reps <- 10\n\nresults <- matrix(unlist(lapply(1:num.reps,  \n                                function(x) mc.sim(P, \n                                                   init.state = x, \n                                                   num.iters = 6) )), \n                  byrow = T, \n                  nrow = num.reps)\n\n\nHere are the results for 10 replicates:\n\n\nCode\nresults\n\n\n      [,1] [,2] [,3] [,4] [,5] [,6]\n [1,]    2    1    2    1    2    1\n [2,]    3    1    2    1    3    1\n [3,]    2    1    3    1    3    1\n [4,]    2    3    1    2    1    2\n [5,]    3    1    3    1    2    1\n [6,]    3    1    2    1    2    3\n [7,]    2    1    2    1    3    1\n [8,]    2    1    2    3    1    2\n [9,]    3    1    3    1    3    1\n[10,]    2    1    3    2    1    2\n\n\nNext, we want to count each time a row has an equal number of \\(2\\)s and \\(3\\)s corresponding to the number of times we visit states \\(A\\) and \\(B\\) an equal number of times.\n\n\nCode: a function to check for equal visits\ncheck.equal.visit <- function(x, num.times = 2){\n  \n  return(length(which(x==2)) == num.times & length(which(x==3)) == num.times)\n\n}\n\ncheck.equal.visit(results[3,], num.times = 2)\n\n\n[1] FALSE\n\n\nApply this function to the rows of the results matrix.\n\n\nCode\nequal2 <- apply(results, MARGIN = 1, function(x) check.equal.visit(x, num.times = 2))\n\n\nSum this vector to get the number of times the equal visits occured, the dived by the number of simulation replicates to estimate the probability of an equal number of visits.\n\n\nCode\nsum(equal2)/num.reps\n\n\n[1] 0.1\n\n\nRepeat the above, but for a large number of replicates.\n\n\nCode\nnum.reps <- 10000\n\nresults <- matrix(unlist(lapply(1:num.reps,  \n                                function(x) mc.sim(P, \n                                                   init.state = x, \n                                                   num.iters = 6) )), \n                  byrow = T, \n                  nrow = num.reps)\n\nequal0 <- apply(results, MARGIN = 1, function(x) check.equal.visit(x, num.times = 0))\nequal1 <- apply(results, MARGIN = 1, function(x) check.equal.visit(x, num.times = 1))\nequal2 <- apply(results, MARGIN = 1, function(x) check.equal.visit(x, num.times = 2))\nequal3 <- apply(results, MARGIN = 1, function(x) check.equal.visit(x, num.times = 3))\n\nsum(equal0)/num.reps\n\n\n[1] 0\n\n\nCode\nsum(equal1)/num.reps\n\n\n[1] 0\n\n\nCode\nsum(equal2)/num.reps\n\n\n[1] 0.2737\n\n\nCode\nsum(equal3)/num.reps\n\n\n[1] 0.001\n\n\nThis agrees with our estimates, but this is not best way to run the simulation because I don’t need to store the results of each run every time. Let’s write something a little more compact.\nEach time I simulate a replicate I will immediately check for equal visits. Then I can throw away the old results instead of storing them.\n\n\nCode\nnum.reps <- 1000000\n\nnum.equal0 <- 0\nnum.equal1 <- 0\nnum.equal2 <- 0\nnum.equal3 <- 0\n\nfor (i in 1:num.reps){\n  \n  results <- mc.sim(P, init.state = 1, num.iters = 6)\n  \n  if(check.equal.visit(results, num.times = 0)){\n    num.equal0 = num.equal0 + 1 \n  }\n  \n  if(check.equal.visit(results, num.times = 1)){\n    num.equal1 = num.equal1 + 1 \n  }\n  \n  if(check.equal.visit(results, num.times = 2)){\n    num.equal2 = num.equal2 + 1 \n  }\n  \n  if(check.equal.visit(results, num.times = 3)){\n    num.equal3 = num.equal3 + 1 \n  }\n  \n  \n}"
  },
  {
    "objectID": "learn.html",
    "href": "learn.html",
    "title": "Learn",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n\n\nBasics of Timeseries AR1 Models\n\n\nMatthew Shisler\n\n\n\n\nMar 12, 2023\n\n\nBrook’s Lemma\n\n\nMatthew Shisler\n\n\n\n\nFeb 3, 2023\n\n\nST746 - Markov Chain Sim\n\n\nMatthew Shisler\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/2023-02-09-ST779-Computer-Assignment-1/index.html",
    "href": "notes/2023-02-09-ST779-Computer-Assignment-1/index.html",
    "title": "ST746 - Computer Assignment 1",
    "section": "",
    "text": "Create a vector of transition probabilities from a single state.\n\nnum.states <- 8\n\nt1 <- runif(num.states)\nt1 <- t1/sum(t1)\n\nround(t1,2)\n\n[1] 0.14 0.11 0.12 0.14 0.14 0.10 0.16 0.09"
  },
  {
    "objectID": "notes/2023-02-09-ST779-Computer-Assignment-1/index.html#step-2",
    "href": "notes/2023-02-09-ST779-Computer-Assignment-1/index.html#step-2",
    "title": "ST746 - Computer Assignment 1",
    "section": "Step 2",
    "text": "Step 2\nCreate the entire transition matrix. Note this actually throws out the first vector we created in step 1, but fulfills the overall spirit of the exercise.\n\nP <- matrix(runif(num.states^2), nrow = num.states)\nP <- P/rowSums(P)\nP\n\n           [,1]        [,2]       [,3]       [,4]       [,5]       [,6]\n[1,] 0.11193581 0.056136668 0.11340961 0.01519883 0.19553658 0.22847192\n[2,] 0.13984015 0.244974539 0.12347343 0.05509530 0.12433990 0.12067982\n[3,] 0.06813361 0.005319949 0.07164270 0.21595467 0.09547992 0.25930061\n[4,] 0.17810852 0.141767005 0.08484206 0.11815161 0.09618577 0.18169679\n[5,] 0.17614704 0.190249460 0.07006348 0.02640642 0.14993625 0.15367540\n[6,] 0.18368179 0.084953081 0.15945042 0.19174078 0.05588617 0.13680837\n[7,] 0.15467920 0.214675434 0.07707736 0.15218087 0.04480904 0.22185226\n[8,] 0.04161197 0.054615209 0.17295864 0.31943602 0.22625664 0.04477216\n           [,7]       [,8]\n[1,] 0.05657105 0.22273953\n[2,] 0.12963006 0.06196680\n[3,] 0.18329172 0.10087682\n[4,] 0.09793110 0.10131715\n[5,] 0.02105228 0.21246968\n[6,] 0.06084563 0.12663376\n[7,] 0.12370830 0.01101754\n[8,] 0.01514322 0.12520615"
  },
  {
    "objectID": "notes/2023-02-09-ST779-Computer-Assignment-1/index.html#step-3",
    "href": "notes/2023-02-09-ST779-Computer-Assignment-1/index.html#step-3",
    "title": "ST746 - Computer Assignment 1",
    "section": "Step 3",
    "text": "Step 3\nSimulate the Markov chain with initial state \\(X_0 = 1\\). Store the proportion of time the chain was in state \\(j\\), \\(j=1,\\dots,8\\).\n\ncurrent.state <- 1\nnum.steps <- 10000\nvisits1 <- rep(0, num.states)\n\nfor (i in 1:num.steps){\n\n  visits1[current.state] <- visits1[current.state] + 1\n  current.state <- sample(1:num.states, 1, prob = P[current.state,])\n    \n}\n\nround(visits1/sum(visits1),3)\n\n[1] 0.134 0.120 0.113 0.132 0.125 0.164 0.081 0.131"
  },
  {
    "objectID": "notes/2023-02-09-ST779-Computer-Assignment-1/index.html#step-4",
    "href": "notes/2023-02-09-ST779-Computer-Assignment-1/index.html#step-4",
    "title": "ST746 - Computer Assignment 1",
    "section": "Step 4",
    "text": "Step 4\nSimulate the Markov chain with initial states \\(X_0 = 2,\\dots,8\\). For each initial state, store the proportion of time the chain was in state \\(j\\), \\(j=1,\\dots,8\\).\n\nvisits2    <- matrix(0, nrow=num.states-1, ncol=num.states)\nnum.steps <- 10000\n\nfor (j in 1:(num.states-1)){\n  current.state <- j\n  \n  for (i in 1:num.steps){\n\n    visits2[j, current.state] <- visits2[j, current.state] + 1\n    current.state <- sample(1:num.states, 1, prob = P[current.state,])\n    \n  }\n  \n}\n\nround(visits2/rowSums(visits2),3)\n\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]\n[1,] 0.130 0.117 0.111 0.140 0.126 0.165 0.083 0.128\n[2,] 0.128 0.122 0.113 0.132 0.130 0.170 0.077 0.127\n[3,] 0.135 0.117 0.112 0.141 0.122 0.164 0.081 0.127\n[4,] 0.133 0.117 0.111 0.136 0.134 0.159 0.079 0.132\n[5,] 0.134 0.123 0.106 0.136 0.132 0.160 0.078 0.131\n[6,] 0.134 0.121 0.118 0.134 0.121 0.168 0.079 0.125\n[7,] 0.137 0.117 0.116 0.135 0.123 0.164 0.080 0.128"
  },
  {
    "objectID": "notes/2023-02-09-ST779-Computer-Assignment-1/index.html#step-5",
    "href": "notes/2023-02-09-ST779-Computer-Assignment-1/index.html#step-5",
    "title": "ST746 - Computer Assignment 1",
    "section": "Step 5",
    "text": "Step 5\nCompare the proportion vectors from questions 3 and 4.\n\nboxplot(visits2/rowSums(visits2))\npoints(visits1/sum(visits1), col = \"red\", cex = 8, pch = \".\")"
  },
  {
    "objectID": "notes/2023-02-09-ST779-Computer-Assignment-1/index.html#step-6",
    "href": "notes/2023-02-09-ST779-Computer-Assignment-1/index.html#step-6",
    "title": "ST746 - Computer Assignment 1",
    "section": "Step 6",
    "text": "Step 6\nCompute the \\(P^{100}\\) matrix multiplication. Are its rows approx equal? How do they compare the row vectors from step 5?\n\nP %^% 100\n\n          [,1]      [,2]      [,3]      [,4]     [,5]      [,6]       [,7]\n[1,] 0.1337951 0.1188422 0.1127099 0.1367886 0.125103 0.1645124 0.08169549\n[2,] 0.1337951 0.1188422 0.1127099 0.1367886 0.125103 0.1645124 0.08169549\n[3,] 0.1337951 0.1188422 0.1127099 0.1367886 0.125103 0.1645124 0.08169549\n[4,] 0.1337951 0.1188422 0.1127099 0.1367886 0.125103 0.1645124 0.08169549\n[5,] 0.1337951 0.1188422 0.1127099 0.1367886 0.125103 0.1645124 0.08169549\n[6,] 0.1337951 0.1188422 0.1127099 0.1367886 0.125103 0.1645124 0.08169549\n[7,] 0.1337951 0.1188422 0.1127099 0.1367886 0.125103 0.1645124 0.08169549\n[8,] 0.1337951 0.1188422 0.1127099 0.1367886 0.125103 0.1645124 0.08169549\n          [,8]\n[1,] 0.1265533\n[2,] 0.1265533\n[3,] 0.1265533\n[4,] 0.1265533\n[5,] 0.1265533\n[6,] 0.1265533\n[7,] 0.1265533\n[8,] 0.1265533"
  },
  {
    "objectID": "notes/2023-02-09-ST779-Computer-Assignment-1/index.html#step-7",
    "href": "notes/2023-02-09-ST779-Computer-Assignment-1/index.html#step-7",
    "title": "ST746 - Computer Assignment 1",
    "section": "Step 7",
    "text": "Step 7\nSolve the equation of stationarity \\(\\boldsymbol\\mu = \\boldsymbol\\mu \\mathbf{P}\\). Is the solution nearly equal to that in step 6?\n\ne    <- eigen(P)\nlvec <- MASS::ginv(e$vectors)\n\nas.numeric(lvec[1,]/sum(lvec[1,]))\n\n[1] 0.13379510 0.11884221 0.11270986 0.13678863 0.12510298 0.16451239 0.08169549\n[8] 0.12655333"
  },
  {
    "objectID": "notes/2023-02-09-ST779-Computer-Assignment-1/index.html#step-8",
    "href": "notes/2023-02-09-ST779-Computer-Assignment-1/index.html#step-8",
    "title": "ST746 - Computer Assignment 1",
    "section": "Step 8",
    "text": "Step 8\nWrite your own conclusions based on this simulation experiment and give an explanation of the phenomenon you observe."
  },
  {
    "objectID": "notes/YYYY-MM-DD-Template/index.html#concepts-from-set-theory",
    "href": "notes/YYYY-MM-DD-Template/index.html#concepts-from-set-theory",
    "title": "ST779 - Intro and Prelims",
    "section": "Concepts from set theory",
    "text": "Concepts from set theory\nThese notes take advantage of naive set theory in the sense that we will use natural language to describe sets and operations on sets."
  },
  {
    "objectID": "notes/YYYY-MM-DD-Template/index.html#relations",
    "href": "notes/YYYY-MM-DD-Template/index.html#relations",
    "title": "ST779 - Intro and Prelims",
    "section": "Relations",
    "text": "Relations\n\nContainment\n\n\n\n\n\n\nDefinition\n\n\n\n\\(A\\) is a subset of \\(B\\), written \\(A \\subset B\\) or \\(B \\supset B\\), iff \\(A \\cap B = A\\) or equivalently iff \\(\\omega \\in A \\implies \\omega \\in B\\).\n\n\nProperties:\n\n\\(A \\subset A\\)\n\\(A \\subset B\\) and \\(B \\subset C\\) \\(\\implies\\) \\(A \\subset C\\)\n\n\n\n\nEquality\n\n\n\n\n\n\nDefinition\n\n\n\nTwo sets \\(A\\) and \\(B\\) are equal, \\(A = B\\), iff \\(A \\subset B\\) and \\(B \\subset A\\). This means \\(\\omega \\in A\\) iff \\(\\omega \\in B\\).\n\n\nNotes:\n\nA proper subset is one which satisfies \\(A \\subset B\\) but \\(A \\ne B\\)."
  },
  {
    "objectID": "notes/YYYY-MM-DD-Template/index.html#set-operations",
    "href": "notes/YYYY-MM-DD-Template/index.html#set-operations",
    "title": "ST779 - Intro and Prelims",
    "section": "Set Operations",
    "text": "Set Operations\n\nComplementation\n\n\n\n\n\n\nDefinition\n\n\n\nThe complement of a subset \\(A \\subset \\Omega\\) is \\[\nA^c := \\{\\omega : \\omega \\notin A\\}\n\\]\n\n\nNotes:\n\n\n\nProperties:\n\n\\(A \\subset B\\) iff \\(B^c \\subset A^c\\)\n\n\n\n\nIntersection\n\n\n\n\n\n\nDefinition\n\n\n\nSuppose \\(T\\) is some index set and for each \\(t \\in T\\) we are given \\(A_t \\in \\Omega\\). The intersection of \\(A_t\\) is \\[\n\\bigcap_{t \\in T} A_t := \\{\\omega : \\omega \\in A_t, \\text{ for all } t \\in T \\}\n\\]\n\n\nNotes:\n\nWhen considering a small number of sets a different version of the “cap” symbol is used, \\(A \\cap B\\).\n\nProperties:\n\nIf \\(C \\subset A\\) and \\(C \\subset B\\), then \\(C \\subset (A \\cap B)\\)\n\n\n\n\nUnion\n\n\n\n\n\n\nDefinition\n\n\n\nSuppose \\(T\\) is some index set and for each \\(t \\in T\\) we are given \\(A_t \\in \\Omega\\). The union of \\(A_t\\) is \\[\n\\bigcup_{t \\in T} A_t := \\{\\omega : \\omega \\in A_t, \\text{ for some } t \\in T \\}\n\\]\n\n\nNotes:\n\nWhen considering a small number of sets a different version of the “cup” symbol is used, \\(A \\cup B\\).\n\nProperties:\n\nIf \\(A \\subset C\\) and \\(B \\subset C\\), then \\((A \\cup B) \\subset C\\)\n\n\n\n\nSet Difference\n\n\n\n\n\n\nDefinition\n\n\n\nGiven two sets \\(A\\) and \\(B\\), the elements that are in \\(A\\) but not in \\(B\\) is \\[\nA \\setminus B = A \\cap B^c\n\\]\n\n\nNotes:\n\nSometimes called “set minus”.\n\n\n\n\nSymmetric Difference\n\n\n\n\n\n\nDefinition\n\n\n\nGiven two sets \\(A\\) and \\(B\\), the set of elements that are in one but not in both is the symmetric difference \\[\nA \\triangle B = (A \\setminus B) \\cup (B \\setminus A)\n\\]"
  },
  {
    "objectID": "notes/YYYY-MM-DD-Template/index.html#common-terminologies",
    "href": "notes/YYYY-MM-DD-Template/index.html#common-terminologies",
    "title": "ST779 - Intro and Prelims",
    "section": "Common Terminologies",
    "text": "Common Terminologies"
  },
  {
    "objectID": "notes/YYYY-MM-DD-Template/index.html#arbitrary-set-operations",
    "href": "notes/YYYY-MM-DD-Template/index.html#arbitrary-set-operations",
    "title": "ST779 - Intro and Prelims",
    "section": "Arbitrary Set Operations",
    "text": "Arbitrary Set Operations"
  },
  {
    "objectID": "notes/YYYY-MM-DD-Template/index.html#countable-set-operations",
    "href": "notes/YYYY-MM-DD-Template/index.html#countable-set-operations",
    "title": "ST779 - Intro and Prelims",
    "section": "Countable Set Operations",
    "text": "Countable Set Operations"
  },
  {
    "objectID": "notes/YYYY-MM-DD-Template/index.html#disjointification",
    "href": "notes/YYYY-MM-DD-Template/index.html#disjointification",
    "title": "ST779 - Intro and Prelims",
    "section": "Disjointification",
    "text": "Disjointification\nLater we will see that the probability of the union of disjoint events is equal to the sum of their probabilities. When faced with an arbitrary union of sets which are not disjoint, a common strategy is to write an equivalent union in terms of disjoint sets. This is called disjointification.\nThe disjoint sets in this equivalent union are constructed from the arbitrary sets in the original union.\nAs a simple example consider two sets \\(A\\) and \\(B\\) not necessarily disjoint. Then consider the two sets \\(A\\) and \\(B \\setminus A\\). Their unions are equivalent \\(A \\cup B = A \\cup (B \\setminus A)\\), but now we know \\(A \\cap (B \\setminus A) = \\emptyset\\).\nFrom now on, we will use a square “cup” to quickly distinguish when a union is taken over disjoint sets.\n\\[\nA \\sqcup B\n\\]\nis the same as\n\\[\nA \\cup B \\text{ with } A \\cap B = \\emptyset.\n\\]\nDisjointification can generalized to any number of sets. Consider the sets \\(B, C\\), and \\(D\\), not necessarily disjoint. Their union can be written as a union of disjoint sets,\n\\[\nB \\cup C \\cup D = B \\sqcup (C\\setminus B) \\sqcup (D \\setminus (B \\cup C))\n\\]\nBut this can get cumbersome when the number of sets becomes large. Instead, arrange the sets into an arbitrary sequence. By this I mean assign some ordering to the sets. There doesn’t need to be anything special about the ordering. It only facilitates book-keeping. For example, we can assign labels to the sets \\(B, C\\) and \\(D\\), say \\(A_1 = B, A_2 = C, A_3 = D\\). Any other ordering is also acceptable. Now we can equivalently write,\n\\[\nB \\cup C \\cup D = \\bigcup_{i=1}^3A_i = \\bigsqcup_{i=1}^3\\left(A_i \\setminus \\left(\\bigcup_{j=1}^{i-1}A_j\\right)\\right)\n\\]\nThe result looks a little ugly, but it is powerful when considering the union of many sets."
  },
  {
    "objectID": "notes/YYYY-MM-DD-Template/index.html#limits-sup-and-inf",
    "href": "notes/YYYY-MM-DD-Template/index.html#limits-sup-and-inf",
    "title": "ST779 - Intro and Prelims",
    "section": "Limits (sup and inf)",
    "text": "Limits (sup and inf)"
  },
  {
    "objectID": "notes/YYYY-MM-DD-Template/index.html#cartesian-products",
    "href": "notes/YYYY-MM-DD-Template/index.html#cartesian-products",
    "title": "ST779 - Intro and Prelims",
    "section": "Cartesian Products",
    "text": "Cartesian Products"
  },
  {
    "objectID": "notes/YYYY-MM-DD-Template/index.html#inverse-map",
    "href": "notes/YYYY-MM-DD-Template/index.html#inverse-map",
    "title": "ST779 - Intro and Prelims",
    "section": "Inverse Map",
    "text": "Inverse Map"
  },
  {
    "objectID": "notes/YYYY-MM-DD-Template/index.html#section",
    "href": "notes/YYYY-MM-DD-Template/index.html#section",
    "title": "ST779 - Intro and Prelims",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "notes/YYYY-MM-DD-Template/index.html#indicator-function",
    "href": "notes/YYYY-MM-DD-Template/index.html#indicator-function",
    "title": "ST779 - Intro and Prelims",
    "section": "Indicator Function",
    "text": "Indicator Function"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nST746 - Computer Assignment 1\n\n\nMatthew Shisler\n\n\n\n\nST779 - Intro and Prelims\n\n\nMatthew Shisler\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/2023-02-02-Step-0-Linear-Regression-Gibbs-Sampling/index.html",
    "href": "research/2023-02-02-Step-0-Linear-Regression-Gibbs-Sampling/index.html",
    "title": "Step 0 - Bayesian Linear Regression with Gibbs Sampling",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tictoc)\n\n\nWe will start by verifying that our Gibbs sampler is actually working for a very simple case. First, we will simulate data from the model\n\\[\\begin{align*}\n\\mathbf{Y} \\sim \\text{Normal}_n(\\mathbf{X}\\mathbf{\\boldsymbol\\delta}, \\; \\sigma^2 \\mathbf{I}_n)\n\\end{align*}\\]\nwhere \\(\\mathbf{Y}\\) is an \\(n \\times 1\\) response vector, \\(\\mathbf{X}\\) is an \\(n \\times p\\) design matrix, and \\(\\boldsymbol\\delta\\) is a \\(p \\times 1\\) vector of parameters. The observations are mutually independent with constant variance, \\(\\text{Cov}(\\mathbf{Y}) = \\sigma^2 \\mathbf{I}_n\\).\nNext specify priors for \\(\\boldsymbol\\delta\\) and \\(\\sigma^2\\), chosen for conjugacy, \\[\\begin{align*}\n\\boldsymbol\\delta &\\sim \\text{Normal}_p(\\boldsymbol\\beta, \\; \\mathbf{\\Omega})\\\\\n\\sigma^2 &\\sim \\text{InvGamma}(a, \\; b)\n\\end{align*}\\]\nWhere \\(\\boldsymbol\\beta = \\boldsymbol0\\), \\(\\mathbf{\\Omega} = \\text{diag}((1000^2, 1000^2))\\), and \\(a = b = 0.1\\). The code alternates between sampling from the full conditionals, \\(p(\\boldsymbol\\delta|\\mathbf{Y},\\sigma^2)\\) and \\(p(\\sigma^2|\\mathbf{Y},\\boldsymbol\\delta)\\). In this case \\[\\begin{align*}\n  \\boldsymbol\\delta|\\mathbf{Y},\\sigma^2 &\\sim \\text{Normal}\\left(V^{-1}M, V^{-1}\\right)\\\\\n  V &= \\frac{1}{\\sigma^{2}}\\mathbf{X}^T\\mathbf{X} + \\mathbf{\\Omega}^{-1}\\\\\n  M &= \\frac{1}{\\sigma^{2}}\\mathbf{X}^T\\mathbf{Y} + \\mathbf{\\Omega}^{-1}\\boldsymbol\\beta\\\\\n  \\sigma^2|\\mathbf{Y},\\boldsymbol\\delta &\\sim \\text{InvGamma}\\left(\\frac{n}{2} + a, \\frac{1}{2}(\\mathbf{Y}-\\mathbf{X}\\boldsymbol\\delta)^T(\\mathbf{Y}-\\mathbf{X}\\boldsymbol\\delta) + b \\right)\n\\end{align*}\\]\nLet’s do a simple example with \\(n=100\\) and \\(p = 2\\). I will start the parameter index at \\(1\\), i.e. \\(\\delta_1\\) is the intercept.\n\n\nSimulate some data\nn  <- 100\np  <- 2    #including intercept\n\n# Generate some fake data (with intercept)\nX      <- matrix(c(rep(1,n), rnorm(n*(p-1))), nrow = n, ncol = p)\ndelta0 <- rnorm(p, mean = 0, sd = 3)\nY      <- matrix(rnorm(n, X%*%delta0, sd = 1))\n\n\n\n\nRun the Gibbs sampler\n# set-up\nniter <- 5000\nkeep_delta  <- matrix(NA, nrow = niter, ncol = p)\nkeep_sigma2 <- rep(NA,niter)\n\n# initial values (chosen to be intentionally poor)\ndelta  <- c(-10,10)\nsigma2 <- 10\nkeep_delta[1,] <- delta\nkeep_sigma2[1] <- sigma2\n\n# prior parameters\na  <- 0.1\nb  <- 0.1\nbeta <- rep(0,p)\nOmega_inv <- diag(rep(1e-06,p))\n\n# pre-computes\nXtY <- t(X)%*%Y\nXtX <- t(X)%*%X\nObeta <- Omega_inv%*%beta # not really necessary since beta = 0\n\n# Gibbs Loop\ntic()\nfor (i in 2:niter){\n  \n  # Sample from delta full conditional\n  M     <- (1/sigma2)*XtY + Obeta\n  V_inv <- solve((1/sigma2)*XtX + Omega_inv)\n  delta <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n    \n  # Sample from sigma2 full conditional\n  A      <- n/2 + a\n  B      <- (1/2)*sum((Y-X%*%delta)^2) + b\n  sigma2 <- 1/rgamma(1, A, B)\n  \n  # store the results\n  keep_delta[i,]  <- delta\n  keep_sigma2[i] <- sigma2\n  \n}\ntoc()\n\n\n0.28 sec elapsed\n\n\nLet’s inspect the resulting trace plots as a quick visual check. The true value of the respective parameter is represented by a red horizontal line. To make the convergence more obvious, I’ve included the poor initial guess.\n\n\nCode: Generate trace plots\nwin <- 1:niter\n\npar(mfrow = c(2,2))\n\nplot(win, keep_delta[win,1], type = \"l\",\n     ylab = expression(delta[1]),\n     xlab = \"iter\")\nabline(h = delta0[1], col = \"red\")\n\nplot(win, keep_delta[win,2], type = \"l\",\n     ylab = expression(delta[2]),\n     xlab = \"iter\")\nabline(h = delta0[2], col = \"red\")\n\nplot(win, keep_sigma2[win],   type = \"l\",\n     ylab = expression(sigma^2),\n     xlab = \"iter\")\nabline(h = 1, col = \"red\")\n\n\n\n\n\nWe can “burn” the first 100 iterations to see the behavior after convergence. Looks good to me!\n\n\nCode: Generate trace plots\nwin <- 100:niter\n\npar(mfrow = c(2,2))\n\nplot(win, keep_delta[win,1], type = \"l\",\n     ylab = expression(delta[1]),\n     xlab = \"iter\")\nabline(h = delta0[1], col = \"red\")\n\nplot(win, keep_delta[win,2], type = \"l\",\n     ylab = expression(delta[2]),\n     xlab = \"iter\")\nabline(h = delta0[2], col = \"red\")\n\nplot(win, keep_sigma2[win],   type = \"l\",\n     ylab = expression(sigma^2),\n     xlab = \"iter\")\nabline(h = 1, col = \"red\")"
  },
  {
    "objectID": "research/2023-02-02-Step-1-Hierarchical-Linear-Regression/index.html",
    "href": "research/2023-02-02-Step-1-Hierarchical-Linear-Regression/index.html",
    "title": "Step 1 - Bayesian Hierarchical Linear Regression",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tictoc)\nlibrary(Rfast)\n\n\nConsider an extension of the setting from Step 1 where \\(\\mathbf{Y}_i\\) is an \\(n_i \\times 1\\) response vector , \\(\\mathbf{X}_i\\) an \\(n_i \\times p\\) design matrix, and \\(\\boldsymbol\\delta_i\\) a \\(p \\times 1\\) parameter vector corresponding to subject \\(i = 1,\\dots,N\\) (where the subject will later be the year). I did not index year using “\\(t\\)” because later we will define \\(t_{ij}\\) to be the day of year \\(i\\) on which the \\(j\\)th measurement of \\(\\mathbf{Y}_i\\) was collected.\nThe parameters \\(\\boldsymbol\\delta_i\\) are drawn from a multivariate normal random effects distribution with mean \\(\\beta\\) and covariance matrix \\(\\mathbf{\\Omega}\\). The entries of \\(\\mathbf{Y}_i\\) are mutually independent with constant variance \\(\\sigma^2\\), \\(\\text{Cov}(\\mathbf{Y}_i) = \\sigma^2 \\mathbf{I}_n\\) for all \\(i\\). Further, \\(\\mathbf{Y}_1,\\dots,\\mathbf{Y}_N\\) are mutually independent.\n\\[\\begin{align*}\n\\mathbf{Y}_i &\\sim \\text{Normal}_{n_i}\\left(\\mathbf{X}_i\\boldsymbol\\delta_i, \\; \\sigma^2 \\mathbf{I}_{n_i}\\right)\\\\\n\\boldsymbol\\delta_i &\\sim \\text{Normal}_p\\left(\\boldsymbol\\beta, \\; \\mathbf{\\Omega}\\right)\n\\end{align*}\\]\nIn this case we will assume \\(\\mathbf{\\Omega}\\) is diagonal and let \\(\\omega_{kk}\\) be the \\(k\\)th diagonal element. Next specify priors, \\[\\begin{align*}\n\\boldsymbol\\beta &\\sim \\text{Normal}_p\\left(\\boldsymbol\\mu, \\; \\mathbf{\\Lambda}\\right)\\\\\n\\omega_{kk} &\\sim \\text{InvGamma}\\left(a_{k}, \\; b_{k} \\right)\\\\\n\\sigma^2 &\\sim \\text{InvGamma}\\left(a, \\; b\\right)\n\\end{align*}\\]\nNote, for simplicity there is no linear trend in the random effects distribution for \\(\\boldsymbol\\delta_i\\). Also, I’ve left its covariance to be diagonal, just to avoid the Inverse Wishart prior for now. This way we can update the diagonal elements of \\(\\mathbf{\\Omega}\\) individually.\nThe full conditionals in this model are. . .\n\\[\\begin{align*}\n\\boldsymbol\\delta_i|\\text{``rest\"} &\\sim \\text{Normal}_p(\\mathbf{V}_i^{-1}\\mathbf{M}_i, \\mathbf{V}_i^{-1})\\\\\n\\mathbf{V}_i &= \\frac{1}{\\sigma^2} \\mathbf{X}_i^T\\mathbf{X}_i + \\mathbf{\\Omega}^{-1}\\\\\n\\mathbf{M}_i &= \\frac{1}{\\sigma^2} \\mathbf{X}_i^T\\mathbf{Y}_i + \\mathbf{\\Omega}^{-1}\\boldsymbol\\beta\\\\\\\\\n\\boldsymbol\\beta|\\text{``rest\"} &\\sim \\text{Normal}_p(\\mathbf{V}_\\beta^{-1}\\mathbf{M}_\\beta, \\mathbf{V}_\\beta^{-1})\\\\\n\\mathbf{V}_\\beta &= N\\mathbf{\\Omega}^{-1} + \\mathbf{\\Lambda}^{-1}\\\\\n\\mathbf{M}_\\beta &= \\mathbf{\\Omega}^{-1}\\sum_{i=1}^N\\boldsymbol\\delta_i + \\mathbf{\\Lambda}^{-1}\\boldsymbol\\mu\\\\\\\\\n\\omega_{kk}|\\text{``rest\"} &\\sim \\text{InvGamma}(A_k,B_k)\\\\\nA_k &= N/2 + a_k\\\\\nB_k &= \\frac{1}{2}\\sum_{i=1}^N (\\delta_{ik} - \\beta_k)^2 + b_k\\\\\\\\\n\\sigma^2|\\text{\"rest\"} &\\sim \\text{InvGamma}(A,B)\\\\\nA &= \\frac{1}{2}\\sum_{i=1}^N n_i + a\\\\\nB &= \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^{n_i} (Y_{ij} - \\mathbf{X}_i\\boldsymbol\\delta_i)^2 + b\n\\end{align*}\\]\nSimulate some data from this model. In this case will we set \\(N = 100\\), \\(p = 2\\), and \\(n_i = n = 100\\). Then we will implement the Gibbs sampler.\n\n\nSimulate some data\nN <- 100\np <- 2\nn <- rep(100, N)\n\nbeta0  <- rnorm(p, mean = 0, sd = 5)\nOmega0 <- diag(c(2,1))\n\ndelta0  <- t(Rfast::rmvnorm(N, beta0, Omega0))\nsigma20 <- 1\n\nY <- list()\nX <- list()\nfor (i in 1:N){\n  X[[i]] <- matrix(c(rep(1,n[i]), rnorm(n[i]*(p-1))), nrow = n[i], ncol = p)\n  Y[[i]] <- matrix(rnorm(n[i], mean = X[[i]]%*%delta0[,i], sd = sqrt(sigma20)), ncol=1)\n}\n\n\n\n\nRun the Gibbs sampler\n# set-up\nniter <- 5000\nkeep_delta  <- array(NA, dim = c(p, N, niter))\nkeep_beta   <- matrix(NA, nrow = niter, ncol = p)\nkeep_Omega  <- matrix(NA, nrow = niter, ncol = p)\nkeep_sigma2 <- rep(NA, niter)\n\n# initial values\ndelta  <- matrix(0, nrow = p, ncol = N)\nbeta   <- rep(10, p)\nsigma2 <- 3\nOmega  <- diag(c(5,5))\nkeep_delta[,,1] <- delta\nkeep_beta[1,]   <- beta\nkeep_Omega[1,]  <- diag(Omega)\nkeep_sigma2[1]  <- sigma2\n\n\n# prior parameters\nmu    <- rep(0, p)\nLambda_inv <- diag(rep(1e-06,p))\na     <- 0.1\nb     <- 0.1\nAo    <- N/2 + a\nAs    <- sum(n)/2 + a\n\n# pre-computes\nXtX <- list()\nXtY <- list()\nfor (k in 1:N){\n  XtX[[k]] <- t(X[[k]])%*%X[[k]]\n  XtY[[k]] <- t(X[[k]])%*%Y[[k]]\n}\nLmu <- Lambda_inv%*%mu\n\ntic()\n# Gibbs Loop\nfor (i in 2:niter){\n  \n  Omega_inv <- diag(1/diag(Omega))\n  \n  # sample deltas\n  for (k in 1:N){\n    M         <- (1/sigma2)*XtY[[k]] + Omega_inv%*%beta\n    V_inv     <- chol2inv(chol((1/sigma2)*XtX[[k]] + Omega_inv))\n    delta[,k] <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n  }\n  \n  # sample beta\n  M     <- Omega_inv%*%rowSums(delta0) + Lmu\n  V_inv <- solve(N*Omega_inv + Lambda_inv)\n  beta  <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n  \n  # sample omegas\n  for (j in 1:p){\n    Bo <- sum((delta[j,] - beta[j])^2)/2 + b\n    Omega[j,j] <- 1/rgamma(1, Ao, Bo)\n  }\n  \n  # sample sigma2\n  SSE <- 0\n  for (k in 1:N){\n    SSE <- SSE + sum((Y[[k]] - X[[k]]%*%delta[,k])^2)\n  }\n  Bs <- SSE/2 + b\n  sigma2 <- 1/rgamma(1, As, Bs)\n  \n  # store everything\n  keep_delta[,,i] <- delta\n  keep_beta[i,]   <- beta\n  keep_Omega[i,]  <- diag(Omega)\n  keep_sigma2[i]  <- sigma2\n}\ntoc()\n\n\n24.1 sec elapsed\n\n\nThis sampler takes about 20 seconds to run on my machine. That seems slow relative to JAGS. I’m sure there are some computational tricks that I can employ. The way I am computing the overall SSE to update \\(\\sigma^2\\) seems particularly naive.\nInspect some trace plots. The true value of the respective parameter is represented by a red horizontal line. Again, I’ve included the poor initial guess to make the convergence more obvious.\n\n\nConstruct trace plots\nwin <- 1:niter\n\npar(mfrow = c(2,2))\n\nfor (k in 1:p){\n  plot(win, keep_beta[win,k], type = \"l\",\n       ylab = bquote(beta[.(k)]),\n       xlab = \"iter\")\n  abline(h = beta0[k], col = \"red\")\n}\n\nfor (k in 1:p){\n  plot(win, keep_Omega[win, k],   type = \"l\",\n       ylab = bquote(omega[.(k+10*k)]),\n       xlab = \"iter\")\n  abline(h = Omega[k,k], col = \"red\")\n}\n\n\n\n\n\nConstruct trace plots\n# par(mfrow = c(1,1))\n\nplot(win, keep_sigma2[win],   type = \"l\",\n     ylab = bquote(sigma^2),\n     xlab = \"iter\")\nabline(h = sigma20, col = \"red\")"
  },
  {
    "objectID": "research/2023-02-02-Step-2-Single Parent Design Matrix/index.html",
    "href": "research/2023-02-02-Step-2-Single Parent Design Matrix/index.html",
    "title": "Step 2 - One Parent Design Matrix",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tictoc)\nlibrary(Rfast)\n\n\nThe setting is the same as in step 1 only now we will generate a single “parent” design matrix, \\(\\mathbf{X}\\), which is \\(m \\times p\\) and \\(m \\ge n_i\\) for all \\(i\\). That is to say we can’t have more rows in a subject’s design matrix than the parent matrix. To implement this we will generate the parent design matrix, then sample \\(n_i\\) integers from the sequence \\(1,\\dots, m\\) and extract the corresponding rows of \\(\\mathbf{X}\\) to construct a subject’s design matrix, \\(\\widetilde{\\mathbf{X}}_i\\).\nThe parameters \\(\\boldsymbol\\delta_i\\) are drawn from a multivariate normal random effects distribution with mean \\(\\beta\\) and covariance \\(\\mathbf{\\Omega}\\). The entries of \\(\\mathbf{Y}_i\\) are mutually independent with constant variance \\(\\sigma^2\\), \\(\\text{Cov}(\\mathbf{Y}_i) = \\sigma^2 \\mathbf{I}_n\\) for all \\(i\\). Further, \\(\\mathbf{Y}_1,\\dots,\\mathbf{Y}_N\\) are mutually independent.\n\\[\\begin{align*}\n\\mathbf{Y}_i &\\sim \\text{Normal}_{n_i}\\left(\\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i, \\; \\sigma^2 \\mathbf{I}_{n_i}\\right)\\\\\n\\boldsymbol\\delta_i &\\sim \\text{Normal}_p\\left(\\boldsymbol\\beta, \\; \\mathbf{\\Omega}\\right)\n\\end{align*}\\]\nIn this case we will assume \\(\\mathbf{\\Omega}\\) is diagonal and let \\(\\omega_{kk}\\) be the \\(k\\)th diagonal element. Next specify priors, \\[\\begin{align*}\n\\boldsymbol\\beta &\\sim \\text{Normal}_p\\left(\\boldsymbol\\mu, \\; \\mathbf{\\Lambda}\\right)\\\\\n\\omega_{kk} &\\sim \\text{InvGamma}\\left(a_{k}, \\; b_{k} \\right)\\\\\n\\sigma^2 &\\sim \\text{InvGamma}\\left(a, \\; b\\right)\n\\end{align*}\\]\nNote, for simplicity there is no linear trend in the random effects distribution for \\(\\boldsymbol\\delta_i\\). Also, I’ve left its covariance to be diagonal, just to avoid the Inverse Wishart prior for now. This way we can update the diagonal elements of \\(\\mathbf{\\Omega}\\) individually.\nThe full conditionals in this model are. . .\n\\[\\begin{align*}\n\\boldsymbol\\delta_i|\\text{ rest} &\\sim \\text{Normal}_p(\\mathbf{V}_i^{-1}\\mathbf{M}_i, \\mathbf{V}_i^{-1})\\\\\n\\mathbf{V}_i &= \\frac{1}{\\sigma^2} \\widetilde{\\mathbf{X}}_i^T\\widetilde{\\mathbf{X}}_i + \\mathbf{\\Omega}^{-1}\\\\\n\\mathbf{M}_i &= \\frac{1}{\\sigma^2} \\widetilde{\\mathbf{X}}_i^T\\mathbf{Y}_i + \\mathbf{\\Omega}^{-1}\\boldsymbol\\beta\\\\\\\\\n\\boldsymbol\\beta|\\text{ rest} &\\sim \\text{Normal}_p(\\mathbf{V}_\\beta^{-1}\\mathbf{M}_\\beta, \\mathbf{V}_\\beta^{-1})\\\\\n\\mathbf{V}_\\beta &= N\\mathbf{\\Omega}^{-1} + \\mathbf{\\Lambda}^{-1}\\\\\n\\mathbf{M}_\\beta &= \\mathbf{\\Omega}^{-1}\\sum_{i=1}^N\\boldsymbol\\delta_i + \\mathbf{\\Lambda}^{-1}\\boldsymbol\\mu\\\\\\\\\n\\omega_{kk}|\\text{ rest} &\\sim \\text{InvGamma}(A_k,B_k)\\\\\nA_k &= N/2 + a_k\\\\\nB_k &= \\frac{1}{2}\\sum_{i=1}^N (\\delta_{ik} - \\beta_k)^2 + b_k\\\\\\\\\n\\sigma^2|\\text{ rest} &\\sim \\text{InvGamma}(A,B)\\\\\nA &= \\frac{1}{2}\\sum_{i=1}^N n_i + a\\\\\nB &= \\frac{1}{2}\\sum_{i=1}^N (\\mathbf{Y}_i - \\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i)^T(\\mathbf{Y}_i - \\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i) + b\n\\end{align*}\\]\nSimulate some data from this model. In this case will we set \\(N = 100\\), \\(p = 2\\), \\(n_i = n = 100\\), and \\(m = 400\\).\n\n\nSimulate the data\nm <- 400\nN <- 100\np <- 2\nn <- rep(100, N)\n\nXp <- matrix(c(rep(1,m), rnorm(m*(p-1), mean = 0, sd = 5)), nrow = m, ncol = p)\n\nbeta0  <- rnorm(p, mean = 0, sd = 5)\nOmega0 <- diag(c(2,1))\n\ndelta0  <- t(Rfast::rmvnorm(N, beta0, Omega0))\nsigma20 <- 1\n\nY <- list()\nX <- list()\nfor (i in 1:N){\n  subject_rows <- sample(1:m, n[i])\n  X[[i]] <- Xp[subject_rows,]\n  Y[[i]] <- matrix(rnorm(n[i], mean = X[[i]]%*%delta0[,i], sd = sqrt(sigma20)), ncol=1)\n}\n\n\nThe Gibbs sampler is identical to that found in Step 1.\n\n\nRun the Gibbs Sampler\n# set-up\nniter <- 5000\nkeep_delta  <- array(NA, dim = c(p, N, niter))\nkeep_beta   <- matrix(NA, nrow = niter, ncol = p)\nkeep_Omega  <- matrix(NA, nrow = niter, ncol = p)\nkeep_sigma2 <- rep(NA, niter)\n\n# initial values\ndelta  <- matrix(0, nrow = p, ncol = N)\nbeta   <- rep(10, p)\nsigma2 <- 3\nOmega  <- diag(c(5,5))\nkeep_delta[,,1] <- delta\nkeep_beta[1,]   <- beta\nkeep_Omega[1,]  <- diag(Omega)\nkeep_sigma2[1]  <- sigma2\n\n\n# prior parameters\nmu    <- rep(0, p)\nLambda_inv <- diag(rep(1e-06,p))\na     <- 0.1\nb     <- 0.1\nAo    <- N/2 + a\nAs    <- sum(n)/2 + a\n\n# pre-computes\nXtX <- list()\nXtY <- list()\nfor (k in 1:N){\n  XtX[[k]] <- t(X[[k]])%*%X[[k]]\n  XtY[[k]] <- t(X[[k]])%*%Y[[k]]\n}\nLmu <- Lambda_inv%*%mu\n\ntic()\n# Gibbs Loop\nfor (i in 2:niter){\n  \n  Omega_inv <- diag(1/diag(Omega))\n  \n  # sample deltas\n  for (k in 1:N){\n    M         <- (1/sigma2)*XtY[[k]] + Omega_inv%*%beta\n    V_inv     <- chol2inv(chol((1/sigma2)*XtX[[k]] + Omega_inv))\n    delta[,k] <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n  }\n  \n  # sample beta\n  M     <- Omega_inv%*%rowSums(delta0) + Lmu\n  V_inv <- solve(N*Omega_inv + Lambda_inv)\n  beta  <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n  \n  # sample omegas\n  for (j in 1:p){\n    Bo <- sum((delta[j,] - beta[j])^2)/2 + b\n    Omega[j,j] <- 1/rgamma(1, Ao, Bo)\n  }\n  \n  # sample sigma2\n  SSE <- 0\n  for (k in 1:N){\n    SSE <- SSE + sum((Y[[k]] - X[[k]]%*%delta[,k])^2)\n  }\n  Bs <- SSE/2 + b\n  sigma2 <- 1/rgamma(1, As, Bs)\n  \n  # store everything\n  keep_delta[,,i] <- delta\n  keep_beta[i,]   <- beta\n  keep_Omega[i,]  <- diag(Omega)\n  keep_sigma2[i]  <- sigma2\n}\ntoc()\n\n\n20.45 sec elapsed\n\n\n\n\nConstruct Trace Plots\nwin <- 1:niter\n\npar(mfrow = c(2,2))\n\nfor (k in 1:p){\n  plot(win, keep_beta[win,k], type = \"l\",\n       ylab = bquote(beta[.(k)]),\n       xlab = \"iter\")\n  abline(h = beta0[k], col = \"red\")\n}\n\nfor (k in 1:p){\n  plot(win, keep_Omega[win, k],   type = \"l\",\n       ylab = bquote(omega[.(k)]^2),\n       xlab = \"iter\")\n  abline(h = Omega[k,k], col = \"red\")\n}\n\n\n\n\n\nConstruct Trace Plots\npar(mfrow = c(1,1))\n\nplot(win, keep_sigma2[win],   type = \"l\",\n     ylab = bquote(sigma^2),\n     xlab = \"iter\")\nabline(h = sigma20, col = \"red\")\n\n\n\n\n\n\n\nConstruct Trace Plots\nwin <- 1:niter\n\npar(mfrow = c(2,2))\nparam_sample <- sample(1:100,4)\n\nfor (i in param_sample){\n  subscr <- paste0(\"1,\",i)\n  plot(win, keep_delta[1,i,win], type = \"l\",\n       ylab = bquote(delta[.(subscr)]),\n       xlab = \"iter\")\n  abline(h = delta0[1,i], col = \"red\")\n}\n\n\n\n\n\nConstruct Trace Plots\nparam_sample <- sample(1:100,4)\n\nfor (i in param_sample){\n  subscr <- paste0(\"2,\",i)\n  plot(win, keep_delta[2,i,win], type = \"l\",\n       ylab = bquote(delta[.(subscr)]),\n       xlab = \"iter\")\n  abline(h = delta0[2,i], col = \"red\")\n}\n\n\n\n\n\nOkay, this appears to be working as well based on the trace plots alone. In the next step we will add a linear trend to the underlying distribution for \\(\\boldsymbol\\delta_i\\)."
  },
  {
    "objectID": "research/2023-02-02-Step-3-Add-a-2nd-Level-Linear-Trend/index.html",
    "href": "research/2023-02-02-Step-3-Add-a-2nd-Level-Linear-Trend/index.html",
    "title": "Step 3 - Add a Second Level Linear Trend",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tictoc)\nlibrary(Rfast)\n\n\nBriefly restating notation from earlier, \\(\\mathbf{Y}_i\\) is an \\(n_i \\times 1\\) response vector , \\(\\mathbf{X}\\) an \\(m \\times p\\) parent design matrix, and \\(\\boldsymbol\\delta_i\\) a \\(p \\times 1\\) parameter vector corresponding to subject \\(i = 1,\\dots,N\\). The matrix \\(\\widetilde{\\mathbf{X}}_i\\) is an \\(n_i \\times p\\) matrix associated with subject \\(i\\) which is construct with a sample of rows from the parent design matrix \\(\\mathbf{X}\\). The entries of \\(\\mathbf{Y}_i\\) are mutually independent with constant variance \\(\\sigma^2\\), \\(\\text{Cov}(\\mathbf{Y}_i) = \\sigma^2 \\mathbf{I}_{n_i}\\) for all \\(i\\). Further, \\(\\mathbf{Y}_1,\\dots,\\mathbf{Y}_N\\) are mutually independent.\nIn this step we will add a linear trend to the mean of the distribution for \\(\\boldsymbol\\delta_i\\). There will be more parameters to estimate and some modification to the full conditionals for the Gibbs sampler. Let \\(\\mathbf{z}_i\\) be a \\(q \\times 1\\) vector of covariates, including an intercept element in the first position.\nThe linear trend in the random effects distribution for \\(\\boldsymbol\\delta_i\\) is represented as a linear combination \\(\\sum_{l=1}^q\\boldsymbol\\beta_l z_{il}\\) where \\(\\boldsymbol\\beta_l\\) are \\(p \\times 1\\) vectors. There are two ways to write this expression in matrix notation. Let \\(\\boldsymbol\\beta = (\\boldsymbol\\beta_1^T, \\boldsymbol\\beta_2^T, \\dots, \\boldsymbol\\beta_q^T)^T\\) be the \\(qp \\times 1\\) vector of stacked \\(\\boldsymbol\\beta_i\\) vectors and let \\(\\mathbf{B} = [\\boldsymbol\\beta_1 \\;\\;\\; \\boldsymbol\\beta_2 \\;\\;\\; \\dots \\;\\;\\; \\boldsymbol\\beta_q]\\) be the \\(p \\times q\\) matrix of arranged \\(\\boldsymbol\\beta_i\\) vectors. Note, \\(\\boldsymbol\\beta = \\text{vec}(\\mathbf{B})\\). Either we write\n\\[\\begin{equation}\n\\tag{1}\n\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta =\n\\begin{bmatrix}\n1 & \\dots & 0 & z_{i2} & \\dots & 0 & & z_{iq} & \\dots & 0\\\\\n\\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots & \\dots & \\vdots & \\ddots & \\vdots\\\\\n0 & \\dots & 1 & 0 & \\dots & z_{i2} & & 0 & \\dots & z_{iq}\n\\end{bmatrix}_{p \\, \\times \\, qp}\n\\begin{bmatrix}\n\\boldsymbol\\beta_1\\\\\n\\boldsymbol\\beta_2\\\\\n\\vdots\\\\\n\\boldsymbol\\beta_q\n\\end{bmatrix}_{qp \\, \\times \\, 1}\n\\end{equation}\\] or we could write \\[\\begin{equation}\n\\tag{2}\n\\mathbf{B}\\mathbf{z}_i =\n\\begin{bmatrix}\n\\boldsymbol\\beta_1 & \\boldsymbol{\\beta}_2 & \\dots & \\boldsymbol\\beta_q\n\\end{bmatrix}_{p \\,\\times \\, q}\n\\begin{bmatrix}\n1\\\\\nz_{i2}\\\\\n\\vdots\\\\\nz_{iq}\n\\end{bmatrix}_{q \\, \\times \\, 1}\n\\end{equation}\\]\nThe former is attractive for deriving analytical expressions and the latter is attractive for some computational advantages. The model is defined to be as follows,\n\\[\\begin{align*}\n\\mathbf{Y}_i &\\sim \\text{Normal}_{n_i}\\left(\\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i, \\; \\sigma^2 \\mathbf{I}_{n_i}\\right)\\\\\n\\boldsymbol\\delta_i &\\sim \\text{Normal}_p\\left(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta, \\; \\mathbf{\\Omega}\\right)\n\\end{align*}\\]\nAgain, we will assume \\(\\mathbf{\\Omega}\\) is diagonal and let \\(\\omega_{kk}\\) be the \\(k\\)th diagonal element. Next we specify priors, \\[\\begin{align*}\n\\boldsymbol\\beta &\\sim \\text{Normal}_{qp}\\left(\\boldsymbol\\mu, \\; \\mathbf{\\Lambda}\\right)\\\\\n\\omega_{kk} &\\sim \\text{InvGamma}\\left(a_\\omega, \\; b_\\omega \\right)\\\\\n\\sigma^2 &\\sim \\text{InvGamma}\\left(a_\\sigma, \\; b_\\sigma\\right)\n\\end{align*}\\]\nThe full conditionals in this model are as follows,\n\\[\\begin{align*}\n\\boldsymbol\\delta_i|\\text{ rest} &\\sim \\text{Normal}_p(\\mathbf{V}_i^{-1}\\mathbf{M}_i, \\mathbf{V}_i^{-1})\\\\\n\\mathbf{V}_i &= \\frac{1}{\\sigma^2} \\widetilde{\\mathbf{X}}_i^T\\widetilde{\\mathbf{X}}_i + \\mathbf{\\Omega}^{-1}\\\\\n\\mathbf{M}_i &= \\frac{1}{\\sigma^2} \\widetilde{\\mathbf{X}}_i^T\\mathbf{Y}_i + \\mathbf{\\Omega}^{-1}\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta\\\\\\\\\n\\boldsymbol\\beta|\\text{ rest} &\\sim \\text{Normal}_{qp}(\\mathbf{V}_\\beta^{-1}\\mathbf{M}_\\beta, \\mathbf{V}_\\beta^{-1})\\\\\n\\mathbf{V}_\\beta &= \\sum_{i=1}^N\\boldsymbol{\\mathcal{Z}}^T_i\\mathbf{\\Omega}^{-1}\\boldsymbol{\\mathcal{Z}}_i + \\mathbf{\\Lambda}^{-1}\\\\\n\\mathbf{M}_\\beta &= \\sum_{i=1}^N\\boldsymbol{\\mathcal{Z}}^T_i\\mathbf{\\Omega}^{-1}\\boldsymbol\\delta_i + \\mathbf{\\Lambda}^{-1}\\boldsymbol\\mu\\\\\\\\\n\\omega_{kk}|\\text{ rest} &\\sim \\text{InvGamma}(A_\\omega,B_\\omega)\\\\\nA_\\omega &= N/2 + a_\\omega\\\\\nB_\\omega &= \\frac{1}{2}\\sum_{i=1}^N (\\delta_{ik} - (\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta)_k)^2 + b_\\omega\\\\\\\\\n\\sigma^2|\\text{ rest} &\\sim \\text{InvGamma}(A_\\sigma,B_\\sigma)\\\\\nA_\\sigma &= \\frac{1}{2}\\sum_{i=1}^N n_i + a_\\sigma\\\\\nB_\\sigma &= \\frac{1}{2}\\sum_{i=1}^N (\\mathbf{Y}_i - \\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i)^T(\\mathbf{Y}_i - \\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i) + b_\\sigma\n\\end{align*}\\]\nA few computational remarks, in the full conditional for \\(\\boldsymbol\\beta\\), we have the expression \\(\\sum_{i=1}^N\\boldsymbol{\\mathcal{Z}}^T_i\\boldsymbol\\Omega^{-1}\\boldsymbol{\\mathcal{Z}}_i\\) which would be much too naive to compute directly for each MCMC iteration. Instead, we will rewrite this expression in such a way that will allow us to simplify computations inside the Gibbs loop. First, consider a different and perhaps more natural organization of the covariates \\(\\mathbf{z}_i\\), into a matrix \\(\\mathbf{Z} = (\\mathbf{z}_1^T, \\dots,\\mathbf{z}_N^T)^T\\) \\[\\begin{equation}\n\\tag{3}\n\\mathbf{Z} =\n\\begin{bmatrix}\n1 & z_{12} & \\dots & z_{1q}\\\\\n\\vdots & & \\vdots &\\\\\n1 & z_{N2} & \\dots & z_{Nq}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{z}_1^T\\\\\n\\vdots\\\\\n\\mathbf{z}_N^T\\\\\n\\end{bmatrix}\n\\end{equation}\\]\nLet \\(\\otimes\\) represent the Kronecker product. The following identity holds, \\[\\begin{equation}\n\\tag{4}\n\\sum_{i=1}^N\\boldsymbol{\\mathcal{Z}}^T_i\\boldsymbol\\Omega^{-1}\\boldsymbol{\\mathcal{Z}}_i = \\mathbf{Z}^T\\mathbf{Z} \\otimes \\boldsymbol\\Omega^{-1}\n\\end{equation}\\]\nWith \\(\\mathbf{Z}\\) known we can compute \\(\\mathbf{Z}^T\\mathbf{Z}\\) outside of the Gibbs loop.\nAlso in the full conditional for \\(\\boldsymbol\\beta\\) we have the expression \\(\\sum_{i=1}^N \\boldsymbol{\\mathcal{Z}}^T_i\\mathbf{\\Omega}^{-1}\\boldsymbol\\delta_i\\). Unfortunately, with \\(\\boldsymbol\\delta_i\\) and \\(\\boldsymbol\\Omega^{-1}\\) being parameters to update, this expression must be fully evaluated at each MCMC iteration. Let \\(\\boldsymbol\\Delta = (\\boldsymbol\\delta_1,\\dots,\\boldsymbol\\delta_N)\\) be the \\(p \\times N\\) matrix of arranged parameter vectors, \\(\\boldsymbol\\delta_i\\), and \\(\\mathbf{Z}\\) be defined as before in \\((3)\\). A useful identity in computing the quantity of interest is \\[\\begin{equation}\n\\tag{5}\n\\sum_{i=1}^N\\boldsymbol{\\mathcal{Z}}^T_i\\mathbf{\\Omega}^{-1}\\boldsymbol\\delta_i =  \\text{vec}(\\Omega^{-1}\\boldsymbol\\Delta\\mathbf{Z}) = (\\mathbf{Z}^T \\otimes \\, \\boldsymbol\\Omega^{-1})\\text{vec}(\\boldsymbol\\Delta).\n\\end{equation}\\]\nFor the other full conditionals in which \\(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta\\) appears, namely \\(\\boldsymbol\\delta_i\\) and \\(\\omega_{kk}\\), we can replace with appropriate and readily computed form \\(\\mathbf{B}\\mathbf{z}_i\\) as defined in \\((2)\\).\nSimulate some data from this model. In this case will we set \\(p = 2\\), \\(q = 2\\), \\(N = 100\\), \\(n_i = n = 100\\), and \\(m = 400\\).\n\n\nSimulate the data\n# dimensions\nN <- 100\nm <- 400\nn <- rep(100, N)\np <- 2\nq <- 2\n\n# Design matrices\nXp <- matrix(c(rep(1,m), rnorm(m*(p-1), mean = 0, sd = 5)), nrow = m, ncol = p)\nZ  <- matrix(c(rep(1,N), rnorm(N*(q-1), mean = 0, sd = 5)), nrow = N, ncol = q)\n\n# beta parameters\nB0     <- Rfast::rmvnorm(q, rep(0,p), (5^2)*diag(2))\nbeta0  <- matrix(c(B0), ncol = 1)\nOmega0 <- diag(c(2,1))\n\n# delta parameters\ndelta0  <- matrix(0, nrow = p, ncol = N)\nsigma20 <- 1\n\n# sample data\nY <- list()\nX <- list()\nfor (i in 1:N){\n  # draw delta\n  delta0[,i]   <- t(Rfast::rmvnorm(1, B0%*%Z[i,], Omega0))\n  \n  # draw rows from parent X\n  subject_rows <- sample(1:m, n[i])\n  X[[i]] <- Xp[subject_rows,]\n  \n  # draw response\n  Y[[i]] <- matrix(rnorm(n[i], mean = X[[i]]%*%delta0[,i], sd = sqrt(sigma20)), ncol=1)\n}\n\n\n\n\nRun the Gibbs Sampler\n# set-up\nniter <- 5000\nkeep_delta  <- array(NA, dim = c(p, N, niter))\nkeep_B      <- array(NA, dim = c(p, q, niter))\nkeep_Omega  <- matrix(NA, nrow = niter, ncol = p)\nkeep_sigma2 <- rep(NA, niter)\n\n# initial values\ndelta  <- matrix(0, nrow = p, ncol = N)\nbeta   <- rep(10, q*p)\nB      <- matrix(c(beta), nrow=p)\nsigma2 <- 3\nOmega  <- diag(c(5,5))\nkeep_delta[,,1] <- delta\nkeep_B[,,1]     <- B\nkeep_Omega[1,]  <- diag(Omega)\nkeep_sigma2[1]  <- sigma2\n\n# prior parameters\nmu    <- rep(0, q*p)\nLambda_inv <- diag(rep(1e-06,q*p))\na     <- 0.1\nb     <- 0.1\n\n# pre-computes\nXtX <- list()\nXtY <- list()\nfor (k in 1:N){\n  XtX[[k]] <- t(X[[k]])%*%X[[k]]\n  XtY[[k]] <- t(X[[k]])%*%Y[[k]]\n}\nZtZ <- t(Z)%*%Z\nLmu <- Lambda_inv%*%mu\nAo    <- N/2 + a\nAs    <- sum(n)/2 + a\n\ntic()\n# Gibbs Loop\nfor (iter in 2:niter){\n\n  Omega_inv <- diag(1/diag(Omega))\n  \n  # sample deltas\n  for (i in 1:N){\n    M         <- (1/sigma2)*XtY[[i]] + Omega_inv%*%B%*%Z[i,]\n    V_inv     <- chol2inv(chol((1/sigma2)*XtX[[i]] + Omega_inv))\n    delta[,i] <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n  }\n  \n  # sample beta\n  M     <- kronecker(t(Z), Omega_inv)%*%matrix(c(delta), ncol = 1) + Lmu\n  V_inv <- solve(kronecker(ZtZ, Omega_inv) + Lambda_inv)\n  beta  <- V_inv%*%M+t(chol(V_inv))%*%rnorm(q*p)\n  B     <- matrix(beta, nrow = p)\n  \n  # sample omegas\n  for (k in 1:p){\n    Bo <- sum((delta[k,] - (B%*%t(Z))[k,])^2)/2 + b\n    Omega[k,k] <- 1/rgamma(1, Ao, Bo)\n  }\n  \n  # sample sigma2\n  SSE <- 0\n  for (i in 1:N){\n    SSE <- SSE + sum((Y[[i]] - X[[i]]%*%delta[,i])^2)\n  }\n  Bs <- SSE/2 + b\n  sigma2 <- 1/rgamma(1, As, Bs)\n  \n  # store everything\n  keep_delta[,,iter] <- delta\n  keep_B[,,iter]     <- B\n  keep_Omega[iter,]  <- diag(Omega)\n  keep_sigma2[iter]  <- sigma2\n}\ntoc()\n\n\n21.13 sec elapsed\n\n\nNow for some trace plots. The following are for \\(\\mathbf{B}\\), \\(\\boldsymbol\\Omega\\), and \\(\\sigma^2\\). And display iterations 100:5000. This visual inspection seems to indicate good convergence!\n\n\nConstruct Trace Plots\nwin <- 100:niter\n\npar(mfrow = c(2,2))\n\nfor(l in 1:q){\n  for (k in 1:p){\n    plot(win, keep_B[k, l, win], type = \"l\",\n         ylab = bquote(beta[paste(.(l),\",\",.(k))]),\n         xlab = \"iter\")\n    abline(h = B0[k, l], col = \"red\")\n  }\n}\n\n\n\n\n\nConstruct Trace Plots\nfor (k in 1:p){\n  plot(win, keep_Omega[win, k], type = \"l\",\n       ylab = bquote(omega[paste(.(k),\",\",.(k))]),\n       xlab = \"iter\")\n  abline(h = Omega[k, k], col = \"red\")\n}\n\nplot(win, keep_sigma2[win], type = \"l\",\n     ylab = bquote(sigma^2),\n     xlab = \"iter\")\nabline(h = sigma20, col = \"red\")"
  },
  {
    "objectID": "research/2023-02-02-Step-4-Application-to-the-Linearized-Double-Logistic-Function/index.html",
    "href": "research/2023-02-02-Step-4-Application-to-the-Linearized-Double-Logistic-Function/index.html",
    "title": "Step 4 - Application to the Linearized Double-Logistic Function",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tictoc)\nlibrary(Rfast)\nlibrary(numDeriv)\n\n\nLet \\(Y_{ij}\\) be an observation collected on day \\(t_{ij}\\) of year \\(i\\), \\(i = 1, \\dots, N\\) and \\(j = 1,\\dots,n_i\\). Assume \\(Y_{ij}\\) to be normally distributed with mean \\(v(t_{ij}, \\, \\boldsymbol\\theta_i)\\) and constant variance \\(\\sigma^2\\). The mean “double-logistic” function, \\(v(t, \\, \\boldsymbol{\\theta})\\), is parameterized by \\(\\boldsymbol\\theta\\) and is defined \\[\\begin{equation}\n\\tag{1}\nv(t, \\, \\boldsymbol\\theta) = \\theta_1 + (\\theta_2 - \\theta_7 t)\\left(\\frac{1}{\\exp\\left\\{\\frac{\\theta_3 - t}{\\theta_4}\\right\\}} - \\frac{1}{\\exp\\left\\{\\frac{\\theta_4 - t}{\\theta_6}\\right\\}}\\right)\n\\end{equation}\\]\nThere is an additional constraint that \\(v(t, \\, \\boldsymbol{\\theta}) \\in [0,1]\\) and \\(\\theta_7 \\ge 0\\). One option would be to put priors on \\(\\theta_1\\) with \\([0,1]\\) support and \\(\\theta_7\\) with \\([0,\\infty)\\) support. One might also consider the constraint \\(\\theta_2 \\in [\\theta_1, 1]\\), but we should be careful to specify a prior whose support depends on another parameter. Instead, we opt not to constrain or transform \\(\\theta_2\\). In general, we would prefer to place a multivariate normal prior over the entire vector \\(\\boldsymbol\\theta\\). To that end, consider a reparameterazation of \\(v(t,\\boldsymbol\\theta)\\),\n\\[\\begin{equation}\n\\tag{2}\nv(t, \\, \\boldsymbol\\theta) = \\frac{1}{1+\\exp\\{\\theta_1\\}} + \\left(\\theta_2 - \\exp\\{\\theta_7\\} t\\right)\\left(\\frac{1}{\\exp\\left\\{\\frac{\\theta_3 - t}{\\theta_4}\\right\\}} - \\frac{1}{\\exp\\left\\{\\frac{\\theta_4 - t}{\\theta_6}\\right\\}}\\right)\n\\end{equation}\\]\nThe function \\(v(t, \\, \\boldsymbol{\\theta})\\) is a non-linear function of \\(\\boldsymbol\\theta\\) which consequently violates conjugacy of the typically assumed priors. To overcome this we will work with a linearized version of \\(v(t, \\, \\boldsymbol{\\theta})\\). This significantly decreases the computational burden of MCMC at the cost of introducing statistical bias. Are the computational gains worth the induced bias?\nThe linearization of \\(v(t, \\, \\boldsymbol\\theta)\\) around \\(\\boldsymbol\\theta_0\\) yields, \\[\\begin{equation}\n\\tag{3}\nv(t, \\, \\boldsymbol\\theta) \\approx v(t, \\, \\boldsymbol\\theta_0) + \\nabla_{\\boldsymbol\\theta}v(t, \\, \\boldsymbol\\theta)|_{\\boldsymbol\\theta=\\boldsymbol\\theta_0}(\\boldsymbol\\theta - \\boldsymbol\\theta_0)\n\\end{equation}\\]\nDefine the following,\n\\[\\begin{equation}\n\\tag{4}\nY^*_{ij} = Y_{ij} - v(t_{ij}, \\, \\boldsymbol\\theta_0)\n\\end{equation}\\]\n\\[\\begin{equation}\n\\tag{5}\nX(t) = \\nabla_{\\boldsymbol\\theta}v(t, \\, \\boldsymbol\\theta)|_{\\boldsymbol\\theta=\\boldsymbol\\theta_0}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\tag{6}\n\\boldsymbol\\delta_i = \\boldsymbol\\theta_i-\\boldsymbol\\theta_0\n\\end{equation}\\]\n\\[\\begin{equation}\n\\mathbf{Y}_i \\sim \\text{Normal}_{n_i}\\left(v(\\mathbf{t}_i, \\boldsymbol{\\theta}_i), \\; \\sigma^2 \\mathbf{I}_{n_i}\\right)\n\\end{equation}\\]\nConstruct the \\(m \\times p\\) parent design matrix \\(\\mathbf{X}\\) with rows \\(X(t)\\) for \\(t=1,\\dots,m\\). Define the “child” design matrix, \\(\\widetilde{\\mathbf{X}}_i\\) to be the subset of rows from \\(\\mathbf{X}\\) specified by \\(\\mathbf{t}_i\\).\nThe \\(N \\times q\\) covariate matrix \\(\\mathbf{Z}\\) organizes known covariates for the mean of the distribution for \\(\\boldsymbol\\delta_i\\). Let \\(\\mathbf{z}_i\\) be the \\(i\\)th row of \\(\\mathbf{Z}\\) and \\(\\boldsymbol{\\mathcal{Z}}_i = (\\mathbf{z}^T_i\\otimes \\mathbf{I}_p)\\) where “\\(\\otimes\\)” represents the Kronecker product.\nNow we can place a multivariate prior on \\(\\boldsymbol\\delta_i\\) as this parameter represents deviations from \\(\\boldsymbol\\theta_0\\). The linearized model is as follows,\n\\[\\begin{align*}\n\\mathbf{Y}^*_i &\\sim \\text{Normal}_{n_i}\\left(\\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i, \\; \\sigma^2 \\mathbf{I}_{n_i}\\right)\\\\\n\\boldsymbol\\delta_i &\\sim \\text{Normal}_p\\left(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta, \\; \\mathbf{\\Omega}\\right)\n\\end{align*}\\]\nAgain, we will assume \\(\\mathbf{\\Omega}\\) is diagonal and let \\(\\omega_{kk}\\) be the \\(k\\)th diagonal element. Next we specify priors, \\[\\begin{align*}\n\\boldsymbol\\beta &\\sim \\text{Normal}_{qp}\\left(\\boldsymbol\\mu, \\; \\mathbf{\\Lambda}\\right)\\\\\n\\omega_{kk} &\\sim \\text{InvGamma}\\left(a_\\omega, \\; b_\\omega \\right)\\\\\n\\sigma^2 &\\sim \\text{InvGamma}\\left(a_\\sigma, \\; b_\\sigma\\right)\n\\end{align*}\\]\nLet \\(\\boldsymbol\\Delta = [\\boldsymbol\\delta_1 \\; \\dots \\; \\boldsymbol\\delta_N]\\) and \\(\\mathbf{B} = [\\boldsymbol\\beta_1 \\; \\dots \\; \\boldsymbol\\beta_q]\\). That is, \\(\\mathbf{B}\\) is a \\(p \\times q\\) matrix and \\(\\text{vec}(\\mathbf{B}) = \\boldsymbol\\beta\\). The full conditionals in this model are as follows,\n\\[\\begin{align*}\n\\boldsymbol\\delta_i|\\text{rest} &\\sim \\text{Normal}_p(\\mathbf{V}_i^{-1}\\mathbf{M}_i, \\mathbf{V}_i^{-1})\\\\\n\\mathbf{V}_i &= \\frac{1}{\\sigma^2} \\widetilde{\\mathbf{X}}_i^T\\widetilde{\\mathbf{X}}_i + \\mathbf{\\Omega}^{-1}\\\\\n\\mathbf{M}_i &= \\frac{1}{\\sigma^2} \\widetilde{\\mathbf{X}}_i^T\\mathbf{Y}_i + \\mathbf{\\Omega}^{-1}\\mathbf{B}\\mathbf{z}_i\\\\\\\\\n\\boldsymbol\\beta|\\text{rest} &\\sim \\text{Normal}_{qp}(\\mathbf{V}_\\beta^{-1}\\mathbf{M}_\\beta, \\mathbf{V}_\\beta^{-1}\\\\\n\\mathbf{V}_\\beta &= \\mathbf{Z}^T\\mathbf{Z} \\otimes \\boldsymbol\\Omega^{-1} + \\mathbf{\\Lambda}^{-1}\\\\\n\\mathbf{M}_\\beta &= (\\mathbf{Z}^T \\otimes \\, \\boldsymbol\\Omega^{-1})\\text{vec}(\\boldsymbol\\Delta) + \\mathbf{\\Lambda}^{-1}\\boldsymbol\\mu\\\\\\\\\n\\omega_{kk}|\\text{rest} &\\sim \\text{InvGamma}(A_\\omega,B_\\omega)\\\\\nA_\\omega &= N/2 + a_\\omega\\\\\nB_\\omega &= \\frac{1}{2}\\sum_{i=1}^N (\\delta_{ik} - (\\mathbf{B}\\mathbf{z}_i)_k)^2 + b_\\omega\\\\\\\\\n\\sigma^2|\\text{rest} &\\sim \\text{InvGamma}(A_\\sigma,B_\\sigma)\\\\\nA_\\sigma &= \\frac{1}{2}\\sum_{i=1}^N n_i + a_\\sigma\\\\\nB_\\sigma &= \\frac{1}{2}\\sum_{i=1}^N (\\mathbf{Y}_i - \\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i)^T(\\mathbf{Y}_i - \\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i) + b_\\sigma\n\\end{align*}\\]\nNext moving on the code implementation. Define the double logistic function and its gradient.\n\n\nUser-defined functions.\n# expit #\n# equivalent to plogis with m=0 & s=1.\nexpit   <- function(x){1/(1+exp(-x))}\n\n# expit_p #\n# first derivative of the expit function. equivalent to dlogis with m=0 & s=1.\nexpit_p <- function(x){expit(x)*(1-expit(x))}\n\n\ndouble_logis <- function(t, theta){\n  # double logistic function.\n  # theta1 is transformed using the logistic function.\n  # theta2 is transformed using the \n  # This allows for all parameters to follow a gaussian distribution\n  \n  theta[1] <- plogis(theta[1])\n  theta[7] <- exp(theta[7])  \n  \n  n1 <- 1\n  d1 <- 1 + exp((theta[3] - t)/theta[4])\n    \n  n2 <- 1\n  d2 <- 1 + exp((theta[5] - t)/theta[6])\n    \n  out <- theta[1] + (theta[2] - theta[7]*t)*(n1/d1 - n2/d2)\n  \n  return(out)\n}\n\n\n# double logistic gradient wrt theta\nbasis_functions <- function(t,theta){\n  t   <- t%%366\n\n  dl0 <- double_logis(t,theta)-expit(theta[1])\n  a <- expit_p(theta[7])\n  theta[7] <- exp(theta[7])\n\n  B1 <- expit_p(theta[1])\n  B2 <- (1/(1+exp((theta[3]-t)/theta[4])))-(1/(1+exp((theta[5]-t)/theta[6])))\n  B3 <- (theta[7]*t - theta[2])/(2*theta[4]*cosh((theta[3]-t)/theta[4])+2*theta[4])\n  B4 <- ((theta[3] - t)*(theta[2]-theta[7]*t)*cosh((theta[3]-t)/(2*theta[4]))^(-2))/(4*theta[4]^2)\n  B5 <- (theta[2] - theta[7]*t)/(2*theta[6]*cosh((theta[5]-t)/theta[6])+2*theta[6])\n  B6 <- ((theta[5] - t)*(theta[7]*t-theta[2])*cosh((theta[5]-t)/(2*theta[6]))^(-2))/(4*theta[6]^2)\n  B7 <- -t*a*((1/(1+exp((theta[3]-t)/theta[4])))-(1/(1+exp((theta[5]-t)/theta[6]))))\n  B <- unname(cbind(B1, B2, B3, B4, B5, B6, B7))\n\n  return(B)\n}\n\n\nWe need to specify a value of \\(\\boldsymbol\\theta_0\\) on which to center the linearization. We will expand on how to obtain this value at a later time.\n\n\nCode\ntheta0 <- c(-1.80, 0.75, 120, 8, 270, 8, -7.5)\n\nplot(1:366, double_logis(1:366, theta0), type = \"l\",\n     xlab = \"t\",\n     ylab = bquote(v(t,theta[0])))\n\n\n\n\n\nWe verify that the analytical gradient is correct by checking the result against a numerical gradient function.\n\n\nConstruct the numerical and analytical gradiant matrix\ntheta0 <- c(-1.80, 0.75, 120, 8, 270, 8, -7.5)\n\ngradinput <- function(x,t){\n  return(double_logis(t,x))\n}\n\n# compute numerical gradient\nG <- matrix(0, nrow = 366, ncol = 7)\nfor (i in 1:366){\n  G[i,] <- grad(gradinput, x = theta0, t=i)\n}\n\n# # compute the analytical gradient\nX <- basis_functions(1:366, theta0)\n\nmatplot(G, type=\"l\", main = \"Numerical Gradient\", xlab = \"t\")\n\n\n\n\n\nConstruct the numerical and analytical gradiant matrix\nmatplot(X, type=\"l\", main = \"Analytical Gradient\", xlab = \"t\")\n\n\n\n\n\nNext we simulate some data from the linearized model. (Need to add details regarding simulation set-up).\n\n\nSimulate the data\n# dimensions\nN <- 40\nm <- 366\nn <- rep(100,N)\np <- 7\nq <- 2\n\ntheta0 <- c(-1.80, 0.75, 120, 8, 270, 8, -7.5)\n\n# Design matrices\ngradinput <- function(x,t){\n  return(double_logis(t,x))\n}\n\n# compute numerical gradient\nXp <- basis_functions(1:366, theta0)\nZ  <- matrix(c(rep(1,N), seq(1:N)), ncol = 2)\n\n# beta parameters\nB0     <- matrix(c(0, 0, -15, 0, 15, 0, 0,\n                   0, 0,   1, 0, -1, 0, 0), nrow=p)\nbeta0  <- matrix(c(B0), ncol = 1)\nOmega0 <- diag(c(0.025, 1e-3,  3, 1,  3, 1, 0.0005))\n\n# delta parameters\ndelta0  <- matrix(0, nrow = p, ncol = N)\nsigma20 <- 0.0025\n\n# sample data\nY <- list()\nX <- list()\nt <- list()\nfor (i in 1:N){\n  # draw delta\n  delta0[,i]   <- t(Rfast::rmvnorm(1, B0%*%Z[i,], Omega0))\n  \n  # draw rows from parent X\n  t[[i]] <- sample(1:m, n[i])\n  X[[i]] <- Xp[t[[i]],]\n  \n  # draw response\n  Y[[i]] <- matrix(rnorm(n[i], mean = X[[i]]%*%delta0[,i], sd = sqrt(sigma20)), ncol=1)\n}\n\n\nPlot the simulated curves. We won’t plot the individual observations.\n\n\nCode\nplot(1:366, double_logis(1:366, theta0), type = \"l\", ylim=c(-0.1, 1.2))\n\nfor (i in 1:N){\n  lines(1:366, double_logis(1:366, theta0) + Xp%*%delta0[,i], type=\"l\", col = i,\n        xlab = \"t\",\n        ylab = \"Y\",\n        main = \"Simulated Curves\")\n  # points(t[[i]], Y[[i]] + double_logis(t[[i]], theta0), pch=16)\n}\n\n\n\n\n\nThe Gibbs loop. Later we will re-write this as a function.\n\n\nRun the Gibbs sampler\n# set-up\nniter <- 5000\nkeep_delta  <- array(NA, dim = c(p, N, niter))\nkeep_B      <- array(NA, dim = c(p, q, niter))\nkeep_Omega  <- matrix(NA, nrow = niter, ncol = p)\nkeep_sigma2 <- rep(NA, niter)\n\n# initial values\ndelta  <- matrix(0, nrow = p, ncol = N)\nbeta   <- rep(0, q*p)\nB      <- matrix(c(beta), nrow=p)\nsigma2 <- 0.01\nOmega  <- diag(c(0.5, 0.5, 5, 0.5, 5, 0.5, 0.001))\nkeep_delta[,,1] <- delta\nkeep_B[,,1]     <- B\nkeep_Omega[1,]  <- diag(Omega)\nkeep_sigma2[1]  <- sigma2\n\n# prior parameters\nmu    <- rep(0, q*p)\nLambda_inv <- diag(rep(1e-06,q*p))\na     <- 0.1\nb     <- 0.1\n\n# pre-computes\nXtX <- list()\nXtY <- list()\nfor (k in 1:N){\n  XtX[[k]] <- t(X[[k]])%*%X[[k]]\n  XtY[[k]] <- t(X[[k]])%*%Y[[k]]\n}\nZtZ <- t(Z)%*%Z\nLmu <- Lambda_inv%*%mu\nAo    <- N/2 + a\nAs    <- sum(n)/2 + a\n\n# Gibbs Loop\ntic()\nfor (iter in 2:niter){\n\n  Omega_inv <- diag(1/diag(Omega))\n  \n  # sample deltas\n  for (i in 1:N){\n    M         <- (1/sigma2)*XtY[[i]] + Omega_inv%*%(B%*%Z[i,])\n    V_inv     <- chol2inv(chol((1/sigma2)*XtX[[i]] + Omega_inv))\n    delta[,i] <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n  }\n  \n  # sample beta\n  M     <- kronecker(t(Z), Omega_inv)%*%matrix(c(delta), ncol = 1) + Lmu\n  V_inv <- solve(kronecker(ZtZ, Omega_inv) + Lambda_inv)\n  beta  <- V_inv%*%M+t(chol(V_inv))%*%rnorm(q*p)\n  B     <- matrix(beta, nrow = p)\n  \n  # sample omegas\n  for (k in 1:p){\n    Bo <- sum((delta[k,] - (B%*%t(Z))[k,])^2)/2 + b\n    Omega[k,k] <- 1/rgamma(1, Ao, Bo)\n  }\n  \n  # sample sigma2\n  SSE <- 0\n  for (i in 1:N){\n    SSE <- SSE + sum((Y[[i]] - X[[i]]%*%delta[,i])^2)\n  }\n  Bs <- SSE/2 + b\n  sigma2 <- 1/rgamma(1, As, Bs)\n  \n  # store everything\n  keep_delta[,,iter] <- delta\n  keep_B[,,iter]     <- B\n  keep_Omega[iter,]  <- diag(Omega)\n  keep_sigma2[iter]  <- sigma2\n}\ntoc()\n\n\n11.15 sec elapsed\n\n\nNow for some trace plots. The following are for \\(\\mathbf{B}\\), \\(\\boldsymbol\\Omega\\), and \\(\\sigma^2\\). And display iterations 100:5000. This visual inspection seems to indicate good convergence!\n\n\nConstruct trace plots\nwin <- 1:niter\n\npar(mfrow = c(2,2))\n\nfor(l in 1:q){\n  for (k in 1:p){\n    plot(win, keep_B[k, l, win], type = \"l\",\n         ylab = bquote(beta[paste(.(l),\",\",.(k))]),\n         xlab = \"iter\")\n    abline(h = B0[k, l], col = \"red\")\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\nConstruct trace plots\nfor (k in 1:p){\n  plot(win, keep_Omega[win, k], type = \"l\",\n       ylab = bquote(omega[paste(.(k),\",\",.(k))]),\n       xlab = \"iter\")\n  abline(h = Omega0[k, k], col = \"red\")\n}\n\n\n\n\n\n\n\n\nConstruct trace plots\nplot(win, keep_sigma2[win], type = \"l\",\n     ylab = bquote(sigma^2),\n     xlab = \"iter\")\nabline(h = sigma20, col = \"red\")\n\n\n\n\n\nTrace plots for 5 subject \\(\\boldsymbol\\delta_i\\) chosen at random.\n\n\nConstruct trace plots\nwin <- 1:niter\n\npar(mfrow = c(2,2))\nparam_sample <- sample(1:N, 5)\n\nfor (year in param_sample){\n  for (k in 1:p){\n    subscr <- paste0(year,\",\",k)\n    plot(win, keep_delta[k, year, win], type = \"l\",\n         ylab = bquote(delta[.(subscr)]),\n         xlab = \"iter\")\n    abline(h = delta0[k, year], col = \"red\")\n  }\n}"
  },
  {
    "objectID": "research/2023-02-02-Step-5-Verify-Convergence/index.html",
    "href": "research/2023-02-02-Step-5-Verify-Convergence/index.html",
    "title": "Step 5 - Verify Convergence",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tictoc)\nlibrary(Rfast)\nlibrary(numDeriv)\n\n\nThe goal here will be to verify convergence of the MCMC developed in the previous steps. First, we’ll define the functions necessary to fit the model.\n\n\nUser-defined functions\n# expit #\n# equivalent to plogis with m=0 & s=1.\nexpit   <- function(x){1/(1+exp(-x))}\n\n# expit_p #\n# first derivative of the expit function. equivalent to dlogis with m=0 & s=1.\nexpit_p <- function(x){expit(x)*(1-expit(x))}\n\n\ndouble_logis <- function(t, theta){\n  # double logistic function.\n  # theta1 is transformed using the logistic function.\n  # theta2 is transformed using the \n  # This allows for all parameters to follow a gaussian distribution\n  \n  theta[1] <- plogis(theta[1])\n  theta[7] <- exp(theta[7])  \n  \n  n1 <- 1\n  d1 <- 1 + exp((theta[3] - t)/theta[4])\n  \n  n2 <- 1\n  d2 <- 1 + exp((theta[5] - t)/theta[6])\n  \n  out <- theta[1] + (theta[2] - theta[7]*t)*(n1/d1 - n2/d2)\n  \n  return(out)\n}\n\n# double logistic gradient wrt theta\nbasis_functions <- function(t,theta){\n  t   <- t%%366\n  \n  dl0 <- double_logis(t,theta)-expit(theta[1])\n  a <- expit_p(theta[7])\n  theta[7] <- exp(theta[7])\n  \n  B1 <- expit_p(theta[1])\n  B2 <- (1/(1+exp((theta[3]-t)/theta[4])))-(1/(1+exp((theta[5]-t)/theta[6])))\n  B3 <- (theta[7]*t - theta[2])/(2*theta[4]*cosh((theta[3]-t)/theta[4])+2*theta[4])\n  B4 <- ((theta[3] - t)*(theta[2]-theta[7]*t)*cosh((theta[3]-t)/(2*theta[4]))^(-2))/(4*theta[4]^2)\n  B5 <- (theta[2] - theta[7]*t)/(2*theta[6]*cosh((theta[5]-t)/theta[6])+2*theta[6])\n  B6 <- ((theta[5] - t)*(theta[7]*t-theta[2])*cosh((theta[5]-t)/(2*theta[6]))^(-2))/(4*theta[6]^2)\n  B7 <- -t*a*((1/(1+exp((theta[3]-t)/theta[4])))-(1/(1+exp((theta[5]-t)/theta[6]))))\n  B <- unname(cbind(B1, B2, B3, B4, B5, B6, B7))\n  \n  return(B)\n}\n\nfit_lm <- function(Y, X, Z,      \n                   delta, sigma2,                     \n                   beta, Omega,                 \n                   mu, Lambda,                      \n                   a, b,                        \n                   niter = 5000){\n  \n  require(Rfast)\n  \n  # indexes\n  p <- dim(delta)[1]\n  q <- dim(Z)[2]\n  N <- dim(delta)[2]\n  n <- sapply(Y, length)\n  \n  # unstack beta\n  B <- matrix(c(beta), nrow=p)\n  Lambda_inv <- diag(1/diag(Lambda))\n  \n  # storage\n  keep_delta  <- array(NA, dim = c(p, N, niter))\n  keep_B      <- array(NA, dim = c(p, q, niter))\n  keep_Omega  <- matrix(NA, nrow = niter, ncol = p)\n  keep_sigma2 <- rep(NA, niter)\n  \n  # set initial values\n  keep_delta[,,1] <- delta\n  keep_B[,,1]     <- B\n  keep_Omega[1,]  <- diag(Omega)\n  keep_sigma2[1]  <- sigma2\n  \n  # pre-computes\n  XtX <- list()\n  XtY <- list()\n  for (k in 1:N){\n    XtX[[k]] <- t(X[[k]])%*%X[[k]]\n    XtY[[k]] <- t(X[[k]])%*%Y[[k]]\n  }\n  ZtZ <- t(Z)%*%Z\n  Lmu <- Lambda_inv%*%mu\n  Ao    <- N/2 + a\n  As    <- sum(n)/2 + a\n  \n  # Gibbs Loop\n  tik <- proc.time()\n  for (iter in 2:niter){\n    \n    Omega_inv <- diag(1/diag(Omega))\n    \n    # sample deltas\n    for (i in 1:N){\n      M         <- (1/sigma2)*XtY[[i]] + Omega_inv%*%(B%*%Z[i,])\n      V_inv     <- chol2inv(chol((1/sigma2)*XtX[[i]] + Omega_inv))\n      delta[,i] <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n    }\n    \n    # sample beta\n    M     <- kronecker(t(Z), Omega_inv)%*%matrix(c(delta), ncol = 1) + Lmu\n    V_inv <- solve(kronecker(ZtZ, Omega_inv) + Lambda_inv)\n    beta  <- V_inv%*%M+t(chol(V_inv))%*%rnorm(q*p)\n    B     <- matrix(beta, nrow = p)\n    \n    # sample omegas\n    for (k in 1:p){\n      Bo <- sum((delta[k,] - (B%*%t(Z))[k,])^2)/2 + b\n      Omega[k,k] <- 1/rgamma(1, Ao, Bo)\n    }\n    \n    # sample sigma2\n    SSE <- 0\n    for (i in 1:N){\n      SSE <- SSE + sum((Y[[i]] - X[[i]]%*%delta[,i])^2)\n    }\n    Bs <- SSE/2 + b\n    sigma2 <- 1/rgamma(1, As, Bs)\n    \n    # store everything\n    keep_delta[,,iter] <- delta\n    keep_B[,,iter]     <- B\n    keep_Omega[iter,]  <- diag(Omega)\n    keep_sigma2[iter]  <- sigma2\n  }\n  tok <- proc.time() - tik\n  \n  return(list(keep_delta = keep_delta, \n              keep_B = keep_B, \n              keep_Omega = keep_Omega, \n              keep_sigma2 = keep_sigma2, \n              comp_time = tok))\n}\n\n\nWe’ll start by simulating some data similar to that in Step 4, then initialize the Gibbs sampler at different values to confirm the chains converge to the same stationary distribution.\n\n\nSimulate the data\n# dimensions\nN <- 40\nm <- 366\nn <- rep(100,N)\np <- 7\nq <- 2\n\ntheta0 <- c(-1.80, 0.75, 120, 8, 270, 8, -7.5)\n\n# Design matrices\ngradinput <- function(x,t){\n  return(double_logis(t,x))\n}\n\n# compute numerical gradient\nXp <- basis_functions(1:366, theta0)\nZ  <- matrix(c(rep(1,N), seq(1:N)), ncol = 2)\n\n# beta parameters\nB0     <- matrix(c(0, 0, -15, 0, 15, 0, 0,\n                   0, 0,   1, 0, -1, 0, 0), nrow=p)\nbeta0  <- matrix(c(B0), ncol = 1)\nOmega0 <- diag(c(0.025, 1e-3,  3, 1,  3, 1, 0.0005))\n\n# delta parameters\ndelta0  <- matrix(0, nrow = p, ncol = N)\nsigma20 <- 0.0025\n\n# sample data\nY <- list()\nX <- list()\nt <- list()\nfor (i in 1:N){\n  # draw delta\n  delta0[,i]   <- t(Rfast::rmvnorm(1, B0%*%Z[i,], Omega0))\n  \n  # draw rows from parent X\n  t[[i]] <- sample(1:m, n[i])\n  X[[i]] <- Xp[t[[i]],]\n  \n  # draw response\n  Y[[i]] <- matrix(rnorm(n[i], mean = X[[i]]%*%delta0[,i], sd = sqrt(sigma20)), ncol=1)\n}\n\n\nFit model with first set of initial values.\n\n\nFit model 1\nniter  <- 5000\ndelta  <- matrix(0, nrow = p, ncol = N)\nbeta   <- rep(0, q*p)\nsigma2 <- 0.01\nOmega  <- diag(c(0.5, 0.5, 5, 0.5, 5, 0.5, 0.001))\n\nmu     <- rep(0, q*p)\nLambda <- diag(rep(1e06,q*p))\na      <- 0.1\nb      <- 0.1\n\nfit1 <- fit_lm(Y, X, Z,\n               delta, sigma2,\n               beta, Omega,\n               mu, Lambda,\n               a, b,\n               niter = niter)\n\n\nFit model with second set of initial values.\n\n\nFit model 2\ndelta  <- matrix(10, nrow = p, ncol = N)\nbeta   <- rep(10, q*p)\nsigma2 <- 2\nOmega  <- 2*diag(p)\n\nmu     <- rep(0, q*p)\nLambda <- diag(rep(1e06,q*p))\na      <- 0.1\nb      <- 0.1\n\nfit2 <- fit_lm(Y, X, Z,\n               delta, sigma2,\n               beta, Omega,\n               mu, Lambda,\n               a, b,\n               niter = niter)\n\n\nFit model with third set of initial values.\n\n\nFit model 3\ndelta  <- matrix(-10, nrow = p, ncol = N)\nbeta   <- rep(-10, q*p)\nsigma2 <- 3\nOmega  <- diag(p)\n\nmu     <- rep(0, q*p)\nLambda <- diag(rep(1e06,q*p))\na      <- 0.1\nb      <- 0.1\n\nfit3 <- fit_lm(Y, X, Z,\n               delta, sigma2,\n               beta, Omega,\n               mu, Lambda,\n               a, b,\n               niter = niter)\n\n\nNext we’ll construct the trace plots of the parameters from each of the three models. Convergence to the same stationary distribution will be apparent if the chains merge together.\nFirst for \\(\\boldsymbol\\beta\\).\n\n\nConstruct trace plots\nwin <- 100:niter\n\nymin <- c(-2, -2, -25, -10, -5, -10, -5)\nymax <- c( 2,  2,   5,  10, 20,  10,  5)\n\npar(mfrow = c(2,2))\n\nfor(l in 1:q){\n  for (k in 1:p){\n    # ymin <- min(fit1$keep_B[k, l, win], fit2$keep_B[k, l, win], fit3$keep_B[k, l, win])\n    # ymax <- max(fit1$keep_B[k, l, win], fit2$keep_B[k, l, win], fit3$keep_B[k, l, win])\n    plot(win, fit1$keep_B[k, l, win], type = \"l\",\n         ylab = bquote(beta[paste(.(l),\",\",.(k))]),\n         xlab = \"iter\",\n         ylim = c(ymin[k], ymax[k]))\n    lines(win, fit2$keep_B[k, l, win], col =\"blue\")\n    lines(win, fit3$keep_B[k, l, win], col =\"green\")\n    abline(h = B0[k, l], col = \"red\", lwd = 2)\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrace plots for \\(\\boldsymbol\\Omega\\).\n\n\nCode\npar(mfrow = c(2,2))\n\nymin <- 0\nymax <- c(0.1, 0.02, 10, 6, 8, 6, 0.3)\n\nfor (k in 1:p){\n  plot(win, fit1$keep_Omega[win, k], type = \"l\",\n       ylab = bquote(omega[paste(.(k),\",\",.(k))]),\n       xlab = \"iter\",\n       ylim = c(ymin, ymax[k]))\n  lines(win, fit2$keep_Omega[win, k], col =\"blue\")\n  lines(win, fit3$keep_Omega[win, k], col =\"green\")\n  abline(h = Omega0[k, k], col = \"red\",lwd = 3)\n}\n\n\n\n\n\n\n\n\nTrace plots for \\(\\sigma^2\\)\n\n\nCode\nplot(win, fit1$keep_sigma2[win], type = \"l\",\n     ylab = bquote(sigma^2),\n     xlab = \"iter\")\nlines(win, fit2$keep_sigma2[win], col =\"blue\")\nlines(win, fit3$keep_sigma2[win], col =\"green\")\nabline(h = sigma20, col = \"red\", lwd = 3)\n\n\n\n\n\nTrace plots for 5 subject \\(\\boldsymbol\\delta_i\\) chosden at random.\n\n\nCode\nwin <- 1:niter\n\npar(mfrow = c(2,2))\nparam_sample <- sample(1:N, 5)\n\nfor (year in param_sample){\n  for (k in 1:p){\n    subscr <- paste0(year,\",\",k)\n    plot(win, fit1$keep_delta[k, year, win], type = \"l\",\n         ylab = bquote(delta[.(subscr)]),\n         xlab = \"iter\")\n    lines(win, fit2$keep_delta[k, year, win], col =\"blue\")\n    lines(win, fit3$keep_delta[k, year, win], col =\"green\")\n    abline(h = delta0[k, year], col = \"red\")\n  }\n}"
  },
  {
    "objectID": "research/2023-02-02-Step-6-Application to the Double-Logistic Function/index.html",
    "href": "research/2023-02-02-Step-6-Application to the Double-Logistic Function/index.html",
    "title": "Step 6 - Application to the (orignal) Double-logistic function",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tictoc)\nlibrary(Rfast)\nlibrary(numDeriv)\n\n\nTODO"
  },
  {
    "objectID": "research/2023-02-02-Step-7-Bayesian spatial CAR model/index.html",
    "href": "research/2023-02-02-Step-7-Bayesian spatial CAR model/index.html",
    "title": "Step 7 - Areal data and the spatial CAR model - Basic Example",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(viridis)\nlibrary(MASS)\nlibrary(Matrix)\nlibrary(tictoc)\nlibrary(extraDistr)\nlibrary(CARBayes)"
  },
  {
    "objectID": "research/2023-02-02-Step-7-Bayesian spatial CAR model/index.html#a-brief-introduction",
    "href": "research/2023-02-02-Step-7-Bayesian spatial CAR model/index.html#a-brief-introduction",
    "title": "Step 7 - Areal data and the spatial CAR model - Basic Example",
    "section": "A Brief Introduction",
    "text": "A Brief Introduction\nHere we’re going to examine the spatial CAR model. CAR stands for Conditionally Autoregressive. The spatial CAR model is in a way an extension of autoregressive models for time series data. Time series data is typically 1-dimensional (in time) and the observations have a natural ordering in the sense that observations can be ordered by the time they were observed. Spatial data can be any dimension and do not necessarily have a natural ordering."
  },
  {
    "objectID": "research/2023-02-02-Step-7-Bayesian spatial CAR model/index.html#what-is-areal-data",
    "href": "research/2023-02-02-Step-7-Bayesian spatial CAR model/index.html#what-is-areal-data",
    "title": "Step 7 - Areal data and the spatial CAR model - Basic Example",
    "section": "What is areal data?",
    "text": "What is areal data?\nA CAR model is commonly applied to areal data. That is data where the spatial domain \\(D\\) is partitioned into a finite number of blocks or areas. A common example is the partition of the United States of America into states, census tracts, or counties. A measurement is then collected for each areal unit.\nThe spatial domain is \\(D\\).\nThe areal units are \\(B_i\\) for \\(i = 1,\\dots n\\).\nThe measurements are \\(Z_i \\equiv Z(B_i)\\) for \\(i = 1, \\dots, n\\).\nBefore we dive into the distributional assumptions related to the CAR model, something must be said about the structure of the blocks. Namely we must define some notion of proximity from one block to the next. It’s difficult in general to do this, particularly for an irregular partition of the spatial domain. The easiest approach is to define and adjacency matrix which captures which blocks are bordering other blocks. If there are \\(n\\) blocks, then this adjacency matrix \\(\\mathbf{W}\\) is \\(n \\times n\\) and\n\\[\nw_{ij} =\n\\begin{cases}\n1 \\quad \\text{if } B_i \\text{ shares a border with } B_j,\\\\\n0 \\quad \\text{otherwise}.\n\\end{cases}\n\\] By convention we say that an areal unit does not share a border with itself hence \\(w_{ii} = 0\\) for all \\(i = 1, \\dots, n\\).\nLet’s define our own spatial domain and partition it into some very basic units.\n\n\nCode\nn <- 4\nspat_domain <- expand.grid(x = 1:n, y = 1:n)\nspat_domain$label <- 1:(n*n)\n\n\nHere is the spatial domain. It is a regular lattice with 16 areal units.\n\n\nCode\nggplot(spat_domain) +\n  geom_tile(aes(x, y), linewidth = 2, color = \"grey50\", fill=\"white\") +\n  geom_text(aes(x, y, label=label), size = 15) +\n  coord_fixed() + \n  theme_void()\n\n\n\n\n\nFigure 1: A spatial domain partitioned into a regular lattice with areal units labeled \\(1,...,n\\).\n\n\n\n\nNext we want to define a neighborhood matrix for this regular lattice. A small digression, the convention used to label the areal units will impact the structure of this matrix. Is there a “best” structure? That remains to be seen. For now, let’s stick with the adjacency matrix that arises from the ordering in figure above. Adjacency matrices are abundant in graph theory. We’ll use the package igraph to construct the adjacency matrix for the areal units above. This is accomplished by first using igraph to create a 4 \\(\\times\\) 4 lattice graph, then using the as_adjacency_matrix function to convert the graph object to a sparse matrix.\n\n\nCode\nspat_domain_g <- make_lattice(c(n,n), mutual = TRUE)\nW <- as_adjacency_matrix(spat_domain_g, sparse=1)\nW\n\n\n16 x 16 sparse Matrix of class \"dgCMatrix\"\n                                     \n [1,] . 1 . . 1 . . . . . . . . . . .\n [2,] 1 . 1 . . 1 . . . . . . . . . .\n [3,] . 1 . 1 . . 1 . . . . . . . . .\n [4,] . . 1 . . . . 1 . . . . . . . .\n [5,] 1 . . . . 1 . . 1 . . . . . . .\n [6,] . 1 . . 1 . 1 . . 1 . . . . . .\n [7,] . . 1 . . 1 . 1 . . 1 . . . . .\n [8,] . . . 1 . . 1 . . . . 1 . . . .\n [9,] . . . . 1 . . . . 1 . . 1 . . .\n[10,] . . . . . 1 . . 1 . 1 . . 1 . .\n[11,] . . . . . . 1 . . 1 . 1 . . 1 .\n[12,] . . . . . . . 1 . . 1 . . . . 1\n[13,] . . . . . . . . 1 . . . . 1 . .\n[14,] . . . . . . . . . 1 . . 1 . 1 .\n[15,] . . . . . . . . . . 1 . . 1 . 1\n[16,] . . . . . . . . . . . 1 . . 1 .\n\n\nWith the spatial domain defined and partitioned, we can continue by simulating spatial data. The most basic case assumes spatial independence.Let’s also make it a bit more interesting by bumping up the number of areal units. Now it will probably be a lot easier to spot spatial dependence by a plot of the data alone.\n\n\nCode\nn <- 70\nspat_domain <- expand.grid(x = 1:n, y = 1:n)\nspat_domain$z <- rnorm(n^2, mean = 0, sd = 1)\nggplot(spat_domain) +\n  geom_tile(aes(x, y, fill=z)) +\n  scale_y_reverse() +\n  scale_fill_gradientn(colors = viridis(10)) +\n  coord_fixed() + \n  theme_void()\n\n\n\n\n\nWe want to simulate data with spatial dependence. We can do this from the CAR model.\nLet’s turn to working with the CAR model through a simple example. Let\n\\[\nZ_i =  \\mathbf{x}^T_i\\beta + \\phi_i + \\varepsilon_i\n\\] Here we have a covariate vector \\(\\mathbf{x}_i\\) indexed by spatial location \\(i\\), \\(\\phi_i\\) is a spatial random effect and \\(\\varepsilon_i\\) is a random error associated with the measurement at location \\(i\\) (later we assume to be normal with zero mean and constant variance). The defining characteristic of the CAR model is to specify a spatial structure through the conditional distributions of \\(\\phi_i\\) accordingly\n\\[\n\\phi_i|\\phi_{j, \\; j \\ne i} \\sim \\text{N}\\left(\\textstyle{\\frac{1}{n-1}\\sum}_{j \\ne i} \\phi_j, \\tau^2_i\\right)\n\\] That is the conditional mean of \\(\\phi_i\\) is just the average of the spatial random effects across all other locations. That being said, it’s probably not reasonable to condition on ALL other locations. Paraphrasing Tobler’s First Law of Geography, “everything is related to everything else, but near things are more related than distant things.”\nPerhaps we don’t need to condition on “distant things.” Instead we’ll condition on only the locations we’ve defined as the neighbors of location \\(i\\). Let \\(\\mathcal{N}_i\\) be the set of locations that are considered neighbors with location \\(i\\). Then we specify the conditional distribution as\n\\[\n\\phi_i|\\phi_{j, j \\in \\mathcal{N}_i} \\sim \\text{N}\\left(\\textstyle{\\frac{1}{|\\mathcal{N}_i|}\\sum}_{j \\in \\mathcal{N}_i} \\phi_j, \\tau^2_i\\right)\n\\] Practically, there are too many parameters in this model. Namely we have specified a location-specific variance, \\(\\tau^2_i\\), in each conditional distribution. We can simplify the model by specifying the conditional variance as a function of a parameter shared across locations and the number of neighbors of a given location. This structure is intuitive because we would expect the conditional variance to decrease as the number of neighbors increases. The new conditional distributions are specified as\n\\[\n\\phi_i|\\phi_{j, j \\in \\mathcal{N}_i} \\sim \\text{N}\\left(\\textstyle{\\frac{1}{|\\mathcal{N}_i|}\\sum}_{j \\in \\mathcal{N}_i} \\phi_j, \\frac{\\tau^2}{|\\mathcal{N}_i|}\\right)\n\\] At the end of the day it is possible to write the joint distribution of the spatial random effects from the conditional distributions. This is called compatibility and note that it is not guaranteed! Let \\(\\boldsymbol\\phi = (\\phi_i,\\dots,\\phi_n)\\), \\(\\mathbf{M}\\) be an \\(n \\times n\\) diagonal matrix containing the number of neighbors for each spatial location on its diagonal, and again \\(\\mathbf{W}\\) is our proximity matrix we defined earlier. Finally, we need to introduce another parameter \\(\\rho\\) to ensure that the distribution is proper (details in the BCG 2003).\n\\[\n\\boldsymbol\\phi \\sim \\text{N}\\left(\\boldsymbol{0}, \\tau^2(\\mathbf{M} - \\rho \\mathbf{W})^{-1}\\right)\n\\] The matrix \\(\\mathbf{M}\\) is fairly easy to obtain. Its diagonal is just the row sums from \\(\\mathbf{W}\\) and all other terms set to 0.\nLet’s try to simulate some data from this model. First we’ll define the spatial domain.\n\n\nCode\nn      <- 70\nnsites <- n^2\nspat_domain <- expand.grid(x = 1:n, y = 1:n)\nspat_domain$label <- 1:nsites\nspat_domain_g <- make_lattice(c(n,n), mutual = TRUE)\nW <- as_adjacency_matrix(spat_domain_g, sparse=1)\n\n\nNext define some parameters and draw from the spatial random effects distribution.\n\n\nCode\ntau2 <- 5\nrho <- 0.99\nM   <- diag(rowSums(W))\nspat_prec <- (1/tau2)*(M-rho*W) # swap this with something else. . .\nspat_domain$phi <- backsolve(chol(spat_prec), matrix(rnorm(nsites), nrow = nsites))\n# spat_cov <- tau^{-2}*solve(M-rho*W)\n# \n# phi <- MASS::mvrnorm(1, mu = rep(0,n^2), Sigma = spat_cov)\n\n\nNext sample the observations from the data distribution.\n\n\nCode\nX    <- rep(1, nsites)\nbeta <- matrix(c(2), nrow = 1)\nsigma2 <- 5\nspat_domain$z  <- rnorm(nsites, mean = X%*%beta + spat_domain$phi, sd = sqrt(sigma2))\n\n\nLet’s generate some plots. First, the spatial random effects, then the observations.\n\n\nCode\nggplot(spat_domain) +\n  geom_tile(aes(x, y, fill=phi)) +\n  scale_y_reverse() +\n  scale_fill_gradientn(colors = viridis(10)) +\n  coord_fixed() + \n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(spat_domain) +\n  geom_tile(aes(x, y, fill=z)) +\n  scale_y_reverse() +\n  scale_fill_gradientn(colors = viridis(10)) +\n  coord_fixed() + \n  theme_void()\n\n\n\n\n\n\n\n\n\nSo we’ve simulated some data from the CAR model and it seems fairly clear that the measurements are spatially correlated. Now we want to fit a CAR model to this data and estimate the parameters, \\(\\boldsymbol\\theta = (\\beta, \\sigma^2, \\tau^2, \\rho)\\).\nWe can do this using MCMC. First, let’s summarize the hierarchical model.\n\\[\\begin{align*}\nZ_i &\\sim \\text{N}\\left(x^T_i\\boldsymbol\\beta + \\phi_i, \\; \\sigma^2\\right)\\\\\n\\mu &\\sim \\text{N}\\left(0, \\lambda^2\\right)\\\\\n\\phi_i|\\phi_{j \\in \\mathcal{N}_i} &\\sim \\text{N}\\left(\\frac{\\rho}{m_i}\\sum_{j\\in \\mathcal{N}_i} \\phi_j, \\frac{\\tau^2}{m_i} \\right)\\\\\n\\sigma^2 &\\sim \\text{IG}\\left(a, b\\right)\\\\\n\\tau^2 &\\sim \\text{IG}\\left(a,b\\right)\\\\\n\\rho &\\sim \\text{Unif}\\left(0,1\\right)\n\\end{align*}\\]\nWe’ve stated the model using conditional distributions of \\(\\phi_i\\), though we learned earlier that it is possible to write the joint distribution of \\(\\boldsymbol\\phi\\). If we do this, we will at some point to need invert a very large matrix in order to sample from the full conditional for \\(\\boldsymbol\\phi\\). Instead it might be faster to cycle through the full conditionals of \\(\\phi_i\\) for each \\(i\\).\nMost of this model can be implemented using Gibbs sampling, except when sampling the \\(\\rho\\) parameter. We’ll need to use a Metropolis-Hastings step for that. The full conditionals are as follows,\n\\[\\begin{align*}\n\\boldsymbol\\beta|\\text{rest} &\\sim \\text{N}\\left(\\mathbf{B}^{-1}\\mathbf{A}, \\mathbf{B}^{-1}\\right)\\\\\n\\mathbf{A} &= \\sigma^{-2} \\mathbf{X}^T(Z - \\boldsymbol\\phi)\\\\\n\\mathbf{B} &= \\sigma^{-2}\\mathbf{X}^T\\mathbf{X} + \\lambda^{-2}\\mathbf{I}\\\\\n\\\\\\\\\n\\phi_i|\\text{rest} &\\sim \\text{N}\\left(\\frac{A}{B}, \\frac{1}{B}\\right)\\\\\nA &= \\frac{\\rho}{\\tau^2}\\sum_{j \\in \\mathcal{N}_i}\\phi_j + \\frac{1}{\\sigma^2}(Z_i - x_i^T\\boldsymbol\\beta)\\\\\nB &= \\frac{m_i}{\\tau^2} + \\frac{1}{\\sigma^2}\\\\\n\\\\\\\\\n\\sigma^2|\\text{rest} &\\sim \\text{IG}\\left(A, B \\right)\\\\\nA &= a + \\frac{n}{2}\\\\\nB &= b + \\frac{1}{2}(Z-\\mathbf{X}\\boldsymbol\\beta-\\phi)^T(Z-\\mathbf{X}\\boldsymbol\\beta-\\boldsymbol\\phi)\\\\\n\\\\\\\\\n\\tau^2|\\text{rest} &\\sim \\text{IG} \\left(A, B\\right)\\\\\nA &= a + \\frac{n}{2}\\\\\nB &= b + \\frac{1}{2}\\sum_{i=1}^n m_i\\left(\\phi_i - \\frac{\\rho}{m_i}\\sum_{j \\in \\mathcal{N}_i}\\phi_j\\right)^2\\\\\n\\\\\\\\\np(\\rho|\\text{rest}) &\\propto \\left[\\prod_{i=1}^n p(\\phi_i|\\phi_{j, j\\in\\mathcal{N}_i}, \\rho)\\right]p(\\rho)\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2\\tau^2} \\sum_{i=1}^n m_i \\left(\\phi_i - \\frac{\\rho}{m_i}\\sum_{j \\in \\mathcal{N}_i} \\phi_j \\right)^2\\right\\} \\mathbf{1}\\left\\{\\rho \\in [0,1]\\right\\}\n\\end{align*}\\]\nClearly we are unable to sample directly from the full conditional for \\(\\rho\\). Instead we will need to implement a Metropolis-Hasting step. We will use a truncated normal proposal distribution from the pacakge extraDistr to match the support of \\(\\rho\\). This also let’s us “ignore” the indicator function in the uniform prior because we will never propose a candidate value outside of \\([0,1]\\).\nAs an aside, we will need to routinely compute averages of neighboring spatial random effects at each location. Rather than extract neighbor information from a large neighbor matrix, we define ineighbors as a list of vectors containing the neighbor indices for each location. Additionally we create nneighbors as a vector containing the number of neighbors for each location. These two objects together should give us all we need to efficiently sample full conditionals.\n\n\nCode\nn <- 10\nnsites <- n^2\nspat_domain <- expand.grid(x = 1:n, y = 1:n)\nspat_domain$label <- 1:(n*n)\nspat_domain_g <- make_lattice(c(n,n), mutual = TRUE)\nW <- as_adjacency_matrix(spat_domain_g, sparse=1)\nineighbors <- apply(W, MARGIN = 1, FUN = function(x) which(x==1))\nnneighbors <- rowSums(W)\n\n\nFirst, simulate a small data set.\nNext sample the spatially independent and spatially dependent covariates.\n\n\nCode\n# spatially independent\n# x1 <- rnorm(nsites, mean = 0, sd = 1)\nX <- matrix(1, nrow = nsites)\n# spatial random effect\ntau20 <- 4\nrho0  <- 0.99\nM     <- diag(rowSums(W))\nspat_prec <- (1/tau20)*(M-rho0*W) # swap this with something else. . .\nspat_domain$phi <- backsolve(chol(spat_prec), matrix(rnorm(nsites), nrow = nsites))\nphi0 <- spat_domain$phi\n\n\nNext sample the observations from the data distribution.\n\n\nCode\nbeta0   <- 2\nsigma20 <- 0.25\nspat_domain$z  <- rnorm(nsites, mean = X%*%beta0 + spat_domain$phi, sd = sqrt(sigma20))\nz <- matrix(spat_domain$z, nrow = nsites)\n\n\n\n\nCode\nnpars  <- length(beta)\nnsites <- n^2\nniters <- 5000\nburn   <- 1000\nkeep_beta   <- matrix(NA, nrow = niters, ncol = npars)\nkeep_phi    <- matrix(NA, nrow = niters, ncol = nsites)\nkeep_sigma2 <- rep(NA, niters)\nkeep_tau2   <- rep(NA, niters)\nkeep_rho    <- rep(NA, niters)\n\n# initial values\nbeta   <- keep_beta[1,]  <- 2 #beta0\nphi    <- keep_phi[1,]   <- rep(10, nsites)\nsigma2 <- keep_sigma2[1] <- 2   #sigma20\ntau2   <- keep_tau2[1]   <- 5  # tau20\nrho    <- keep_rho[1]    <- rho0\n\n# prior parameters\na <- 0.1\nb <- 0.1\nlambda2 <- 10000\n\n# Metropolis-Hastings\natt <- 0\nacc <- 0\nMH  <- 0.1\n\n# pre-computes\nXtX <- t(X)%*%X\n\n\n## TODO:\n##    - review rho_loglike funtion.\n# rho_loglike <- function(rho, phi, ineighbors, nneighbors){\n# \n#   sneighbors <- sapply(ineighbors, FUN = function(x) sum(phi[x]))\n#   aneighbors <- sneighbors/nneighbors\n#   \n#   t1 <- sum(nneighbors*phi^2)\n#   t2 <- 2*rho*sum(phi*sneighbors)\n#   t3 <- (rho^2)*sum(nneighbors*(sneighbors^2))\n#   \n#   return(t1 - t2 + t3)\n# }\n\n\ntic()\nfor(iter in 2:niters){\n  \n  # sample mu [Gibbs]\n  A     <- (1/sigma2)*(t(X)%*%(z-phi))\n  B_inv <- solve((1/sigma2)*XtX + (1/lambda2)*diag(npars))\n  beta  <- B_inv%*%A+t(chol(B_inv))%*%rnorm(npars)\n  # beta <- beta0\n  \n  # sample phi [Gibbs]\n  for (site in 1:nsites){\n    A         <- (rho/tau2)*sum(phi[ineighbors[[site]]]) + \n                 (1/sigma2)*(z[site] - X[site,]%*%beta)\n    B_inv     <- 1/(nneighbors[site]/tau2 + 1/sigma2) \n    phi[site] <- rnorm(1, mean = B_inv*A, sd = sqrt(B_inv))\n  }\n  # phi <- phi0\n  \n  # sample sigma2 [Gibbs]\n  A      <- a + (nsites/2)\n  B      <- b + (1/2)*sum((z - X%*%beta - phi)^2)\n  sigma2 <- 1/rgamma(1, A, B)\n  # sigma2 <- sigma20\n  \n  # sample tau2 [Gibbs]\n  aneighbors <- sapply(ineighbors, FUN = function(x) sum(phi[x]))/nneighbors\n  A    <- a + (nsites/2)\n  B    <- b + (1/2)*sum(nneighbors*(phi - rho*aneighbors)^2)\n  tau2 <- 1/rgamma(1, A, B)\n  # tau2 <- tau20\n  \n  ## TODO:\n  ##    - review rho M-H step. Fix rho for now.\n  ## sample rho [M-H]\n  # att <- att + 1 \n  # can <- rtnorm(1, rho, MH, a = 0, b = 1)\n  # R   <- rho_loglike(can, phi, ineighbors, nneighbors) - # Likelihood\n  #        rho_loglike(rho, phi, ineighbors, nneighbors) +\n  #        dtnorm(rho, can , a = 0, b = 1, log = T) -      # M-H adjustment\n  #        dtnorm(can, rho , a = 0, b = 1, log = T)\n  # if(log(runif(1)) < R){\n  #   acc <- acc + 1\n  #   rho <- can\n  # }\n  # rho <- rho0\n  # \n  # # tuning\n  # if(iter < burn){\n  #   if(att > 50){\n  #     if(acc/att < 0.3){MH <- MH*0.8}\n  #     if(acc/att > 0.6){MH <- MH*1.2}\n  #     acc <- att <- 0\n  #   }\n  # }\n  rho <- rho0\n  \n  # storage\n  keep_beta[iter,]  <- beta\n  keep_phi[iter,]   <- phi\n  keep_sigma2[iter] <- sigma2\n  keep_tau2[iter]   <- tau2\n  keep_rho[iter]    <- rho\n  \n}\ntoc()\n\n\n4.64 sec elapsed\n\n\nNow let’s inspect some trace plots.\n\n\nCode\nwin = 1:niters\nplot(win, keep_beta[win,1], type = \"l\")\nabline(h = beta0[1], col = \"red\")\n\n\n\n\n\nCode\nplot(win, keep_sigma2[win], type = \"l\")\nabline(h = sigma20, col = \"red\")\n\n\n\n\n\nCode\nplot(win, keep_tau2[win], type = \"l\")\nabline(h = tau20, col = \"red\")\n\n\n\n\n\nCode\nplot(win, keep_rho[win], type = \"l\")\nabline(h = rho0, col = \"red\")\n\n\n\n\n\nCheck trace plots for \\(\\phi\\). 10 random locations.\n\n\nCode\nsampled_sites <- sample(1:nsites, size = 10)\nfor (site in sampled_sites){\n  plot(win, keep_phi[win, site] + keep_beta[win,1], type = \"l\")\n  abline(h = phi0[site], col = \"red\")\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere’s clearly something wrong. I’ll need to revisit this and verify the derivations and/or debug the code.\nWhat if we compared to the CARBayes package? Note that CARBayes uses a modified version of the model proposed by Leroux (cite)\n\n\nCode\n# convert neighbord matrix from sparse dgCMatrix format.\nW <- as.matrix(W)\n# fit model from CARBayes.\ncb.model <- S.CARleroux(z~1, family = \"gaussian\", \n                        W=W, burnin = 1000, n.sample = 5000, \n                        rho = rho0, verbose = FALSE)\ncb.model\n\n\n\n#################\n#### Model fitted\n#################\nLikelihood model - Gaussian (identity link function) \nRandom effects model - Leroux CAR\nRegression equation - z ~ 1\nNumber of missing observations - 0\n\n############\n#### Results\n############\nPosterior quantities and DIC\n\n              Mean   2.5%  97.5% n.effective Geweke.diag\n(Intercept) 2.9989 2.8790 3.1165      4675.6        -1.1\nnu2         0.2900 0.0034 1.0582        25.2         0.5\ntau2        3.3438 1.0646 5.3418        31.6        -0.4\nrho         0.9900 0.9900 0.9900          NA          NA\n\nDIC =  27.7571       p.d =  -26.96194       LMPL =  -114.25 \n\n\nLet’s look at the CARbayes samples…\n\n\nCode\nplot(cb.model$samples$beta)\n\n\n\n\n\nCode\nplot(cb.model$samples$nu2)\n\n\n\n\n\nCode\nplot(cb.model$samples$tau2)\n\n\n\n\n\nCode\n# plot(cb.model$samples$phi)"
  },
  {
    "objectID": "research/2023-02-02-Step-8-Simulating-Multivariate-Areal-Data/index.html",
    "href": "research/2023-02-02-Step-8-Simulating-Multivariate-Areal-Data/index.html",
    "title": "Step 8 - Simulating Multivariate Areal Data (scrapped, ref step 9)",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(viridis)"
  },
  {
    "objectID": "research/2023-02-02-Step-8-Simulating-Multivariate-Areal-Data/index.html#intro",
    "href": "research/2023-02-02-Step-8-Simulating-Multivariate-Areal-Data/index.html#intro",
    "title": "Step 8 - Simulating Multivariate Areal Data (scrapped, ref step 9)",
    "section": "Intro",
    "text": "Intro\nWe are going to examine the spatial CAR model in the context of the land surface phenology problem.\nConsider a spatial domain \\(\\mathcal{D}\\) partitioned into a regular lattice of areal units \\(s \\in \\mathcal{D}\\). Then, for a single year, each areal unit will have associated with it a “greenness” curve that is a description of that areal unit’s land surface phenology over the course of the year. The true greenness curve is the object of study. We observe the curve at only a finite number of points according to the capabilities of the collecting satellite. The goal is to recover this curve from these finite points.\nWe model this greenness curve with the so called “double-logistic” function, \\(\\nu(t,\\boldsymbol\\theta)\\), a function of time that is parameterized by the vector \\(\\boldsymbol\\theta = (\\theta_1,\\dots,\\theta_p)\\). Here, \\(p = 7\\). Note: link to previous post mentioning \\(\\nu\\).\nIt is difficult to identify the double-logistic function best characterizing the greenness curve for a single site and single year in isolation due to a paucity of data. Such an approach also neglects the spatial dependence across sites in the domain.\nInstead, we should draw information from other observations that are adjacent in either space or time (on a year-to-year scale).\nFor now, let’s consider modeling the spatial dependence. Fix ourselves to a snapshot in time, a single year, and consider the spatial domain \\(\\mathcal{D}\\) described above. Suppose that in some earlier stage of the analysis, data was aggregated to allow for a estimate to be made for \\(\\boldsymbol\\theta\\) which we will refer to as \\(\\boldsymbol\\theta_{\\mathcal{D}}\\). The result is a “region mean”."
  },
  {
    "objectID": "research/2023-02-02-Step-8-Simulating-Multivariate-Areal-Data/index.html#a-basic-simulation",
    "href": "research/2023-02-02-Step-8-Simulating-Multivariate-Areal-Data/index.html#a-basic-simulation",
    "title": "Step 8 - Simulating Multivariate Areal Data (scrapped, ref step 9)",
    "section": "A basic simulation",
    "text": "A basic simulation\nIn this example, we abstract away the data layer of the model and focus on the “deviation parameters”, \\(\\delta_{sj}\\).\nRecall that \\(\\delta_{sj}\\) is a scalar parameter describing the deviation of the \\(j\\)th parameter at location \\(s\\) from the regional average model, \\((\\boldsymbol\\theta_{\\mathcal{D}})_j\\). This is a product of the linearization of the double-logistic function with respect to \\(\\boldsymbol\\theta\\).\nWe will arrange these parameters into vectors in a few different ways. First, let \\(\\boldsymbol\\delta_s\\) be the \\(p \\times 1\\) vector of parameters associated with location \\(s \\in \\mathcal{D}\\). Here \\(\\boldsymbol\\delta_s\\) describes the deviation of the entire phenology curve at location \\(s\\) from the regional average phenology curve. Next, let \\(\\boldsymbol\\delta_{\\cdot j}\\) be the \\(n \\times 1\\) vector of the \\(j\\)th parameter from \\(\\boldsymbol\\delta_s\\) for all \\(s = 1,\\dots,n\\).\nLet the spatial domain be a regular square lattice with \\(n=25\\) areal units.\n\n\nCode\nn      <- 5\nnsites <- n^2\nspat_domain <- expand.grid(x = 1:n, y = 1:n)\nspat_domain$label <- 1:(n*n)\nspat_domain_g <- make_lattice(c(n,n), mutual = TRUE)\nW <- as.matrix(as_adjacency_matrix(spat_domain_g, sparse=1))\nM <- diag(rowSums(W))\n\nggplot(spat_domain) +\n  geom_tile(aes(x, y), linewidth = 2, color = \"grey50\", fill=\"white\") +\n  geom_text(aes(x, y, label=label), size = 15) +\n  coord_fixed() + \n  theme_void()\n\n\n\n\n\nFirst, we sample \\(\\boldsymbol\\delta_{\\cdot j}\\) from a univariate CAR model for each \\(j = 1,\\dots,p\\). Let \\(\\mathbf{W}\\) be the proximity matrix defining the neighbor sites of each site, \\(\\mathbf{M}\\) be a diagonal matrix where the diagonal contains the number of neighboring sites, \\(\\rho\\) be a parameter controlling the strength of spatial association, and \\(\\tau^2_j\\) be [insert interpretation]. Note that \\(\\rho\\) is shared across the \\(j\\) elements and \\(\\tau^2_j\\) is specific to the \\(j\\)th element of \\(\\boldsymbol\\delta_s\\).\n\\[\n\\boldsymbol\\delta_{\\cdot j} \\sim \\text{N}(\\boldsymbol0, \\tau_j^2(\\mathbf{M}-\\rho\\mathbf{W})^{-1})\n\\]\nThis induces spatial dependence independently for each \\(j\\) element of \\(\\boldsymbol\\delta_s\\).\n\n\nCode: univariate CAR samples\np    <- 7\ntau2 <- c(1,1,1,1,1,1,1)\nrho  <- 0.99\nspat_re <- matrix(NA, nrow = nsites, ncol = p)\n\nfor (i in 1:p){\n  spat_prec   <- (1/tau2[i])*(M-rho*W)\n  spat_domain[, ncol(spat_domain) + 1] <- backsolve(chol(spat_prec), \n                                                    matrix(rnorm(nsites), \n                                                           nrow = nsites))\n  colnames(spat_domain)[ncol(spat_domain)] <- paste0(\"phi\", i)\n}\n\nspat_domain <- gather(spat_domain, key = \"phi\", value = \"value\", -c(x,y,label))\n\n\n\n\nCode\nggplot(spat_domain) +\n  geom_tile(aes(x, y, fill=value)) +\n  scale_fill_gradientn(colors = viridis(10)) +\n  coord_fixed() +  \n  theme_void() +\n  facet_wrap(~phi)\n\n\n\n\n\nI’m thinking this might not be a great idea. . . shouldn’t we expect the spatial patterns to be related somehow? Yes, I think so. . , but I believe this will require either the matrix normal distribution or a very large kronecker product."
  },
  {
    "objectID": "research/2023-02-03-Step-9-The Multivariate CAR Model/index.html",
    "href": "research/2023-02-03-Step-9-The Multivariate CAR Model/index.html",
    "title": "Step 9 - The multivariate CAR model with common strength of spatial correlation",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(viridis)"
  },
  {
    "objectID": "research/2023-02-03-Step-9-The Multivariate CAR Model/index.html#intro",
    "href": "research/2023-02-03-Step-9-The Multivariate CAR Model/index.html#intro",
    "title": "Step 9 - The multivariate CAR model with common strength of spatial correlation",
    "section": "Intro",
    "text": "Intro\nHere we will extend the univariate CAR model to the multivariate case.\nConsider a spatial domain \\(\\mathcal{D} \\in \\mathbb{R}^2\\) that is partitioned into \\(n\\) areal units. The structure of the spatial domain is captured in the neighborhood matrix. Recall our definition of a neighborhood matrix \\(\\mathbf{W} = \\{w_{ij}\\}\\), where \\[\nw_{ij} =\n\\begin{cases}\n1 \\quad \\text{if} \\quad j \\in \\mathcal{N}(i),\\\\\n0 \\quad \\text{otherwise}.\n\\end{cases}\n\\] and by convention \\(w_{ii}=0\\). Further, define \\(w_{i+} = \\sum_{j=1}^pw_{ij}\\), i.e. the number of neighbors of location \\(i\\), \\(|\\mathcal{N}(i)|\\).\nIn the univariate case we specified a spatial random effect \\(\\boldsymbol\\phi = (\\phi_1,\\dots,\\phi_n)\\) meant to characterize spatial dependence. In the multivariate case, say of dimension \\(p\\), we specify a \\(p \\times 1\\) spatial random vector \\(\\boldsymbol\\phi_i = (\\phi_{i1}, \\phi_{i2},\\dots, \\phi_{ip})\\) at each location \\(i\\), \\(i = 1,\\dots,n\\). Arrange these vectors as rows in a matrix \\(\\boldsymbol\\Phi\\) \\[\n\\boldsymbol\\Phi =\n\\begin{pmatrix}\n\\boldsymbol\\phi_{1}\\\\\n\\boldsymbol\\phi_2\\\\\n\\vdots\\\\\n\\boldsymbol\\phi_n\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\phi_{11} & \\phi_{12} & \\dots & \\phi_{1p}\\\\\n\\phi_{21} & \\phi_{22} & \\dots & \\phi_{2p}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\phi_{n1} & \\phi_{n2} & \\dots & \\phi_{np}\\\\\n\\end{pmatrix}.\n\\]\nDefine \\(\\boldsymbol\\phi' = \\text{vec}(\\boldsymbol\\Phi^T)\\), i.e. stacked columns of \\(\\boldsymbol\\Phi^T\\). Here we will use \\(\\boldsymbol\\phi' = \\text{vec}(\\boldsymbol\\Phi^T)\\) to characterize spatial dependence.\nWe concern ourselves with the joint distribution \\(\\boldsymbol\\phi'\\) by specifying conditional distributions of \\(\\boldsymbol\\phi_i\\). Under the Markov Random Field (MRF) assumption, the conditional distributions of \\(\\boldsymbol\\phi_i\\) can be specified as \\[\np(\\boldsymbol\\phi_i|\\boldsymbol\\phi_{j\\ne i}, \\boldsymbol\\Gamma_i) = N\\left(\\sum_{i \\sim j} \\mathbf{B}_{ij}\\boldsymbol\\phi_j, \\boldsymbol\\Gamma_i\\right), \\quad i,j = 1,\\dots,n.\n\\]\nwhere \\(\\boldsymbol\\Gamma_i\\) and \\(\\mathbf{B}_{ij}\\) are \\(p \\times p\\) matrices. Here we use \\(i \\sim j\\) to say that \\(j\\) is in the neighborhood of \\(i\\), i.e. \\(j \\in \\mathcal{N}(i)\\).\nThe role of \\(\\boldsymbol\\Gamma_i\\) and \\(\\mathbf{B}_{ij}\\) are analogous to the roles of \\(\\tau_i^2\\) and \\(b_{ij}\\), respectively, in the univariate CAR models. The matrix \\(\\boldsymbol\\Gamma_i\\) is the within-location covariance matrix which describes the dependence of the variables in the vector \\(\\boldsymbol\\phi_i\\). The matrix \\(\\mathbf{B}_{ij}\\) is a matrix that allows us to weight observations from locations in the neighborhood of location \\(i\\). A convenient special case is to set \\(\\mathbf{B}_{ij} = b_{ij}\\mathbf{I}_p\\) where \\(b_{ij} = w_{ij}/w_{i+}\\), i.e. the neighboring observations are equally weighted.\nSpecifying the conditional distributions in this way implies the unique joint distribution, via Brook’s Lemma, to be \\[\np(\\boldsymbol\\phi' \\;| \\;\\{\\boldsymbol\\Gamma_i\\}) \\propto \\exp\\left\\{-\\frac{1}{2}\\boldsymbol\\phi^T\\boldsymbol\\Gamma^{-1}(\\mathbf{I}_{np} - \\tilde{\\mathbf{B}})\\phi\\right\\}\n\\] where \\(\\boldsymbol\\Gamma\\) is block-diagonal with block \\(\\boldsymbol\\Gamma_i\\), and \\(\\tilde{\\mathbf{B}}\\) is \\(np \\times np\\) with the \\((i,j)\\)-th block \\(\\mathbf{B}_{ij}\\).\nOf course, the conditions for a proper multivariate normal distribution are symmetry and positive definiteness of \\(\\boldsymbol\\Gamma^{-1}(\\mathbf{I}_{np} - \\tilde{\\mathbf{B}})\\).\nSetting \\(\\mathbf{B}_{ij} = b_{ij}\\mathbf{I}_p\\) where \\(b_{ij} = w_{ij}/w_{i+}\\) leads to the symmetry condition \\(b_{ij}\\boldsymbol\\Gamma_j = b_{ji}\\boldsymbol\\Gamma_i\\).\nFurther, a common simplifying assumption would be to consider equal within-location covariance. That is, set \\(\\boldsymbol\\Gamma_i = w_{i+}^{-1}\\boldsymbol\\Lambda\\) where \\(\\boldsymbol\\Lambda\\) is the \\(p \\times p\\) within-location covariance matrix for \\(\\boldsymbol\\phi_i\\) common across spatial locations \\(i=1,\\dots,n\\).\nUnder these assumptions we can write \\(\\boldsymbol\\Gamma = (\\mathbf{D}^{-1}\\otimes\\boldsymbol\\Lambda)\\) where \\(\\mathbf{D}\\) is an \\(n \\times n\\) diagonal matrix with \\(\\mathbf{D}_{ii} = w_{i+}\\). And also \\(\\tilde{\\mathbf{B}} = \\mathbf{B}\\otimes\\mathbf{I}_p\\) where \\(\\mathbf{B}\\) is a \\(n \\times n\\) matrix and elements of \\(\\mathbf{B}\\) are \\(\\mathbf{B} = \\{b_{ij}\\} = \\{w_{ij}/w_{i+}\\}\\), not to be confused with the block matrix specification \\(\\mathbf{B}_{ij}\\) given previously.\nWith this in mind, we can write the precision of the above joint distribution in an alternate form,\n\\[\\begin{align*}\n\n\\Gamma^{-1}(\\mathbf{I}_{np} - \\tilde{\\mathbf{B}}) &= (\\mathbf{D}^{-1} \\otimes \\boldsymbol\\Lambda)^{-1}(\\mathbf{I}_{np} - \\mathbf{B}\\otimes \\mathbf{I}_p)\\\\\n&= (\\mathbf{D} \\otimes \\boldsymbol\\Lambda^{-1})(\\mathbf{I}_{np} - \\mathbf{B}\\otimes \\mathbf{I}_p) & \\text{Kronecker Prod - Inverse}\\\\\n&= (\\mathbf{D} \\otimes \\boldsymbol\\Lambda^{-1}) - (\\mathbf{D} \\otimes \\boldsymbol\\Lambda^{-1})( \\mathbf{B}\\otimes \\mathbf{I}_p) & \\text{(Matrix Mult - distributive wrt matrix addition)}\\\\\n&= (\\mathbf{D} \\otimes \\boldsymbol\\Lambda^{-1}) - (\\mathbf{D}\\mathbf{B}) \\otimes (\\boldsymbol\\Lambda^{-1}\\mathbf{I}_p) & \\text{(Kronecker Prod - mixed-product property)}\\\\\n&= (\\mathbf{D} - \\mathbf{D}\\mathbf{B})\\otimes\\boldsymbol\\Lambda^{-1} & \\text{(Kronecker Prod - distributive wrt matrix addition)}\\\\\n&= (\\mathbf{D} - \\mathbf{W}) \\otimes \\boldsymbol\\Lambda^{-1}\n\\end{align*}\\]\nwhere the last step follows from the fact that \\(\\mathbf{B} = \\mathbf{D}^{-1}\\mathbf{W}\\).\nNow there is no need to construct a large block-diagonal \\(\\boldsymbol\\Gamma\\) matrix. Also, the neighborhood matrix \\(\\mathbf{W}\\) and \\(\\mathbf{D}\\), the diagonal matrix of its row sums, are fixed and known quantities easily computed outside of any MCMC sampling loop. The joint distribution is then\n\\[\np(\\boldsymbol\\phi' \\;| \\boldsymbol\\Lambda) \\propto \\exp\\left\\{-\\frac{1}{2}\\boldsymbol\\phi^T\\left((\\mathbf{D} - \\mathbf{W})\\otimes \\boldsymbol\\Lambda^{-1}\\right)\\phi\\right\\}\n\\] or \\[\n\\boldsymbol\\phi'|\\boldsymbol\\Lambda \\sim N\\left(\\boldsymbol 0, (\\mathbf{D} - \\mathbf{W})^{-1} \\otimes \\boldsymbol\\Lambda\\right).\n\\]\nHowever, we again encounter the issue that \\((\\mathbf{D} - \\mathbf{W}) \\otimes \\boldsymbol\\Lambda^{-1}\\) is singular, since \\((\\mathbf{D} - \\mathbf{W})\\) is singular. This motivates the introduction of a scalar “spatial strength” parameter, \\(\\rho\\), also analogous to the univariate case. There are in fact more general conditions for recovering positive definiteness, but these are not explored here. More details can be found in. . .\nIntroducing \\(\\rho\\) results in a proper joint distribution with form\n\\[\np(\\boldsymbol\\phi' \\;| \\boldsymbol\\Lambda) \\propto \\exp\\left\\{-\\frac{1}{2}\\boldsymbol\\phi^T\\left((\\mathbf{D} - \\rho\\mathbf{W})\\otimes \\boldsymbol\\Lambda^{-1}\\right)\\boldsymbol\\phi\\right\\}\n\\] provided that \\(|\\rho|<1\\).\nThis model is denoted as \\(\\text{MCAR}(\\rho,\\boldsymbol\\Lambda)\\).\nLet’s simulate a simple scenario. Let the spatial domain be partitioned into a \\(40 \\times 40\\) regular lattice with \\(n = 1600\\) spatial locations. The regular lattice partition will induce a neighborhood matrix \\(\\mathbf{W}\\) and diagonal matrix \\(\\mathbf{D}\\) for the row sums of \\(\\mathbf{W}\\).\nLet \\(p = 2\\) so that we have a \\(p \\times 1\\) vector \\(\\boldsymbol\\phi_i\\) at each location \\(i = 1,\\dots,n\\). Next, specify the common within-location covariance matrix \\[\n\\boldsymbol\\Lambda =\n\\begin{pmatrix}\n1 & 0.9\\\\\n0.9 & 1\n\\end{pmatrix}\n\\] This way \\(\\text{Var}(\\phi_{i1}) = 1\\), \\(\\text{Var}(\\phi_{i2}) = 1\\), and \\(\\text{Corr}(\\phi_{i1}, \\phi_{i2}) = 0.9\\). Lastly, set \\(\\rho = 0.99\\) to induce strong spatial correlation. Here is a naive way to sample from the joint distribution for \\(\\boldsymbol\\phi'\\). It is naive because it does not take advantage of the sparsity of the precision matrix. We leave computational advantages to a later investigation.\n\n\nCode\nset.seed(831)\n\n# specify spatial domain\nn <- 40^2\nspat_domain <- expand.grid(x = 1:sqrt(n), y = 1:sqrt(n))\nspat_domain$label <- 1:n\n\nspat_domain_g <- make_lattice(c(sqrt(n),sqrt(n)), mutual = TRUE)\nW <- as_adjacency_matrix(spat_domain_g, sparse=0)\nD <- diag(rowSums(W))\n\n# specify parameters\np <- 2\nLambda <- matrix(c(1, 0.9,\n                   0.9, 1), byrow = T, ncol = p)\nrho <- 0.99\n\n# construct precision matrix for join dist of phi\ninv_Sigma <- kronecker((D-rho*W), solve(Lambda))\n\n# sample from joint dist of phi (technique borrowed for precision matrices)\nspat_phi <- backsolve(chol(inv_Sigma), rnorm(n*p))\n\n# inverse vectorize phi  \nspat_phi <- matrix(spat_phi, byrow = T, ncol = p)\n\n# assign values to spatial data set\nspat_domain$phi1 <- spat_phi[,1]\nspat_domain$phi2 <- spat_phi[,2]\n\n\nSince we specified this example with both strong spatial and strong within-location correlation, plotting the data for \\(\\phi_{i1}\\) and \\(\\phi_{i2}\\), we should expect to observe the same spatial correlation patterns for the first and second elements of \\(\\boldsymbol\\phi_i\\).\n\n\nCode\nggplot(spat_domain) +\n  geom_tile(aes(x, y, fill=phi1)) +\n  labs(fill = bquote(phi[i1])) +\n  scale_y_reverse() +\n  scale_fill_gradientn(colors = viridis(10), limits = c(-3.3,3.3)) +\n  coord_fixed() + \n  theme_void()\n\n\n\n\n\nFigure 1: Plot of…\n\n\n\n\n\n\nCode\nggplot(spat_domain) +\n  geom_tile(aes(x, y, fill=phi2)) +\n  labs(fill = bquote(phi[i2])) +\n  scale_y_reverse() +\n  scale_fill_gradientn(colors = viridis(10), limits = c(-3.3,3.3)) +\n  coord_fixed() + \n  theme_void()\n\n\n\n\n\nFigure 2: Plot of …\n\n\n\n\nGood news! It looks like we were successful in simulating from the MCAR model! For the next post, we will examine the case where \\(\\rho\\) is not shared among elements of \\(\\boldsymbol\\phi_i\\)"
  },
  {
    "objectID": "research/2023-03-06-Step-10-MCAR-different-rho/index.html",
    "href": "research/2023-03-06-Step-10-MCAR-different-rho/index.html",
    "title": "Step 10 - The multivariate CAR model with DIFFERENT strengths of spatial correlation (WIP)",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(viridis)"
  },
  {
    "objectID": "research/2023-03-06-Step-10-MCAR-different-rho/index.html#intro",
    "href": "research/2023-03-06-Step-10-MCAR-different-rho/index.html#intro",
    "title": "Step 10 - The multivariate CAR model with DIFFERENT strengths of spatial correlation (WIP)",
    "section": "Intro",
    "text": "Intro\nWe explored the multivariate CAR model is the previous post finishing with a simulation of data from the proper version of the model where the scalar parameter controlling the strength of spatial correlation, \\(\\rho\\) was common across elements of the multivariate response vector. This assumption may not always be reasonable. Here we will summarize one approach to introducing \\(p-1\\) additional scalar parameters for a total of \\(p\\) “strength of spatial correlation” parameters, one for each element of the multivariate response.\nConsider a spatial domain \\(\\mathcal{D} \\in \\mathbb{R}^2\\) that is partitioned into \\(n\\) areal units. The structure of the spatial domain is captured in the neighborhood matrix. Recall our definition of a neighborhood matrix \\(\\mathbf{W} = \\{w_{ij}\\}\\), where \\[\nw_{ij} =\n\\begin{cases}\n1 \\quad \\text{if} \\quad j \\in \\mathcal{N}(i),\\\\\n0 \\quad \\text{otherwise}.\n\\end{cases}\n\\] and by convention \\(w_{ii}=0\\). Further, define \\(w_{i+} = \\sum_{j=1}^pw_{ij}\\), i.e. the number of neighbors of location \\(i\\), \\(|\\mathcal{N}(i)|\\).\nIn the univariate case we specified a spatial random effect \\(\\boldsymbol\\phi = (\\phi_1,\\dots,\\phi_n)\\) meant to characterize spatial dependence. In the multivariate case, say of dimension \\(p\\), we specify a \\(p \\times 1\\) spatial random vector \\(\\boldsymbol\\phi_i = (\\phi_{i1}, \\phi_{i2},\\dots, \\phi_{ip})\\) at each location \\(i\\), \\(i = 1,\\dots,n\\). Arrange these vectors as rows in a matrix \\(\\boldsymbol\\Phi\\) \\[\n\\boldsymbol\\Phi =\n\\begin{pmatrix}\n\\boldsymbol\\phi_{1}\\\\\n\\boldsymbol\\phi_2\\\\\n\\vdots\\\\\n\\boldsymbol\\phi_n\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\phi_{11} & \\phi_{12} & \\dots & \\phi_{1p}\\\\\n\\phi_{21} & \\phi_{22} & \\dots & \\phi_{2p}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\phi_{n1} & \\phi_{n2} & \\dots & \\phi_{np}\\\\\n\\end{pmatrix}.\n\\]\nDefine \\(\\boldsymbol\\phi' = \\text{vec}(\\boldsymbol\\Phi^T)\\), i.e. stacked columns of \\(\\boldsymbol\\Phi^T\\). Here we will use \\(\\boldsymbol\\phi' = \\text{vec}(\\boldsymbol\\Phi^T)\\) to characterize spatial dependence. From the previous post, the proper joint distribution of \\(\\boldsymbol\\phi'\\) was\n\\[\n\\boldsymbol\\phi'|\\boldsymbol\\Lambda \\sim N\\left(\\boldsymbol 0, (\\mathbf{D} - \\rho\\mathbf{W})^{-1} \\otimes \\boldsymbol\\Lambda\\right).\n\\] where \\(\\mathbf{W}\\) is the neighborhood matrix and \\(\\mathbf{D}\\), the diagonal matrix of its row sums, \\(\\rho\\) is the scalar strength of spatial correlation parameter, and \\(\\boldsymbol\\Lambda\\) is the within-location covariance.\nOur goal is to re-specify this model to allow for different \\(\\rho_k\\), \\(k = 1,\\dots,p\\). One approach requires a re-arrangement of \\(\\boldsymbol\\phi'\\). Where previously \\(\\boldsymbol\\phi'\\) stacked \\(n\\) vectors of length \\(p\\) on top of each other, now consider \\(\\boldsymbol\\phi\\) (no prime) to stack \\(p\\) vectors of length \\(n\\). That is we collect the \\(n\\) instances of the \\(p\\)-th element of \\(\\boldsymbol\\phi_i\\), \\(i=1,\\dots,n\\). Both are \\(np \\times 1\\) vectors$ with the entries permuted.\nThis operation is essentially transforming the vectorization of the \\(n \\times p\\) matrix \\(\\boldsymbol\\Phi\\) from the vectorization of its transpose and is accomplished via a commutation matrix.\nWe can transform \\(\\boldsymbol\\phi' = \\text{vec}(\\Phi^T)\\) by applying the commutation matrix \\(\\mathbf{K}^{(p,n)}\\) for the result \\(\\mathbf{K}^{(p,n)}\\text{vec}(\\Phi^T) = \\text{vec}(\\Phi)\\). Given that\n\\[\n\\text{vec}(\\boldsymbol\\Phi^T) = \\boldsymbol\\phi' \\sim N\\left(\\boldsymbol 0, (\\mathbf{D}-\\rho\\mathbf{W})^{-1} \\otimes \\boldsymbol\\Lambda\\right)\n\\]\nthen the result of left multiplying by the commutation matrix yields a multivariate distribution with \\(\\text{E}[\\boldsymbol\\phi] = \\boldsymbol 0\\) and covariance matrix \\[\n\\begin{align*}\n\\text{Cov}(\\mathbf{K}^{(p,n)}\\boldsymbol\\phi')&= \\mathbf{K}^{(p,n)}\\text{Cov}(\\boldsymbol\\phi')\\left(\\mathbf{K}^{(p,n)}\\right)^T\\\\\n&= \\mathbf{K}^{(p,n)}\\left[(\\mathbf{D}-\\rho\\mathbf{W})^{-1} \\otimes \\boldsymbol\\Lambda\\right]\\left(\\mathbf{K}^{(p,n)}\\right)^T \\\\\n&=\\mathbf{K}^{(p,n)}\\left[(\\mathbf{D}-\\rho\\mathbf{W})^{-1} \\otimes \\boldsymbol\\Lambda\\right]\\mathbf{K}^{(n,p)}\\\\\n&=\\boldsymbol\\Lambda \\otimes(\\mathbf{D}-\\rho\\mathbf{W})^{-1}\n\\end{align*}\n\\] The effect is that the Kronecker product in the covariance is commuted, hence the name commutation matrix for \\(\\mathbf{K}^{(p,n)}\\). Then when the resulting Kronecker product is expanded, the scalar elements of the common non-spatial covariance matrix \\(\\boldsymbol\\Lambda\\) are multiplied with identical blocks \\((\\mathbf{D}-\\rho\\mathbf{W})\\),\n\\[\n\\boldsymbol\\Lambda \\otimes(\\mathbf{D}-\\rho\\mathbf{W})^{-1} =\n\\begin{pmatrix}\n\\lambda_{11} (\\mathbf{D}-\\rho\\mathbf{W})^{-1} & \\lambda_{12}(\\mathbf{D}-\\rho\\mathbf{W})^{-1} & \\dots & \\lambda_{1p}(\\mathbf{D}-\\rho\\mathbf{W})^{-1} \\\\\n\\lambda_{21} (\\mathbf{D}-\\rho\\mathbf{W})^{-1} & \\lambda_{22}(\\mathbf{D}-\\rho\\mathbf{W})^{-1} & \\dots & \\lambda_{2p}(\\mathbf{D}-\\rho\\mathbf{W})^{-1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\lambda_{p1} (\\mathbf{D}-\\rho\\mathbf{W})^{-1} & \\lambda_{p2}(\\mathbf{D}-\\rho\\mathbf{W})^{-1} & \\dots & \\lambda_{pp}(\\mathbf{D}-\\rho\\mathbf{W})^{-1} \\\\\n\\end{pmatrix}\n\\]\nwhere \\(\\lambda_{ij}\\) are the scalar elements of \\(\\boldsymbol\\Lambda\\).\nIt is actually more convenient to work with the precision matrix, \\[\n\\left[\\boldsymbol\\Lambda \\otimes(\\mathbf{D}-\\rho\\mathbf{W})^{-1}\\right]^{-1} = \\boldsymbol\\Lambda^{-1} \\otimes(\\mathbf{D}-\\rho\\mathbf{W})\n\\] For illustration, set \\(p = 2\\). Then the precision matrix has form\n\\[\n\\boldsymbol\\Lambda^{-1} \\otimes(\\mathbf{D}-\\rho\\mathbf{W}) =\n\\begin{pmatrix}\n\\lambda^{(-1)}_{11} (\\mathbf{D}-\\rho\\mathbf{W}) & \\lambda^{(-1)}_{12}(\\mathbf{D}-\\rho\\mathbf{W}) \\\\\n\\lambda^{(-1)}_{21} (\\mathbf{D}-\\rho\\mathbf{W}) & \\lambda^{(-1)}_{22}(\\mathbf{D}-\\rho\\mathbf{W})\n\\end{pmatrix}\n\\]\nwhere \\(\\lambda^{(-1)}_{ij}\\) are the scalar entries of \\(\\boldsymbol\\Lambda^{-1}\\).\nIt is in this form that we can introduce different \\(\\rho_k\\) for each of the \\(p\\) elements. One strategy is to consider the a singular value decomposition of \\(\\mathbf{D}-\\rho\\mathbf{W}\\). . .\nAnother representation of this precision is \\[\n\\boldsymbol\\Lambda^{-1} \\otimes(\\mathbf{D}-\\rho\\mathbf{W}) =\n\\begin{pmatrix}\n\\mathbf{U}^T_1 & \\mathbf{0} \\\\\n\\mathbf{0} & \\mathbf{U}^T_2\n\\end{pmatrix}\n(\\boldsymbol\\Lambda^{-1} \\otimes \\mathbf{I}_n)\n\\begin{pmatrix}\n\\mathbf{U}_1 & \\mathbf{0} \\\\\n\\mathbf{0} & \\mathbf{U}_2\n\\end{pmatrix}\n\\] where \\(\\mathbf{U}_1^T\\mathbf{U}_1 = \\mathbf{D}-\\rho_1\\mathbf{W}\\) and \\(\\mathbf{U}_2^T\\mathbf{U}_2 = \\mathbf{D}-\\rho_2\\mathbf{W}\\) WIP"
  },
  {
    "objectID": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html",
    "href": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html",
    "title": "Step 11 - A Bayesian HM for multivariate Guassian data and MCAR prior - derivation of full conditionals and comments on computing",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(viridis)"
  },
  {
    "objectID": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#intro",
    "href": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#intro",
    "title": "Step 11 - A Bayesian HM for multivariate Guassian data and MCAR prior - derivation of full conditionals and comments on computing",
    "section": "Intro",
    "text": "Intro\nIn this post we will derive full conditionals for a Bayesian hierarchical model with an MCAR prior specified in the second stage. For simplicity, we will consider the case common spatial strength parameter \\(\\rho\\).\nLet \\(\\mathcal{D}\\) be a spatial domain partitioned into \\(n\\) areal units indexed by \\(i\\). Following the notation of Step 3 (link), let \\(\\boldsymbol\\delta_i\\) be a multivariate (\\(p \\times 1\\)) response vector with conditional expectation \\[\n\\text{E}(\\boldsymbol\\delta_i \\,|\\,\\boldsymbol\\phi_i)  = \\boldsymbol{\\mathcal{Z}}_i \\boldsymbol\\beta + \\boldsymbol\\phi_i\n\\] and covariance \\[\n\\text{Cov}(\\boldsymbol\\delta_i \\,|\\, \\boldsymbol\\phi_i) = \\text{Cov}(\\boldsymbol\\epsilon_i) = \\boldsymbol\\Omega\n\\]\n\\[\n\\boldsymbol\\delta_i = \\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\boldsymbol\\phi_i + \\boldsymbol\\epsilon_i\n\\] where \\(\\boldsymbol\\epsilon_i \\sim \\text{N}\\left(\\boldsymbol 0, \\boldsymbol\\Omega\\right)\\) is a random noise vector.\nWe can specify the model with conditional priors on \\(\\boldsymbol\\phi{i}\\)\n\\[\n\\begin{align*}\n\\boldsymbol\\delta_i &\\sim \\text{Normal}_{p}\\left(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\boldsymbol\\phi_i, \\,  \\boldsymbol\\Omega\\right)\\\\\n\\boldsymbol\\beta &\\sim \\text{Normal}_{qp}(\\boldsymbol\\mu, \\, \\boldsymbol\\Gamma)\\\\\n\\boldsymbol\\phi_i|\\boldsymbol\\phi_{j\\ne i},\\boldsymbol\\Lambda &\\sim \\text{Normal}_p\\left(\\rho\\sum_{i \\sim j}\\mathbf{B}_{ij}\\boldsymbol\\phi_j, \\, w^{-1}_{i+}\\boldsymbol\\Lambda\\right)\\\\\n\\omega_{kk} &\\sim \\text{InvGamm}(a_k, \\, b_k) \\quad \\text{OR} \\quad \\boldsymbol\\Omega \\sim \\text{InvWish}(\\psi,\\mathbf{G})\\\\\n\\boldsymbol\\Lambda &\\sim \\text{InvWish}(\\nu, \\mathbf{R})\\\\\n\\rho &\\sim \\text{Unif}(0, \\,1)\n\\end{align*}\n\\]\nOr we can write the model in another way, with the joint distribution for \\(\\boldsymbol\\phi'\\), the vector formed by stacking \\(\\boldsymbol\\phi_i\\), \\(i = 1,\\dots,n\\).\n\\[\n\\begin{align*}\n\\boldsymbol\\delta_i &\\sim \\text{Normal}_p\\left(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\boldsymbol\\phi_i, \\;  \\boldsymbol\\Omega\\right)\\\\\n\\boldsymbol\\beta &\\sim \\text{Normal}_{qp}\\left(\\boldsymbol\\mu, \\boldsymbol\\Gamma\\right)\\\\\n\\boldsymbol\\phi' &\\sim \\text{Normal}_{np}\\left(\\boldsymbol 0, (\\mathbf{D-\\rho\\mathbf{W}})^{-1} \\otimes \\boldsymbol\\Lambda\\right)\\\\\n\\omega_{kk} &\\sim \\text{InvGamm}(a_k, b_k) \\quad \\text{OR} \\quad \\boldsymbol\\Omega \\sim \\text{InvWish}(\\psi,\\mathbf{G})\\\\\n\\boldsymbol\\Lambda &\\sim \\text{InvWish}(\\nu, \\mathbf{R})\\\\\n\\rho &\\sim \\text{Unif}(0,1)\n\\end{align*}\n\\]\nIn this specification we take elements of \\(\\boldsymbol\\delta_i\\) to be conditionally independent (i.e. \\(\\boldsymbol\\Omega\\) is a diagonal matrix) which in general need not be the case."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMar 7, 2023\n\n\nStep 11 - A Bayesian HM for multivariate Guassian data and MCAR prior - derivation of full conditionals and comments on computing\n\n\nMatthew Shisler\n\n\n\n\nMar 6, 2023\n\n\nStep 10 - The multivariate CAR model with DIFFERENT strengths of spatial correlation (WIP)\n\n\nMatthew Shisler\n\n\n\n\nMar 5, 2023\n\n\nStep 9 - The multivariate CAR model with common strength of spatial correlation\n\n\nMatthew Shisler\n\n\n\n\nFeb 10, 2023\n\n\nStep 8 - Simulating Multivariate Areal Data (scrapped, ref step 9)\n\n\nMatthew Shisler\n\n\n\n\nFeb 9, 2023\n\n\nStep 7 - Areal data and the spatial CAR model - Basic Example\n\n\nMatthew Shisler\n\n\n\n\nFeb 8, 2023\n\n\nStep 6 - Application to the (orignal) Double-logistic function\n\n\nMatthew Shisler\n\n\n\n\nFeb 7, 2023\n\n\nStep 5 - Verify Convergence\n\n\nMatthew Shisler\n\n\n\n\nFeb 6, 2023\n\n\nStep 4 - Application to the Linearized Double-Logistic Function\n\n\nMatthew Shisler\n\n\n\n\nFeb 5, 2023\n\n\nStep 3 - Add a Second Level Linear Trend\n\n\nMatthew Shisler\n\n\n\n\nFeb 4, 2023\n\n\nStep 2 - One Parent Design Matrix\n\n\nMatthew Shisler\n\n\n\n\nFeb 3, 2023\n\n\nStep 1 - Bayesian Hierarchical Linear Regression\n\n\nMatthew Shisler\n\n\n\n\nFeb 2, 2023\n\n\nStep 0 - Bayesian Linear Regression with Gibbs Sampling\n\n\nMatthew Shisler\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "learn/2023-03-11-Brooks-Lemma/index.html",
    "href": "learn/2023-03-11-Brooks-Lemma/index.html",
    "title": "Brook’s Lemma",
    "section": "",
    "text": "Here we will explore Brook’s Lemma and some of its implications and consequences. Brook’s lemma is particularly useful in the context of spatial statistics.\nConsider a discrete Markov Chain \\(\\{X_t, t \\in T \\}\\) with finite state space.\nRecall the Markov Property. The transition probabilities for \\(X_t\\) depend only on the previous state \\(X_{t-1}\\). One way to describe the Markov property in the context of a time-ordered evolving process is to say that, conditioned on the present, the past and future are independent. \\[\n\\Pr(X_t = s_t|X_{t-1}=s_{t-1},\\dots,X_0 = s_0) = \\Pr(X_t = s_t|X_{t-1}=s_{t-1})\n\\] where \\(s_t\\) is the state at stage \\(t\\).\nIn this setting, we can factor the joint probability mass/density function, denoted \\(p(\\,\\cdot\\,)\\), of \\(t\\) random variables, using the definition of conditional probability and the Markov Property. Given an initial state \\(X_0\\),\n\\[\n\\begin{align*}\np(X_t,X_{t-1},\\dots,X_1|X_0) &= p(X_{t}|X_{t-1},\\dots,X_1,X_0) \\cdot p(X_{t-1},\\dots,X_1|X_0)\\\\\n               &= p(X_{t}|X_{t-1},\\dots,X_1,X_0)\\cdot p(X_{t-1}|X_{t-2},\\dots,X_1,X_0)\\cdot p(X_{t-2},\\dots,X_1|X_0)\\\\\n               &\\quad \\vdots\\\\\n               &= p(X_{t}|X_{t-1},\\dots,X_1,X_0)\\cdot p(X_{t-1}|X_{t-2},\\dots,X_1,X_0)\\cdot \\dots \\cdot p(X_1|X_0)\\\\\n               &= p(X_{t}|X_{t-1})\\cdot p(X_{t-1}|X_{t-2})\\cdot \\dots \\cdot p(X_1|X_0)\\\\\n               &= \\prod_{j=1}^t p(X_j|X_{j-1})\n\\end{align*}\n\\] Above, in the first several equalities we apply the definition of conditional probability to write a \\(k\\)-variate joint probability density as the product of a univariate conditional and \\((k-1)\\)-variate (joint) marginal probability densities. In the penultimate line, we apply the Markov Property to conclude that each conditional probability depends only on the immediately preceding variate in the Markov chain. The last line puts the expression into product notation.\nThis approach works in the case of a “natural ordering” or “favored direction” of the Markov Chain. Commonly an ordering is implied by the idea that the stochastic process is evolving over time. The process starts at an initial state \\(X_0\\), then \\(X_1\\) follows \\(X_0\\), \\(X_2\\) follows \\(X_1\\), …, \\(X_t\\) follows \\(X_{t-1}\\), and so on. Under the Markov property it is not useful to specify a model through conditional probabilities of the form \\(P(X_{t-1}|X_t)\\) (past event conditioned on future event) because by time \\(t\\) the past event at time \\(t-1\\) will have already been “realized” in the stochastic process. That is the state at time \\(t\\) has no influence on the state at time \\(t-1\\) in the system.\nNote: conditional probabilities of the form \\(P(X_{t-1}|X_t)\\) might be interesting from the perspective of an observer who perceives the state of the system at time \\(t\\) and wishes to characterize the probability of the previous state.\nBut this natural ordering is not present in all stochastic processes. The most basic example might be the case where \\[\n\\Pr(X_t = s_t|X_{t-1}=s_{t-1},\\dots,X_0 = s_0) = \\Pr(X_t = s_t|X_{t-1}=s_{t-1})\n\\]\n(MORE CONTENT)\nAn application of Brook’s Lemma to Gaussian data.\nConsider the simple case of a spatial domain partitioned into two areal units, each neighbors of the other, with univariate outcome \\(X_i\\) at each unit \\(i=1,2\\).\nOne option is to specify a conditionally autoregressive model to capture spatial dependencies.\n\\[\n\\begin{align*}\nX_1 \\,|\\, X_2 &\\sim \\text{N}(b_{12}X_2, \\tau_1^2)\\\\\nX_2 \\,|\\, X_1 &\\sim \\text{N}(b_{21}X_1, \\tau_2^2)\\\\\n\\end{align*}\n\\]\nThe joint density of \\((X_1, X_2)\\) follows by application of Brook’s Lemma,\n\\[\n\\begin{align*}\np(x_1,x_2) &= \\frac{p(x_1|x_2)}{p(x_{1,0}|x_2)}\\frac{p(x_2|x_{1,0})}{p(x_{1,0}|x_{2,0})}p(x_{1,0},x_{2,0})\\\\\n           &\\propto  \\frac{p(x_1|x_2)}{p(x_{1,0}|x_2)}p(x_2|x_{1,0})\\\\\n           &\\propto  \\frac{\\exp\\left\\{-\\frac{1}{2}(x_1 - b_{12}x_2)^2\\right\\}}{\\exp\\left\\{-\\frac{1}{2}(x_{1,0} - b_{12}x_2)^2\\right\\}} \\exp\\left\\{-\\frac{1}{2}(x_2 - b_{21}x_{1,0})^2\\right\\}\\\\\n           &=  \\frac{\\exp\\left\\{-\\frac{1}{2}(x_1 - b_{12}x_2)^2\\right\\}}{\\exp\\left\\{-\\frac{1}{2}((0) - b_{12}x_2)^2\\right\\}} \\exp\\left\\{-\\frac{1}{2}(x_2 - b_{21}(0))^2\\right\\} & \\quad \\text{choose } (x_{1,0},x_{2,0}) = (0,0)\\\\\n           &=  \\frac{\\exp\\left\\{-\\frac{1}{2\\tau^2_1}(x^2_1 - 2b_{12}x_1x_2 + b_{12}^2x_2^2)\\right\\}}{\\exp\\left\\{-\\frac{1}{2\\tau^2_1}b_{12}^2x_2^2\\right\\}} \\exp\\left\\{-\\frac{1}{2\\tau^2_2}x_2^2\\right\\}\\\\\n           &= \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{1}{\\tau^2_1}\\left(x_1^2 - 2b_{12}x_1x_2 + x_2^2\\right) - \\frac{1}{\\tau_1^2}x_2^2 + \\frac{1}{\\tau_2^2}x_2^2\\right]\\right\\}\\\\\n           &= \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{1}{\\tau^2_1}\\left(x_1^2 - 2b_{12}x_1x_2\\right) + \\frac{1}{\\tau_2^2}x_2^2\\right]\\right\\}\\\\\n           &= \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{1}{\\tau^2_1}x_1^2 - 2\\frac{b_{12}}{\\tau^2_1}x_1x_2 + \\frac{1}{\\tau_2^2}x_2^2\\right]\\right\\}\\\\\n           &= \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{1}{\\tau^2_1}x_1^2 - \\frac{b_{12}}{\\tau^2_1}x_1x_2 - \\frac{b_{21}}{\\tau^2_2}x_1x_2 +\\frac{1}{\\tau_2^2}x_2^2\\right]\\right\\} & \\quad \\text{symmetry condition, } \\frac{b_{12}}{\\tau^2_1} =  \\frac{b_{21}}{\\tau^2_2}\\\\\n           &= \\exp\\left\\{-\\frac{1}{2}\\left[\n           \\begin{pmatrix}\n           x_1 & x_2\n           \\end{pmatrix}\n           \\begin{pmatrix}\n           \\tau^{-2}_1 & -\\tau^{-2}_1b_{12} \\\\\n           -\\tau^{-2}_2b_{21} & \\tau^{-2}_2\n           \\end{pmatrix}\n           \\begin{pmatrix}\n           x_1\\\\\n           x_2\n           \\end{pmatrix}\n           \\right]\\right\\} & \\quad \\text{matrix representation}\\\\\n           &= \\exp\\left\\{-\\frac{1}{2}\\left[\n           \\begin{pmatrix}\n           x_1 & x_2\n           \\end{pmatrix}\n            \\begin{pmatrix}\n           \\tau^{-2}_1 & 0 \\\\\n           0 & \\tau^{-2}_2\n           \\end{pmatrix}\n           \\begin{pmatrix}\n           1 & -b_{12} \\\\\n           -b_{21} & 1\n           \\end{pmatrix}\n           \\begin{pmatrix}\n           x_1\\\\\n           x_2\n           \\end{pmatrix}\n           \\right]\\right\\}\\\\\n           &= \\exp\\left\\{\\frac{1}{2}\\mathbf{x}^T\\mathbf{D}^{-1}(\\mathbf{I} - \\mathbf{B})\\mathbf{x}\\right\\}\n\\end{align*}\n\\] Which suggests that $(X_1, X_2) (, )"
  },
  {
    "objectID": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#fc-for-covariance-of-spatial-random-vector",
    "href": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#fc-for-covariance-of-spatial-random-vector",
    "title": "Step 11 - A Bayesian HM for multivariate Guassian data and MCAR prior - derivation of full conditionals and comments on computing",
    "section": "FC for Covariance of Spatial Random Vector",
    "text": "FC for Covariance of Spatial Random Vector\nNext, we will consider the full conditional for \\(\\boldsymbol\\Omega\\) under two cases. Either \\(\\boldsymbol\\Omega\\) is diagonal, i.e. \\(\\boldsymbol\\delta_i\\) is independent conditional on \\(\\boldsymbol\\phi_i\\), or \\(\\boldsymbol\\Omega\\) is symmetric positive-definite matrix, i.e. no assumed structure.\nUnder independence we can update each element of the diagonal of \\(\\boldsymbol\\Omega\\) individually \\[\n\\begin{align*}\np(\\boldsymbol\\Lambda \\,|\\, \\text{rest}) &\\propto p(\\{\\boldsymbol\\delta_i\\}, \\boldsymbol\\phi', \\boldsymbol\\beta, \\boldsymbol\\Omega, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto\np(\\boldsymbol\\phi'|\\boldsymbol\\Lambda,\\rho)p(\\boldsymbol\\Lambda)\\\\\n&\\propto\n\\big|(\\mathbf{D} - \\rho\\mathbf{W})^{-1} \\otimes \\boldsymbol\\Lambda\\big|^{-1/2} \\exp\\left\\{-\\frac{1}{2}\\left(\\boldsymbol\\phi'\\right)^T\\left((\\mathbf{D} - \\rho\\mathbf{W})^{-1}\\otimes\\boldsymbol\\Lambda\\right)^{-1}\\boldsymbol\\phi'\\right\\}\\\\\n&\\quad \\;\n\\times \\big|\\boldsymbol\\Lambda\\big|^{-(\\nu + p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}(\\boldsymbol\\Lambda^{-1}\\mathbf{R})\\right\\}\\\\\n&\\propto\n\\big|\\boldsymbol\\Lambda\\big|^{-p/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left(\\left((\\mathbf{D} - \\rho\\mathbf{W})\\otimes\\boldsymbol\\Lambda^{-1}\\right)\\boldsymbol\\phi'\\left(\\boldsymbol\\phi'\\right)^T\\right)\\right\\}\\\\\n&\\quad \\;\n\\times \\big|\\boldsymbol\\Lambda\\big|^{-(\\nu + p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}(\\boldsymbol\\Lambda^{-1}\\mathbf{R})\\right\\}\\\\\n&\\propto\n\\big|\\boldsymbol\\Lambda\\big|^{-p/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left(\\left((\\mathbf{D} - \\rho\\mathbf{W})\\otimes\\boldsymbol\\Lambda^{-1}\\right)\\left(\\boldsymbol\\phi'\\left(\\boldsymbol\\phi'\\right)^T \\otimes 1\\right)\\right)\\right\\}\\\\\n&\\quad \\;\n\\times \\big|\\boldsymbol\\Lambda\\big|^{-(\\nu + p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}(\\boldsymbol\\Lambda^{-1}\\mathbf{R})\\right\\}\\\\\n&= \\text{(now what?)}\n\\end{align*}\n\\] At this step I wanted to used the mixed-product property of Kronecker products to expand the trace, but this property only holds if the matrix products are well defined. Unfortunately, \\((\\mathbf{D}-\\rho\\mathbf{W})\\boldsymbol\\phi'(\\boldsymbol\\phi')^T\\) is not well defined because \\((\\mathbf{D}-\\rho\\mathbf{W})\\) is \\(n \\times n\\) and \\(\\boldsymbol\\phi'(\\boldsymbol\\phi')^T\\) is \\(np \\times np\\).\nWithout an immediate solution, let’s attempt to derive the full conditional for \\(\\boldsymbol\\Lambda^{-1}\\) using the conditional distributions for \\(\\boldsymbol\\phi_i\\).\n\\[\n\\begin{align*}\np(\\boldsymbol\\Lambda \\,|\\, \\text{rest}) &\\propto p(\\{\\boldsymbol\\delta_i\\}, \\{\\boldsymbol\\phi_i\\}, \\boldsymbol\\beta, \\boldsymbol\\Omega, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto p(\\boldsymbol\\phi'|\\boldsymbol\\Lambda,\\rho)p(\\boldsymbol\\Lambda)\\\\\n&= \\frac{p(\\boldsymbol\\phi_1 \\,|\\, \\{\\boldsymbol\\phi_{i}\\}_{i=2}^n)}{p(\\boldsymbol\\phi_{1,0} \\,|\\, \\{\\boldsymbol\\phi_{i}\\}_{i=2}^n)} \\frac{p(\\boldsymbol\\phi_2 \\,|\\, \\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^1,\\{\\boldsymbol\\phi_{i}\\}_{i=3}^n)}{p(\\boldsymbol\\phi_{2,0} \\,|\\, \\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^1,\\{\\boldsymbol\\phi_{i}\\}_{i=3}^n)}\n\\times \\\\\n&\\quad \\quad \\vdots \\\\\n&\\quad \\;\\times\\frac{p(\\boldsymbol\\phi_k \\,|\\, \\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^{k-1},\\{\\boldsymbol\\phi_{i}\\}_{i={k+1}}^n)}{p(\\boldsymbol\\phi_{k,0} \\,|\\, \\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^{k-1},\\{\\boldsymbol\\phi_{i}\\}_{i={k+1}}^n)} \\times\\\\\n&\\quad \\quad \\vdots \\\\\n&\\quad \\;\\times\\frac{p(\\boldsymbol\\phi_n \\,| \\,\n\\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^{n-1})}{p(\\boldsymbol\\phi_{n,0} \\,| \\,\n\\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^{n-1})} p(\\boldsymbol\\phi' = \\mathbf{0} \\,|\\, \\boldsymbol\\Lambda,\\rho)p(\\boldsymbol\\Lambda)\n\\end{align*}\n\\] where the last line follows by application of Brook’s Lemma to \\(p(\\boldsymbol\\phi'|\\boldsymbol\\Lambda,\\rho)\\). We’ve dropped \\(\\boldsymbol\\Lambda\\) and \\(\\rho\\) from conditioning notation in the resulting product of ratios, but the conditioning is very much still present. We choose \\(\\boldsymbol\\phi'_0 = \\boldsymbol{0}\\) for the fixed point in the support of \\(\\boldsymbol\\phi'\\). This should simplify things later.\n\n\n\n\n\n\nNote\n\n\n\nCould I have derived the full conditionals incorrectly for the univariate case? I think I just used the product of the conditionals there and not Brook’s Lemma. . . this might explain why my MCMC had issues with convergence. I should go back and check.\n\n\nWe have a particularly tedious product of ratios in the last line of the above derivation. Let’s focus on the \\(k\\)th term first, then condsider the entire product to understand the form of the full conditional for \\(\\boldsymbol\\Lambda\\). Let \\[\n\\mathbf{m}_k = \\rho\\sum_{\\substack j \\in \\mathcal{N}_k \\\\ j \\, > \\, k}\\mathbf{B}_{kj}\\boldsymbol\\phi_j\n\\] Note that at the \\(k\\)th term we are considering the density as a function of some non-zero conditioned variates. The \\(\\boldsymbol\\phi_j\\) for \\(j < k\\) have been set to \\(\\mathbf{0}\\). This may affect the sum in the conditional mean, so we restrict the sum to terms that are non-zero, that is \\(\\boldsymbol\\phi_j\\) where \\(j \\in \\mathcal{N}_k\\) and \\(j > k\\).\nThen the \\(k\\)th term, \\[\n\\begin{align*}\n\\frac{p(\\boldsymbol\\phi_k \\,|\\, \\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^{k-1},\\{\\boldsymbol\\phi_{i}\\}_{i={k+1}}^n)}{p(\\boldsymbol\\phi_{k,0} \\,|\\, \\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^{k-1},\\{\\boldsymbol\\phi_{i}\\}_{i={k+1}}^n)}\n&=\n\\frac{\\exp\\left\\{-\\frac{w_{k+}}{2}\\left(\\boldsymbol\\phi_k-\\mathbf{m}_k\\right)^T\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol\\phi_k-\\mathbf{m}_k\\right)\\right\\}}{\\exp\\left\\{-\\frac{w_{k+}}{2}\\left(\\boldsymbol{0}-\\mathbf{m}_k\\right)^T\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol{0}-\\mathbf{m}_k\\right)\\right\\}}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\left(\\boldsymbol\\phi_k-\\mathbf{m}_k\\right)^T\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol\\phi_k-\\mathbf{m}_k\\right) + \\frac{w_{k+}}{2}\\mathbf{m}_k^T\\boldsymbol\\Lambda^{-1}\\mathbf{m}_k\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\left[\\left(\\boldsymbol\\phi_k-\\mathbf{m}_k\\right)^T\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol\\phi_k-\\mathbf{m}_k\\right) -\\mathbf{m}_k^T\\boldsymbol\\Lambda^{-1}\\mathbf{m}_k\\right]\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\left[\\boldsymbol\\phi^T_k\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k - 2\\mathbf{m}_k^T\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi^T_k\\right]\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\text{Tr}\\left[\\boldsymbol\\phi^T_k\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k - 2\\mathbf{m}_k^T\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k\\right]\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\left(\\text{Tr}\\left[\\boldsymbol\\phi^T_1\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k\\right] - \\text{Tr}\\left[ 2\\mathbf{m}_k^T\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k\\right]\\right)\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\left(\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k\\right] - \\text{Tr}\\left[ 2\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k\\mathbf{m}_k^T\\right]\\right)\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k -  2\\boldsymbol\\phi_k\\mathbf{m}_k^T\\right)\\right]\\right\\}\\\\\n\\end{align*}\n\\]\nThen the entire product will be \\[\n\\begin{align*}\np(\\boldsymbol\\phi'|\\boldsymbol\\Lambda,\\rho)\n&=\n\\left[\\prod_{k=1}^n \\frac{p(\\boldsymbol\\phi_k \\,|\\, \\{\\boldsymbol\\phi_{j,0} = \\mathbf{0}\\}_{j=1}^{k-1},\\{\\boldsymbol\\phi_{i}\\}_{i={k+1}}^n, \\boldsymbol\\Lambda, \\rho)}{p(\\boldsymbol\\phi_{k,0} = \\mathbf{0} \\,|\\, \\{\\boldsymbol\\phi_{j,0} = \\mathbf{0}\\}_{j=1}^{k-1},\\{\\boldsymbol\\phi_{i}\\}_{i={k+1}}^n, \\boldsymbol\\Lambda, \\rho)}\\right]p(\\boldsymbol\\phi' = \\mathbf{0} \\,|\\, \\boldsymbol\\Lambda,\\rho)\\\\\n&=\n\\left[\\prod_{k=1}^n \\exp\\left\\{-\\frac{w_{k+}}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k -  2\\boldsymbol\\phi_k\\mathbf{m}_k^T\\right)\\right]\\right\\}\\right]p(\\boldsymbol\\phi' = \\mathbf{0} \\,|\\, \\boldsymbol\\Lambda,\\rho)\\\\\n&=\n\\exp\\left\\{-\\frac{1}{2}\\sum_{k=1}^n\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1}w_{k+}\\left(\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k -  2\\boldsymbol\\phi_k\\mathbf{m}_k^T\\right)\\right]\\right\\}p(\\boldsymbol\\phi' = \\mathbf{0} \\,|\\, \\boldsymbol\\Lambda,\\rho)\\\\\n&=\n\\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1} \\sum_{k=1}^n \\mathbf{S}_k\\right]\\right\\} \\times \\big|(\\mathbf{D} - \\rho\\mathbf{W})^{-1} \\otimes \\boldsymbol\\Lambda\\big|^{-1/2} \\exp\\left\\{-\\frac{1}{2}\\left(\\mathbf{0}\\right)^T\\left((\\mathbf{D} - \\rho\\mathbf{W})^{-1}\\otimes\\boldsymbol\\Lambda\\right)^{-1}\\mathbf{0}\\right\\}\\\\\n&= \\big|(\\mathbf{D} - \\rho\\mathbf{W})^{-1} \\otimes \\boldsymbol\\Lambda\\big|^{-1/2}\n\\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1} \\sum_{k=1}^n \\mathbf{S}_k\\right]\\right\\}\n\\end{align*}\n\\]\nwhere \\(\\mathbf{S}_k = w_{k+}\\left(\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k - 2\\boldsymbol\\phi_k\\mathbf{m}_k^T\\right)\\).\nFinally, the full conditional for \\(\\boldsymbol\\Lambda\\) is \\[\n\\begin{align*}\np(\\boldsymbol\\Lambda \\,|\\, \\text{rest}) &\\propto p(\\{\\boldsymbol\\delta_i\\}, \\{\\boldsymbol\\phi_i\\}, \\boldsymbol\\beta, \\boldsymbol\\Omega, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto p(\\boldsymbol\\phi'|\\boldsymbol\\Lambda,\\rho)p(\\boldsymbol\\Lambda)\\\\\n&=\n\\big|(\\mathbf{D} - \\rho\\mathbf{W})^{-1} \\otimes \\boldsymbol\\Lambda\\big|^{-1/2}\n\\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1} \\sum_{k=1}^n \\mathbf{S}_k\\right]\\right\\}\\\\\n&\\quad \\;\\times\n\\big|\\boldsymbol\\Lambda\\big|^{-(\\nu + p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}(\\boldsymbol\\Lambda^{-1}\\mathbf{R})\\right\\}\\\\\n&\\propto\n\\big|\\boldsymbol\\Lambda\\big|^{-p/2}\n\\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1} \\sum_{k=1}^n \\mathbf{S}_k\\right]\\right\\}\\\\\n&\\quad \\;\\times\n\\big|\\boldsymbol\\Lambda\\big|^{-(\\nu + p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1}\\mathbf{R}\\right]\\right\\}\\\\\n&=\n\\big|\\boldsymbol\\Lambda\\big|^{-(\\nu + 2p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1}\\left(\\mathbf{R} + \\sum_{k=1}^n \\mathbf{S}_k\\right)\\right]\\right\\}\\\\\n\\end{align*}\n\\] and we conclude \\(\\boldsymbol\\Lambda \\,|\\, \\text{rest} \\sim \\text{InvWish}\\left(p + \\nu,\\, \\mathbf{R} + \\sum_{k=1}^n \\mathbf{S}_k\\right)\\)"
  },
  {
    "objectID": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#fc-for-fixed-effects",
    "href": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#fc-for-fixed-effects",
    "title": "Step 11 - A Bayesian HM for multivariate Guassian data and MCAR prior - derivation of full conditionals and comments on computing",
    "section": "FC for Fixed effects",
    "text": "FC for Fixed effects\nStart with the full conditional for \\(\\boldsymbol\\beta\\). \\[\n\\begin{align*}\np(\\boldsymbol\\beta \\,|\\, \\text{rest}) &\\propto p(\\{\\boldsymbol\\delta_i\\}, \\boldsymbol\\phi', \\boldsymbol\\beta, \\boldsymbol\\Omega, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto p(\\{\\boldsymbol\\delta_i\\} \\,|\\, \\boldsymbol\\beta, \\boldsymbol\\phi', \\boldsymbol\\Omega)p(\\boldsymbol\\beta)\\\\\n&= \\left[\\prod_{i=1}^n p(\\boldsymbol\\delta_i \\,|\\, \\boldsymbol\\beta,\\boldsymbol\\phi_i,\\boldsymbol\\Omega)\\right]p(\\boldsymbol\\beta)\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^n\\left[\\boldsymbol\\delta_i-(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\boldsymbol\\phi_i)\\right]^T\\boldsymbol\\Omega^{-1}\\left[\\boldsymbol\\delta_i-(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\boldsymbol\\phi_i)\\right]\\right\\}\\\\\n&\\quad \\, \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol\\beta-\\boldsymbol\\mu\\right]^T\\boldsymbol\\Gamma^{-1}\\left[\\boldsymbol\\beta-\\boldsymbol\\mu\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol\\beta^T\\left(\\sum_{i=1}^n\\boldsymbol{\\mathcal{Z}}_i^T\\boldsymbol\\Omega^{-1}\\boldsymbol{\\mathcal{Z}}_i\\right)\\boldsymbol\\beta - 2\\left(\\sum_{i=1}^n(\\boldsymbol\\delta_i-\\boldsymbol\\phi_i)^T\\boldsymbol\\Omega^{-1}\\boldsymbol{\\mathcal{Z}}_i\\right)\\boldsymbol\\beta\\right]\\right\\}\\\\\n&\\quad \\, \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol\\beta^T\\boldsymbol\\Gamma^{-1}\\boldsymbol\\beta - 2\\boldsymbol\\mu^T\\boldsymbol\\Gamma^{-1}\\boldsymbol\\beta\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol\\beta^T\\left(\\sum_{i=1}^n\\boldsymbol{\\mathcal{Z}}_i^T\\boldsymbol\\Omega^{-1}\\boldsymbol{\\mathcal{Z}}_i + \\boldsymbol\\Gamma^{-1}\\right)\\boldsymbol\\beta - 2\\left(\\sum_{i=1}^n(\\boldsymbol\\delta_i-\\boldsymbol\\phi_i)^T\\boldsymbol\\Omega^{-1}\\boldsymbol{\\mathcal{Z}}_i + \\boldsymbol\\mu^T\\boldsymbol\\Gamma^{-1}\\right)\\boldsymbol\\beta\\right]\\right\\}\n\\end{align*}\n\\] Thus, \\[\n\\begin{align*}\n\\boldsymbol\\beta\\,|\\,\\text{rest} &\\sim \\text{Normal}_{qp}(\\mathbf{V}_\\beta^{-1}\\mathbf{M}_\\beta, \\mathbf{V}_\\beta^{-1})\\\\\n\\mathbf{V}_\\beta &= \\sum_{i=1}^n\\boldsymbol{\\mathcal{Z}}^T_i\\mathbf{\\Omega}^{-1}\\boldsymbol{\\mathcal{Z}}_i + \\mathbf{\\Gamma}^{-1}\\\\\n\\mathbf{M}_\\beta &= \\sum_{i=1}^n\\boldsymbol{\\mathcal{Z}}^T_i\\mathbf{\\Omega}^{-1}(\\boldsymbol\\delta_i-\\boldsymbol\\phi_i) + \\mathbf{\\Gamma}^{-1}\\boldsymbol\\mu\n\\end{align*}\n\\]"
  },
  {
    "objectID": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#fc-for-spatial-random-vector",
    "href": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#fc-for-spatial-random-vector",
    "title": "Step 11 - A Bayesian HM for multivariate Guassian data and MCAR prior - derivation of full conditionals and comments on computing",
    "section": "FC for Spatial Random Vector",
    "text": "FC for Spatial Random Vector\nNext we must determine the full conditional distribution for the spatial random vector. There are two approaches here, we could either derive full conditionals for \\(\\boldsymbol\\phi_i\\) and draw samples in \\(n\\) sub-blocks of parameters, or we could derive the full conditional for \\(\\boldsymbol\\phi'\\) and update all \\(\\boldsymbol\\phi_i\\) in a single block. It is hard to say which would be computationally preferable. The latter seems more interesting, though, so let’s try to tackle that first, then derive the full conditionals for \\(\\boldsymbol\\phi_i\\).\n\nJoint FC approach\nThe trickiness of updating \\(\\boldsymbol\\phi'\\) as a single block is due to our model specification for \\(\\boldsymbol\\delta_i\\). The likelihood for \\(\\boldsymbol\\delta_i\\) is not compatible with the prior on \\(\\boldsymbol\\phi'\\) because the spatial term in the former is specified in terms of \\(p \\times 1\\) vectors while in the latter spatial term is a \\(np \\times 1\\) vector.\nWe could either construct the joint distribution of the \\(\\boldsymbol\\delta_i\\) vectors or we could try to write \\(\\boldsymbol\\phi_i\\) in terms of \\(\\boldsymbol\\phi'\\) and substitute into the corresponding \\(\\boldsymbol\\delta_i\\) distributions.\nLet, \\[\n\\boldsymbol\\Delta =\n\\begin{pmatrix}\n\\delta_{11} & \\dots & \\delta_{1p}\\\\\n\\delta_{21} & \\dots & \\delta_{2p}\\\\\n\\vdots & \\vdots & \\vdots\\\\\n\\delta_{n1} & \\dots & \\delta_{np}\\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\boldsymbol\\delta_1^T\\\\\n\\boldsymbol\\delta_2^T\\\\\n\\vdots\\\\\n\\boldsymbol\\delta_n^T\\\\\n\\end{pmatrix}\n\\]\nThen define \\(\\boldsymbol\\delta' = \\text{vec}(\\boldsymbol\\Delta^T)\\). Since \\(\\boldsymbol\\delta_i\\) are conditionally independent, the joint distribution for \\(\\boldsymbol\\delta'\\) is\n\\[\n\\boldsymbol\\delta' \\sim \\text{Normal}_{np}\n\\left(\n\\begin{pmatrix}\n\\boldsymbol{\\mathcal{Z}}_1\\boldsymbol\\beta\\\\\n\\boldsymbol{\\mathcal{Z}}_2\\boldsymbol\\beta\\\\\n\\vdots \\\\\n\\boldsymbol{\\mathcal{Z}}_n\\boldsymbol\\beta\\\\\n\\end{pmatrix} + \\boldsymbol\\phi', \\;\n\\mathbf{I}_n \\otimes \\boldsymbol\\Omega\n\\right)\n\\] Again, the purpose of constructing the joint distribution in this way was to ensure the mean is in terms of \\(\\boldsymbol\\phi'\\). But now the fixed effects term is a bit awkward. Let’s avoid that temporarily by letting \\[\n\\mathbf{A} =\n\\begin{pmatrix}\n\\boldsymbol{\\mathcal{Z}}_1\\boldsymbol\\beta\\\\\n\\boldsymbol{\\mathcal{Z}}_2\\boldsymbol\\beta\\\\\n\\vdots \\\\\n\\boldsymbol{\\mathcal{Z}}_n\\boldsymbol\\beta\\\\\n\\end{pmatrix}.\n\\]\nThen if we proceed in deriving the full conditional for \\(\\boldsymbol\\phi'\\), we will have\n\\[\n\\begin{align*}\np(\\boldsymbol\\phi' \\,|\\, \\text{rest}) &\\propto p(\\boldsymbol\\delta', \\boldsymbol\\phi', \\boldsymbol\\beta, \\boldsymbol\\Omega, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto p(\\{\\boldsymbol\\delta' \\,|\\, \\boldsymbol\\beta, \\boldsymbol\\phi', \\boldsymbol\\Omega)p(\\boldsymbol\\phi')\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol\\delta'-(\\mathbf{A} + \\boldsymbol\\phi')\\right]^T\\left(\\mathbf{I}_n \\otimes \\boldsymbol\\Omega\\right)^{-1}\\left[\\boldsymbol\\delta_i-(\\mathbf{A} + \\boldsymbol\\phi')\\right]\\right\\}\\\\\n&\\quad \\, \\times \\exp\\left\\{-\\frac{1}{2}(\\boldsymbol\\phi')^T\\left((\\mathbf{D-\\rho\\mathbf{W}}) \\otimes \\boldsymbol\\Lambda^{-1}\\right)\\boldsymbol\\phi'\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[(\\boldsymbol\\phi')^T\\left(\\mathbf{I}_n \\otimes \\boldsymbol\\Omega\\right)^{-1}\\boldsymbol\\phi' - 2\\left((\\boldsymbol\\delta'-\\mathbf{A})^T\\left(\\mathbf{I}_n \\otimes \\boldsymbol\\Omega\\right)^{-1}\\right)\\boldsymbol\\phi'\\right]\\right\\}\\\\\n&\\quad \\, \\times \\exp\\left\\{-\\frac{1}{2}(\\boldsymbol\\phi')^T\\left((\\mathbf{D-\\rho\\mathbf{W}}) \\otimes \\boldsymbol\\Lambda^{-1}\\right)\\boldsymbol\\phi'\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[(\\boldsymbol\\phi')^T\\left(\\left(\\mathbf{I}_n \\otimes \\boldsymbol\\Omega\\right)^{-1} + \\left((\\mathbf{D-\\rho\\mathbf{W}}) \\otimes \\boldsymbol\\Lambda^{-1}\\right)\\right)\\boldsymbol\\phi' - 2\\left((\\boldsymbol\\delta'-\\mathbf{A})^T\\left(\\mathbf{I}_n \\otimes \\boldsymbol\\Omega\\right)^{-1}\\right)\\boldsymbol\\phi'\\right]\\right\\}\n\\end{align*}\n\\] Thus,\n\\[\n\\begin{align*}\n\\boldsymbol\\phi'\\,|\\,\\text{rest} &\\sim \\text{Normal}_{np}(\\mathbf{V}_{\\phi'}^{-1}\\mathbf{M}_{\\phi'}, \\mathbf{V}_{\\phi'}^{-1})\\\\\n\\mathbf{V}_{\\phi'} &= \\left(\\mathbf{I}_n \\otimes \\boldsymbol\\Omega\\right)^{-1} + \\left((\\mathbf{D-\\rho\\mathbf{W}}) \\otimes \\boldsymbol\\Lambda^{-1}\\right)\\\\\n\\mathbf{M}_{\\phi'} &= \\left(\\mathbf{I}_n \\otimes \\boldsymbol\\Omega\\right)^{-1}(\\boldsymbol\\delta'-\\mathbf{A})\n\\end{align*}\n\\]\nThis full conditional looks to be pretty clean, though we will need to consider efficient ways to compute \\(\\mathbf{A}\\). Before moving on, let’s explore an alternative derivation for the full conditional of \\(\\boldsymbol\\phi'\\).\nIf we instead wrote \\(\\boldsymbol\\phi_i\\) in terms of \\(\\boldsymbol\\phi'\\) when describing the distribution of \\(\\boldsymbol\\delta_i\\)? Consider the way in which one “extracts” a sub-vector of elements from a vector. We know there exists a \\(p \\times np\\) matrix \\(\\mathbf{U}_i\\) which will “extract” the \\(p \\times 1\\) dimension \\(\\boldsymbol\\phi_i\\) vector from the \\(np \\times 1\\) dimenion \\(\\boldsymbol\\phi'\\) vector. This \\(\\mathbf{U}_i\\) matrix will have all elements equal to \\(0\\) except for \\(p\\) columns forming a \\(p \\times p\\) identity matrix. The position of this \\(p \\times p\\) identity matrix in \\(\\mathbf{U}_i\\) determines which \\(\\boldsymbol\\phi_i\\) to “extract” from \\(\\boldsymbol\\phi'\\). In order to extract \\(\\boldsymbol\\phi_i\\), then the \\(p \\times p\\) identity matrix must begin at column \\((i-1)p + 1\\) in \\(\\mathbf{U}_i\\)\n\\[\n\\begin{align*}\n\\boldsymbol\\phi_i &=\n\\mathbf{U}_i\\boldsymbol\\phi'\\\\\n&= \\begin{pmatrix}\n\\boldsymbol{0}_p & \\dots & \\boldsymbol{0}_p & \\mathbf{I}_p & \\boldsymbol{0}_p & \\dots & \\boldsymbol{0}_p\n\\end{pmatrix}\n\\boldsymbol\\phi'\n\\end{align*}\n\\] where \\(\\boldsymbol 0_p\\) is a \\(p \\times p\\) matrix of \\(0\\)’s. Now we can write\n\\[\n\\boldsymbol\\delta_i \\sim \\text{Normal}_p\\left(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\mathbf{U}_i\\boldsymbol\\phi', \\;  \\boldsymbol\\Omega\\right)\n\\] which will facilitate the derivation of the full conditional for \\(\\boldsymbol\\phi'\\).\n\\[\n\\begin{align*}\np(\\boldsymbol\\phi' \\,|\\, \\text{rest}) &\\propto p(\\{\\boldsymbol\\delta_i\\}, \\boldsymbol\\phi', \\boldsymbol\\beta, \\boldsymbol\\Omega, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto p(\\{\\boldsymbol\\delta_i\\} \\,|\\, \\boldsymbol\\beta, \\boldsymbol\\phi', \\boldsymbol\\Omega)p(\\boldsymbol\\phi')\\\\\n&= \\left[\\prod_{i=1}^n p(\\boldsymbol\\delta_i \\,|\\, \\boldsymbol\\beta,\\boldsymbol\\phi',\\boldsymbol\\Omega)\\right]p(\\boldsymbol\\phi')\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^n\\left[\\boldsymbol\\delta_i-(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\mathbf{U}_i\\boldsymbol\\phi')\\right]^T\\boldsymbol\\Omega^{-1}\\left[\\boldsymbol\\delta_i-(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\mathbf{U}_i\\boldsymbol\\phi')\\right]\\right\\}\\\\\n&\\quad \\, \\times \\exp\\left\\{-\\frac{1}{2}(\\boldsymbol\\phi')^T\\left((\\mathbf{D-\\rho\\mathbf{W}}) \\otimes \\boldsymbol\\Lambda^{-1}\\right)\\boldsymbol\\phi'\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[(\\boldsymbol\\phi')^T\\left(\\sum_{i=1}^n\\mathbf{U}_i^T\\boldsymbol\\Omega^{-1}\\mathbf{U}_i\\right)\\boldsymbol\\phi' - 2\\left(\\sum_{i=1}^n(\\boldsymbol\\delta_i-\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta)^T\\boldsymbol\\Omega^{-1}\\mathbf{U}_i\\right)\\boldsymbol\\phi'\\right]\\right\\}\\\\\n&\\quad \\, \\times \\exp\\left\\{-\\frac{1}{2}(\\boldsymbol\\phi')^T\\left((\\mathbf{D-\\rho\\mathbf{W}}) \\otimes \\boldsymbol\\Lambda^{-1}\\right)\\boldsymbol\\phi'\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[(\\boldsymbol\\phi')^T\\left(\\sum_{i=1}^n\\mathbf{U}_i^T\\boldsymbol\\Omega^{-1}\\mathbf{U}_i + \\left((\\mathbf{D-\\rho\\mathbf{W}}) \\otimes \\boldsymbol\\Lambda^{-1}\\right)\\right)\\boldsymbol\\phi' - 2\\left(\\sum_{i=1}^n(\\boldsymbol\\delta_i-\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta)^T\\boldsymbol\\Omega^{-1}\\mathbf{U}_i\\right)\\boldsymbol\\phi'\\right]\\right\\}\n\\end{align*}\n\\] Thus, \\[\n\\begin{align*}\n\\boldsymbol\\phi'\\,|\\,\\text{rest} &\\sim \\text{Normal}_{np}(\\mathbf{V}_{\\phi'}^{-1}\\mathbf{M}_{\\phi'}, \\mathbf{V}_{\\phi'}^{-1})\\\\\n\\mathbf{V}_{\\phi'} &= \\sum_{i=1}^n\\mathbf{U}_i^T\\boldsymbol\\Omega^{-1}\\mathbf{U}_i + \\left((\\mathbf{D-\\rho\\mathbf{W}}) \\otimes \\boldsymbol\\Lambda^{-1}\\right)\\\\\n\\mathbf{M}_{\\phi'} &= \\sum_{i=1}^n\\mathbf{U}^T_i\\mathbf{\\Omega}^{-1}(\\boldsymbol\\delta_i-\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta)\n\\end{align*}\n\\]\nThese two full conditionals for \\(\\boldsymbol\\phi'\\) are equivalent. To see this we need to apply an identity previously described in Step 3 during the discussion of the full conditional for \\(\\boldsymbol\\beta\\). In Step 3, we considered an organization of the covariates \\(\\mathbf{z}_i\\), into a matrix \\(\\mathbf{Z} = (\\mathbf{z}_1^T, \\dots,\\mathbf{z}_N^T)^T\\) \\[\n\\begin{equation}\n\\tag{3}\n\\mathbf{Z} =\n\\begin{bmatrix}\n1 & z_{12} & \\dots & z_{1q}\\\\\n\\vdots & & \\vdots &\\\\\n1 & z_{N2} & \\dots & z_{Nq}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{z}_1^T\\\\\n\\vdots\\\\\n\\mathbf{z}_N^T\\\\\n\\end{bmatrix}.\n\\end{equation}\n\\] Notice, \\(\\boldsymbol{\\mathcal{Z}}_i = \\mathbf{z}^T_i \\otimes \\mathbf{I}_p\\). Then the following relate the sums to Kronecker products, (make this more general?)\n\\[\n\\begin{equation}\n\\tag{4}\n\\sum_{i=1}^N\\boldsymbol{\\mathcal{Z}}^T_i\\boldsymbol\\Omega^{-1}\\boldsymbol{\\mathcal{Z}}_i = \\mathbf{Z}^T\\mathbf{Z} \\otimes \\boldsymbol\\Omega^{-1}\n\\end{equation}\n\\]\n\\[\n\\begin{equation}\n\\tag{5}\n\\sum_{i=1}^N\\boldsymbol{\\mathcal{Z}}^T_i\\mathbf{\\Omega}^{-1}\\boldsymbol\\delta_i =  \\text{vec}(\\Omega^{-1}\\boldsymbol\\Delta\\mathbf{Z}) = (\\mathbf{Z}^T \\otimes \\, \\boldsymbol\\Omega^{-1})\\text{vec}(\\boldsymbol\\Delta).\n\\end{equation}\n\\] In the case of the full conditional for \\(\\boldsymbol\\phi'\\), we have \\(\\mathbf{U}_i = \\mathbf{u}^T_i \\otimes I_n\\) where \\(\\mathbf{u}_i\\) is an elementary vector with \\(1\\) in position \\(i\\) and \\(0\\) elsewhere. Stacking rows of \\(\\mathbf{u}_i^T\\) into a matrix constructs the identity matrix \\(\\mathbf{I}_n\\). So, substituting \\(\\mathbf{U}_i\\) for \\(\\boldsymbol{\\mathcal{Z}}_i\\) and \\(\\mathbf{I}_n\\) for \\(\\mathbf{Z}\\) yields\n\\[\n\\begin{equation}\n\\tag{4}\n\\sum_{i=1}^N\\mathbf{U}_i^T\\boldsymbol\\Omega^{-1}\\mathbf{U}_i = \\mathbf{I}_n^T\\mathbf{I}_n \\otimes \\boldsymbol\\Omega^{-1} = \\mathbf{I}_n \\otimes \\boldsymbol\\Omega^{-1}\n\\end{equation}\n\\]\nAlso, substituting \\(\\mathbf{U}_i\\) for \\(\\boldsymbol{\\mathcal{Z}}_i\\) and \\(\\boldsymbol\\delta_i - \\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta\\) for \\(\\boldsymbol\\delta_i\\) yields \\[\n\\begin{equation}\n\\tag{5}\n\\sum_{i=1}^N\\mathbf{U}_i\\mathbf{\\Omega}^{-1}(\\boldsymbol\\delta_i - \\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta) =  \\text{vec}(\\Omega^{-1}\\boldsymbol\\Delta\\mathbf{I}_n) = (\\mathbf{I}_n^T \\otimes \\, \\boldsymbol\\Omega^{-1})(\\boldsymbol\\delta'-\\mathbf{A}).\n\\end{equation}\n\\] This shows that the two approaches to deriving the full conditional for \\(\\boldsymbol\\phi'\\) yield the same results.\nReturning to \\(\\mathbf{A}\\), which is necessary to compute in the block full conditional for \\(\\boldsymbol\\phi'\\). We recognize that\n\\[\n\\begin{align*}\n\\mathbf{A} &=\n\\begin{pmatrix}\n\\boldsymbol{\\mathcal{Z}}_1\\boldsymbol\\beta\\\\\n\\vdots \\\\\n\\boldsymbol{\\mathcal{Z}}_n\\boldsymbol\\beta\\\\\n\\end{pmatrix}\\\\\n&=\n\\begin{pmatrix}\n\\mathbf{B}\\mathbf{z_1}\\\\\n\\vdots \\\\\n\\mathbf{B}\\mathbf{z_n}\\\\\n\\end{pmatrix}\\\\\n&= \\text{vec}(\\mathbf{B}\\mathbf{Z^T})\n\\end{align*}\n\\] So we can substitute, \\[\n(\\boldsymbol\\delta' - \\mathbf{A}) = \\text{vec}(\\boldsymbol\\Delta-\\mathbf{B}\\mathbf{Z}^T)\n\\] in the full conditional for \\(\\boldsymbol\\phi'\\).\n\n\nMarginal (site-by-site) FC approach\nWe might desire to sample full conditionals for \\(\\boldsymbol\\phi_i\\) rather than the full joint distribution for \\(\\boldsymbol\\phi'\\).\n\\[\n\\begin{align*}\np(\\boldsymbol\\phi_i \\,|\\, \\text{rest})\n&\\propto\np(\\{\\boldsymbol\\delta_i\\}, \\{\\boldsymbol\\phi_i\\}, \\boldsymbol\\beta, \\boldsymbol\\Omega, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto\np(\\boldsymbol\\delta_i \\,|\\, \\boldsymbol\\phi_i, \\boldsymbol\\beta, \\boldsymbol\\Omega)p(\\boldsymbol\\phi_i \\,|\\, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto\n\\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol\\delta_i-(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\boldsymbol\\phi_i)\\right]^T\\boldsymbol\\Omega^{-1}\\left[\\boldsymbol\\delta_i-(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\boldsymbol\\phi_i)\\right]\\right\\}\\\\\n&\\quad \\, \\times\n\\exp\\left\\{-\\frac{w_{i+}}{2}\\left(\\boldsymbol\\phi_i - \\rho\\sum_{j \\in \\mathcal{N}_i}\\mathbf{B}_{ij}\\boldsymbol\\phi_j\\right)^T\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol\\phi_i - \\rho\\sum_{j \\in \\mathcal{N}_i}\\mathbf{B}_{ij}\\boldsymbol\\phi_j\\right)\\right\\}\\\\\n&\\propto\n\\exp\\left\\{-\\frac{1}{2}\\left[(\\boldsymbol\\phi_i)^T\\boldsymbol\\Omega^{-1}\\boldsymbol\\phi_i - 2\\left(\\boldsymbol\\delta_i^T -\\boldsymbol\\beta^T\\boldsymbol{\\mathcal{Z}}^T_i\\right)\\boldsymbol\\Omega^{-1}\\boldsymbol\\phi_i\\right]\\right\\}\\\\\n&\\quad \\, \\times\n\\exp\\left\\{-\\frac{1}{2}\\left[(\\boldsymbol\\phi_i)^Tw_{i+}\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_i - 2\\rho w_{i+}\\left(\\sum_{j \\in \\mathcal{N}_i}\\boldsymbol\\phi_j^T\\mathbf{B}_{ij}^T\\right)\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_i\\right]\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{1}{2}\\left[(\\boldsymbol\\phi_i)^T\\left(\\boldsymbol\\Omega^{-1} + w_{i+}\\boldsymbol\\Lambda^{-1}\\right)\\boldsymbol\\phi_i - 2\\left[\\left(\\boldsymbol\\delta_i^T -\\boldsymbol\\beta^T\\boldsymbol{\\mathcal{Z}}^T_i\\right)\\boldsymbol\\Omega^{-1} + \\rho w_{i+}\\left(\\sum_{j \\in \\mathcal{N}_i}\\boldsymbol\\phi_j^T\\mathbf{B}_{ij}^T\\right)\\boldsymbol\\Lambda^{-1}\\right]\\boldsymbol\\phi_i\\right]\\right\\}\\\\\n\\end{align*}\n\\]\nThus,\n\\[\n\\begin{align*}\n\\boldsymbol\\phi_i\\,|\\,\\text{rest} &\\sim \\text{Normal}_{p}(\\mathbf{V}_{\\phi_i}^{-1}\\mathbf{M}_{\\phi_i}, \\mathbf{V}_{\\phi_i}^{-1})\\\\\n\\mathbf{V}_{\\phi_i} &= \\boldsymbol\\Omega^{-1} + w_{i+}\\boldsymbol\\Lambda^{-1}\\\\\n\\mathbf{M}_{\\phi_i} &= \\boldsymbol\\Omega^{-1}\\left(\\boldsymbol\\delta_i-\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta\\right) + \\rho w_{i+}\\boldsymbol\\Lambda^{-1}\\sum_{j \\in \\mathcal{N}_i}\\mathbf{B}_{ij}\\boldsymbol\\phi_j\n\\end{align*}\n\\] taking \\(\\mathbf{B}_{ij} = \\frac{w_{ij}}{w_{i+}}\\mathbf{I}_p\\), we will have \\(\\mathbf{M}_{\\phi_i} = \\boldsymbol\\Omega^{-1}\\left(\\boldsymbol\\delta_i-\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta\\right) + \\rho \\boldsymbol\\Lambda^{-1}\\sum_{j \\in \\mathcal{N}_i}w_{ij}\\boldsymbol\\phi_j\\)."
  },
  {
    "objectID": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#fc-for-boldsymbolomega",
    "href": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#fc-for-boldsymbolomega",
    "title": "Step 11 - A Bayesian HM for multivariate Guassian data and MCAR prior - derivation of full conditionals and comments on computing",
    "section": "FC for \\(\\boldsymbol\\Omega\\)",
    "text": "FC for \\(\\boldsymbol\\Omega\\)\nNext, we will consider the full conditional for \\(\\boldsymbol\\Omega\\) under two cases. Either \\(\\boldsymbol\\Omega\\) is diagonal, i.e. \\(\\boldsymbol\\delta_i\\) is independent conditional on \\(\\boldsymbol\\phi_i\\), or \\(\\boldsymbol\\Omega\\) is symmetric positive-definite matrix, i.e. no assumed structure.\n\nGeneral \\(\\boldsymbol\\Omega\\)\nWe’ll get right into it\n\\[\n\\begin{align*}\np(\\boldsymbol\\Omega \\,|\\, \\text{rest})\n&\\propto\np(\\{\\boldsymbol\\delta_i\\}, \\{\\boldsymbol\\phi_i\\}, \\boldsymbol\\beta,\\boldsymbol\\Omega, \\boldsymbol\\Lambda,\\rho)\\\\\n&\\propto\n\\left[\\prod_{i=1}^n p(\\boldsymbol\\delta_i \\,|\\, \\boldsymbol\\beta,\\boldsymbol\\phi_i,\\boldsymbol\\Omega)\\right]p(\\boldsymbol\\Omega)\\\\\n&\\propto\n|\\boldsymbol\\Omega|^{n/2}\\exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^n\\left[\\boldsymbol\\delta_i-(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\boldsymbol\\phi_i)\\right]^T\\boldsymbol\\Omega^{-1}\\left[\\boldsymbol\\delta_i-(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\boldsymbol\\phi_i)\\right]\\right\\}\\\\\n&\\quad \\; \\times |\\boldsymbol\\Omega|^{-(\\xi + p + 1)/2}\\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left(\\boldsymbol\\Omega^{-1}\\mathbf{G}\\right)\\right\\}\\\\\n&= |\\boldsymbol\\Omega|^{-(n + \\xi + p + 1)/2}\\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left(\\boldsymbol\\Omega^{-1}(\\mathbf{H} + \\mathbf{G})\\right)\\right\\}\\\\\n\\end{align*}\n\\]\nwhere \\(\\mathbf{H} = \\sum_{i=1}^n\\left[\\boldsymbol\\delta_i-(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\boldsymbol\\phi_i)\\right]\\left[\\boldsymbol\\delta_i-(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\boldsymbol\\phi_i)\\right]^T\\)\nThus, \\[\n\\begin{align*}\n\\boldsymbol\\Omega \\,|\\, \\text{rest} &\\sim \\text{InvWish}\\left(n+\\xi, \\mathbf{H}+\\mathbf{G}\\right)\\\\\nn &= \\text{number of spatial locations}\\\\\n\\mathbf{H} &= \\sum_{i=1}^n\\left[\\boldsymbol\\delta_i-(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\boldsymbol\\phi_i)\\right]\\left[\\boldsymbol\\delta_i-(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\boldsymbol\\phi_i)\\right]^T\n\\end{align*}\n\\]\n\n\nDiagonal \\(\\boldsymbol\\Omega\\)\nTODO"
  },
  {
    "objectID": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#fc-for-covariance-of-the-spatial-random-vector",
    "href": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#fc-for-covariance-of-the-spatial-random-vector",
    "title": "Step 11 - A Bayesian HM for multivariate Guassian data and MCAR prior - derivation of full conditionals and comments on computing",
    "section": "FC for Covariance of the Spatial Random Vector",
    "text": "FC for Covariance of the Spatial Random Vector\n\\[\n\\begin{align*}\np(\\boldsymbol\\Lambda \\,|\\, \\text{rest}) &\\propto p(\\{\\boldsymbol\\delta_i\\}, \\boldsymbol\\phi', \\boldsymbol\\beta, \\boldsymbol\\Omega, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto\np(\\boldsymbol\\phi'|\\boldsymbol\\Lambda,\\rho)p(\\boldsymbol\\Lambda)\\\\\n&\\propto\n\\big|(\\mathbf{D} - \\rho\\mathbf{W})^{-1} \\otimes \\boldsymbol\\Lambda\\big|^{-1/2} \\exp\\left\\{-\\frac{1}{2}\\left(\\boldsymbol\\phi'\\right)^T\\left((\\mathbf{D} - \\rho\\mathbf{W})^{-1}\\otimes\\boldsymbol\\Lambda\\right)^{-1}\\boldsymbol\\phi'\\right\\}\\\\\n&\\quad \\;\n\\times \\big|\\boldsymbol\\Lambda\\big|^{-(\\nu + p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}(\\boldsymbol\\Lambda^{-1}\\mathbf{R})\\right\\}\\\\\n&\\propto\n\\big|\\boldsymbol\\Lambda\\big|^{-p/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left(\\left((\\mathbf{D} - \\rho\\mathbf{W})\\otimes\\boldsymbol\\Lambda^{-1}\\right)\\boldsymbol\\phi'\\left(\\boldsymbol\\phi'\\right)^T\\right)\\right\\}\\\\\n&\\quad \\;\n\\times \\big|\\boldsymbol\\Lambda\\big|^{-(\\nu + p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}(\\boldsymbol\\Lambda^{-1}\\mathbf{R})\\right\\}\\\\\n&\\propto\n\\big|\\boldsymbol\\Lambda\\big|^{-p/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left(\\left((\\mathbf{D} - \\rho\\mathbf{W})\\otimes\\boldsymbol\\Lambda^{-1}\\right)\\left(\\boldsymbol\\phi'\\left(\\boldsymbol\\phi'\\right)^T \\otimes 1\\right)\\right)\\right\\}\\\\\n&\\quad \\;\n\\times \\big|\\boldsymbol\\Lambda\\big|^{-(\\nu + p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}(\\boldsymbol\\Lambda^{-1}\\mathbf{R})\\right\\}\\\\\n&= \\text{(now what?)}\n\\end{align*}\n\\] At this step I wanted to used the mixed-product property of Kronecker products to expand the trace, but this property only holds if the matrix products are well defined. Unfortunately, \\((\\mathbf{D}-\\rho\\mathbf{W})\\boldsymbol\\phi'(\\boldsymbol\\phi')^T\\) is not well defined because \\((\\mathbf{D}-\\rho\\mathbf{W})\\) is \\(n \\times n\\) and \\(\\boldsymbol\\phi'(\\boldsymbol\\phi')^T\\) is \\(np \\times np\\).\nWithout an immediate solution, let’s attempt to derive the full conditional for \\(\\boldsymbol\\Lambda\\) using the conditional distributions for \\(\\boldsymbol\\phi_i\\).\n\\[\n\\begin{align*}\np(\\boldsymbol\\Lambda \\,|\\, \\text{rest}) &\\propto p(\\{\\boldsymbol\\delta_i\\}, \\{\\boldsymbol\\phi_i\\}, \\boldsymbol\\beta, \\boldsymbol\\Omega, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto p(\\boldsymbol\\phi'|\\boldsymbol\\Lambda,\\rho)p(\\boldsymbol\\Lambda)\\\\\n&= \\frac{p(\\boldsymbol\\phi_1 \\,|\\, \\{\\boldsymbol\\phi_{i}\\}_{i=2}^n)}{p(\\boldsymbol\\phi_{1,0} \\,|\\, \\{\\boldsymbol\\phi_{i}\\}_{i=2}^n)} \\frac{p(\\boldsymbol\\phi_2 \\,|\\, \\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^1,\\{\\boldsymbol\\phi_{i}\\}_{i=3}^n)}{p(\\boldsymbol\\phi_{2,0} \\,|\\, \\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^1,\\{\\boldsymbol\\phi_{i}\\}_{i=3}^n)}\n\\times \\\\\n&\\quad \\quad \\vdots \\\\\n&\\quad \\;\\times\\frac{p(\\boldsymbol\\phi_k \\,|\\, \\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^{k-1},\\{\\boldsymbol\\phi_{i}\\}_{i={k+1}}^n)}{p(\\boldsymbol\\phi_{k,0} \\,|\\, \\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^{k-1},\\{\\boldsymbol\\phi_{i}\\}_{i={k+1}}^n)} \\times\\\\\n&\\quad \\quad \\vdots \\\\\n&\\quad \\;\\times\\frac{p(\\boldsymbol\\phi_n \\,| \\,\n\\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^{n-1})}{p(\\boldsymbol\\phi_{n,0} \\,| \\,\n\\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^{n-1})} p(\\boldsymbol\\phi' = \\boldsymbol\\phi'_0 \\,|\\, \\boldsymbol\\Lambda,\\rho)p(\\boldsymbol\\Lambda)\n\\end{align*}\n\\] where the last line follows by application of Brook’s Lemma to \\(p(\\boldsymbol\\phi'|\\boldsymbol\\Lambda,\\rho)\\). We’ve dropped \\(\\boldsymbol\\Lambda\\) and \\(\\rho\\) from conditioning notation in the resulting product of ratios, but the conditioning is very much still present. We choose \\(\\boldsymbol\\phi'_0 = \\boldsymbol{0}\\) for the fixed point in the support of \\(\\boldsymbol\\phi'\\). This will simplify things later and allow us to avoid the situation we ran into ealier.\n\n\n\n\n\n\nNote\n\n\n\nCould I have derived the full conditionals incorrectly for the univariate case? I think I just used the product of the conditionals there and not Brook’s Lemma. . . this might explain why my MCMC had issues with convergence. I should go back and check.\n\n\nWe have a particularly tedious product of ratios. Let’s focus on the \\(k\\)th term first, then consider the entire product to understand the form of the full conditional for \\(\\boldsymbol\\Lambda\\). Let \\[\n\\mathbf{m}_k = \\rho\\sum_{\\substack{j \\in \\mathcal{N}_k \\\\ j \\, > \\, k}}\\mathbf{B}_{kj}\\boldsymbol\\phi_j\n\\] Note that at the \\(k\\)th term we are considering the density as a function of some non-zero conditioned variates. The \\(\\boldsymbol\\phi_j\\) for \\(j < k\\) have been set to \\(\\mathbf{0}\\). This may affect the sum in the conditional mean, so we restrict the sum to terms that are non-zero, that is \\(\\boldsymbol\\phi_j\\) where \\(j \\in \\mathcal{N}_k\\) and \\(j > k\\).\nThen the \\(k\\)th term, \\[\n\\begin{align*}\n\\frac{p(\\boldsymbol\\phi_k \\,|\\, \\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^{k-1},\\{\\boldsymbol\\phi_{i}\\}_{i={k+1}}^n)}{p(\\boldsymbol\\phi_{k,0} \\,|\\, \\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^{k-1},\\{\\boldsymbol\\phi_{i}\\}_{i={k+1}}^n)}\n&=\n\\frac{\\exp\\left\\{-\\frac{w_{k+}}{2}\\left(\\boldsymbol\\phi_k-\\mathbf{m}_k\\right)^T\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol\\phi_k-\\mathbf{m}_k\\right)\\right\\}}{\\exp\\left\\{-\\frac{w_{k+}}{2}\\left(\\boldsymbol{0}-\\mathbf{m}_k\\right)^T\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol{0}-\\mathbf{m}_k\\right)\\right\\}}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\left(\\boldsymbol\\phi_k-\\mathbf{m}_k\\right)^T\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol\\phi_k-\\mathbf{m}_k\\right) + \\frac{w_{k+}}{2}\\mathbf{m}_k^T\\boldsymbol\\Lambda^{-1}\\mathbf{m}_k\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\left[\\left(\\boldsymbol\\phi_k-\\mathbf{m}_k\\right)^T\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol\\phi_k-\\mathbf{m}_k\\right) -\\mathbf{m}_k^T\\boldsymbol\\Lambda^{-1}\\mathbf{m}_k\\right]\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\left[\\boldsymbol\\phi^T_k\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k - 2\\mathbf{m}_k^T\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi^T_k\\right]\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\text{Tr}\\left[\\boldsymbol\\phi^T_k\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k - 2\\mathbf{m}_k^T\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k\\right]\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\left(\\text{Tr}\\left[\\boldsymbol\\phi^T_1\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k\\right] - \\text{Tr}\\left[ 2\\mathbf{m}_k^T\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k\\right]\\right)\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\left(\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k\\right] - \\text{Tr}\\left[ 2\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k\\mathbf{m}_k^T\\right]\\right)\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k -  2\\boldsymbol\\phi_k\\mathbf{m}_k^T\\right)\\right]\\right\\}\\\\\n\\end{align*}\n\\]\nThen the entire product will be \\[\n\\begin{align*}\np(\\boldsymbol\\phi'|\\boldsymbol\\Lambda,\\rho)\n&=\n\\left[\\prod_{k=1}^n \\frac{p(\\boldsymbol\\phi_k \\,|\\, \\{\\boldsymbol\\phi_{j,0} = \\mathbf{0}\\}_{j=1}^{k-1},\\{\\boldsymbol\\phi_{i}\\}_{i={k+1}}^n, \\boldsymbol\\Lambda, \\rho)}{p(\\boldsymbol\\phi_{k,0} = \\mathbf{0} \\,|\\, \\{\\boldsymbol\\phi_{j,0} = \\mathbf{0}\\}_{j=1}^{k-1},\\{\\boldsymbol\\phi_{i}\\}_{i={k+1}}^n, \\boldsymbol\\Lambda, \\rho)}\\right]p(\\boldsymbol\\phi' = \\mathbf{0} \\,|\\, \\boldsymbol\\Lambda,\\rho)\\\\\n&=\n\\left[\\prod_{k=1}^n \\exp\\left\\{-\\frac{w_{k+}}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k -  2\\boldsymbol\\phi_k\\mathbf{m}_k^T\\right)\\right]\\right\\}\\right]p(\\boldsymbol\\phi' = \\mathbf{0} \\,|\\, \\boldsymbol\\Lambda,\\rho)\\\\\n&=\n\\exp\\left\\{-\\frac{1}{2}\\sum_{k=1}^n\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1}w_{k+}\\left(\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k -  2\\boldsymbol\\phi_k\\mathbf{m}_k^T\\right)\\right]\\right\\}p(\\boldsymbol\\phi' = \\mathbf{0} \\,|\\, \\boldsymbol\\Lambda,\\rho)\\\\\n&=\n\\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1} \\sum_{k=1}^n \\mathbf{S}_k\\right]\\right\\} \\times \\big|(\\mathbf{D} - \\rho\\mathbf{W})^{-1} \\otimes \\boldsymbol\\Lambda\\big|^{-1/2} \\exp\\left\\{-\\frac{1}{2}\\left(\\mathbf{0}\\right)^T\\left((\\mathbf{D} - \\rho\\mathbf{W})^{-1}\\otimes\\boldsymbol\\Lambda\\right)^{-1}\\mathbf{0}\\right\\}\\\\\n&= \\big|(\\mathbf{D} - \\rho\\mathbf{W})^{-1} \\otimes \\boldsymbol\\Lambda\\big|^{-1/2}\n\\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1} \\sum_{k=1}^n \\mathbf{S}_k\\right]\\right\\}\n\\end{align*}\n\\]\nwhere \\(\\mathbf{S}_k = w_{k+}\\left(\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k - 2\\boldsymbol\\phi_k\\mathbf{m}_k^T\\right)\\).\nFinally, the full conditional for \\(\\boldsymbol\\Lambda\\) is \\[\n\\begin{align*}\np(\\boldsymbol\\Lambda \\,|\\, \\text{rest}) &\\propto p(\\{\\boldsymbol\\delta_i\\}, \\{\\boldsymbol\\phi_i\\}, \\boldsymbol\\beta, \\boldsymbol\\Omega, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto p(\\boldsymbol\\phi'|\\boldsymbol\\Lambda,\\rho)p(\\boldsymbol\\Lambda)\\\\\n&=\n\\big|(\\mathbf{D} - \\rho\\mathbf{W})^{-1} \\otimes \\boldsymbol\\Lambda\\big|^{-1/2}\n\\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1} \\sum_{k=1}^n \\mathbf{S}_k\\right]\\right\\}\\\\\n&\\quad \\;\\times\n\\big|\\boldsymbol\\Lambda\\big|^{-(\\nu + p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}(\\boldsymbol\\Lambda^{-1}\\mathbf{R})\\right\\}\\\\\n&\\propto\n\\big|\\boldsymbol\\Lambda\\big|^{-p/2}\n\\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1} \\sum_{k=1}^n \\mathbf{S}_k\\right]\\right\\}\\\\\n&\\quad \\;\\times\n\\big|\\boldsymbol\\Lambda\\big|^{-(\\nu + p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1}\\mathbf{R}\\right]\\right\\}\\\\\n&=\n\\big|\\boldsymbol\\Lambda\\big|^{-(\\nu + 2p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1}\\left(\\mathbf{R} + \\sum_{k=1}^n \\mathbf{S}_k\\right)\\right]\\right\\}\\\\\n\\end{align*}\n\\] and we conclude \\(\\boldsymbol\\Lambda \\,|\\, \\text{rest} \\sim \\text{InvWish}\\left(p + \\nu,\\, \\mathbf{R} + \\sum_{k=1}^n \\mathbf{S}_k\\right)\\)\nThis full conditional seems reasonable, but computing \\(\\sum_{k=1}^n \\mathbf{S}_k\\) may prove a bit tricky. Perhaps there are some simplifications to be made for this term in particular.\n\\[\n\\begin{align*}\n\\sum_{k=1}^n \\mathbf{S}_k &= \\sum_{k=1}^nw_{k+}\\left(\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k -  2\\boldsymbol\\phi_k\\mathbf{m}_k^T\\right)\\\\\n&= \\sum_{k=1}^nw_{k+}\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k - \\sum_{k=1}^nw_{k+}2\\boldsymbol\\phi_k\\mathbf{m}_k^T\\\\\n&= \\sum_{k=1}^nw_{k+}\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k - \\sum_{k=1}^nw_{k+}2\\boldsymbol\\phi_k\\rho\\sum_{\\substack{j \\in \\mathcal{N}_k \\\\ j \\, > \\, k}}\\boldsymbol\\phi_j^T\\mathbf{B}^T_{kj}\\\\\n&= \\mathbf{D}\\boldsymbol\\Phi^T\\boldsymbol\\Phi - 2\\rho\\sum_{k=1}^nw_{k+}\\boldsymbol\\phi_k\\sum_{\\substack{j \\in \\mathcal{N}_k \\\\ j \\, > \\, k}}\\boldsymbol\\phi_j^T\\mathbf{B}^T_{kj}\\\\\n&= \\mathbf{D}\\boldsymbol\\Phi^T\\boldsymbol\\Phi - 2\\rho\\sum_{k=1}^nw_{k+}\\boldsymbol\\phi_k\\sum_{\\substack{j \\in \\mathcal{N}_k \\\\ j \\, > \\, k}}\\frac{w_{kj}}{w_{k+}}\\boldsymbol\\phi_j^T\\mathbf{I}_p \\quad \\quad \\text{set } \\mathbf{B}_{kj} = \\frac{w_{kj}}{w_{k+}}\\mathbf{I}_p\\\\\n&= \\mathbf{D}\\boldsymbol\\Phi^T\\boldsymbol\\Phi - 2\\rho\\sum_{k=1}^n\\boldsymbol\\phi_k\\sum_{\\substack{j \\in \\mathcal{N}_k \\\\ j \\, > \\, k}}w_{kj}\\boldsymbol\\phi_j^T\\\\\n&= \\mathbf{D}\\boldsymbol\\Phi^T\\boldsymbol\\Phi - 2\\rho\\boldsymbol\\Phi^T\\mathbf{W}_{U}\\boldsymbol\\Phi\\\\\n&=\n\\end{align*}\n\\] Where \\(\\mathbf{W}_U\\) is ab \\(n \\times n\\) upper triangular matrix constructed by zeroing out the lower triangle of \\(\\mathbf{W}\\). This has the affect of setting \\(\\boldsymbol\\phi_j = \\boldsymbol 0\\) for \\(j \\le k\\) which defines the sum \\(\\sum_{\\substack{j \\in \\mathcal{N}_k \\\\ j \\, > \\, k}}w_{kj}\\boldsymbol\\phi_j^T\\).\nIt is important to note that \\(\\boldsymbol\\Phi\\) is constructed with rows equal to \\(\\boldsymbol\\phi_k\\) which results in flipping \\(\\boldsymbol\\Phi\\boldsymbol\\Phi^T\\), what we would expect, to instead be \\(\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\).\n\n\n\n\n\n\nNote\n\n\n\nThis is suspiciously close to \\((\\mathbf{D}-\\rho\\mathbf{W})\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\). Perhaps \\(2\\boldsymbol\\Phi^T\\mathbf{W}_U\\boldsymbol\\Phi = \\boldsymbol\\Phi^T\\mathbf{W}\\boldsymbol\\Phi\\)? It would be nice, but actually I don’t think this is true."
  },
  {
    "objectID": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#metropolis-hastings-step-for-spatial-stength-parameter-rho",
    "href": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#metropolis-hastings-step-for-spatial-stength-parameter-rho",
    "title": "Step 11 - A Bayesian HM for multivariate Guassian data and MCAR prior - derivation of full conditionals and comments on computing",
    "section": "Metropolis-Hastings Step for Spatial Stength parameter \\(\\rho\\)",
    "text": "Metropolis-Hastings Step for Spatial Stength parameter \\(\\rho\\)\nTODO"
  },
  {
    "objectID": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#fc-for-fixed-effects---boldsymbolbeta",
    "href": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#fc-for-fixed-effects---boldsymbolbeta",
    "title": "Step 11 - A Bayesian HM for multivariate Guassian data and MCAR prior - derivation of full conditionals and comments on computing",
    "section": "FC for Fixed effects - \\(\\boldsymbol\\beta\\)",
    "text": "FC for Fixed effects - \\(\\boldsymbol\\beta\\)\nStart with the full conditional for \\(\\boldsymbol\\beta\\). \\[\n\\begin{align*}\np(\\boldsymbol\\beta \\,|\\, \\text{rest}) &\\propto p(\\{\\boldsymbol\\delta_i\\}, \\boldsymbol\\phi', \\boldsymbol\\beta, \\boldsymbol\\Omega, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto p(\\{\\boldsymbol\\delta_i\\} \\,|\\, \\boldsymbol\\beta, \\boldsymbol\\phi', \\boldsymbol\\Omega)p(\\boldsymbol\\beta)\\\\\n&= \\left[\\prod_{i=1}^n p(\\boldsymbol\\delta_i \\,|\\, \\boldsymbol\\beta,\\boldsymbol\\phi_i,\\boldsymbol\\Omega)\\right]p(\\boldsymbol\\beta)\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^n\\left[\\boldsymbol\\delta_i-(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\boldsymbol\\phi_i)\\right]^T\\boldsymbol\\Omega^{-1}\\left[\\boldsymbol\\delta_i-(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\boldsymbol\\phi_i)\\right]\\right\\}\\\\\n&\\quad \\, \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol\\beta-\\boldsymbol\\mu\\right]^T\\boldsymbol\\Gamma^{-1}\\left[\\boldsymbol\\beta-\\boldsymbol\\mu\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol\\beta^T\\left(\\sum_{i=1}^n\\boldsymbol{\\mathcal{Z}}_i^T\\boldsymbol\\Omega^{-1}\\boldsymbol{\\mathcal{Z}}_i\\right)\\boldsymbol\\beta - 2\\left(\\sum_{i=1}^n(\\boldsymbol\\delta_i-\\boldsymbol\\phi_i)^T\\boldsymbol\\Omega^{-1}\\boldsymbol{\\mathcal{Z}}_i\\right)\\boldsymbol\\beta\\right]\\right\\}\\\\\n&\\quad \\, \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol\\beta^T\\boldsymbol\\Gamma^{-1}\\boldsymbol\\beta - 2\\boldsymbol\\mu^T\\boldsymbol\\Gamma^{-1}\\boldsymbol\\beta\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol\\beta^T\\left(\\sum_{i=1}^n\\boldsymbol{\\mathcal{Z}}_i^T\\boldsymbol\\Omega^{-1}\\boldsymbol{\\mathcal{Z}}_i + \\boldsymbol\\Gamma^{-1}\\right)\\boldsymbol\\beta - 2\\left(\\sum_{i=1}^n(\\boldsymbol\\delta_i-\\boldsymbol\\phi_i)^T\\boldsymbol\\Omega^{-1}\\boldsymbol{\\mathcal{Z}}_i + \\boldsymbol\\mu^T\\boldsymbol\\Gamma^{-1}\\right)\\boldsymbol\\beta\\right]\\right\\}\n\\end{align*}\n\\] Thus, \\[\n\\begin{align*}\n\\boldsymbol\\beta\\,|\\,\\text{rest} &\\sim \\text{Normal}_{qp}(\\mathbf{V}_\\beta^{-1}\\mathbf{M}_\\beta, \\mathbf{V}_\\beta^{-1})\\\\\n\\mathbf{V}_\\beta &= \\sum_{i=1}^n\\boldsymbol{\\mathcal{Z}}^T_i\\mathbf{\\Omega}^{-1}\\boldsymbol{\\mathcal{Z}}_i + \\mathbf{\\Gamma}^{-1}\\\\\n\\mathbf{M}_\\beta &= \\sum_{i=1}^n\\boldsymbol{\\mathcal{Z}}^T_i\\mathbf{\\Omega}^{-1}(\\boldsymbol\\delta_i-\\boldsymbol\\phi_i) + \\mathbf{\\Gamma}^{-1}\\boldsymbol\\mu\n\\end{align*}\n\\]"
  },
  {
    "objectID": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#fc-for-spatial-random-vector---boldsymbolphi_i-or-boldsymbolphi",
    "href": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#fc-for-spatial-random-vector---boldsymbolphi_i-or-boldsymbolphi",
    "title": "Step 11 - A Bayesian HM for multivariate Guassian data and MCAR prior - derivation of full conditionals and comments on computing",
    "section": "FC for Spatial Random Vector - \\(\\boldsymbol\\phi_i\\) or \\(\\boldsymbol\\phi'\\)",
    "text": "FC for Spatial Random Vector - \\(\\boldsymbol\\phi_i\\) or \\(\\boldsymbol\\phi'\\)\nNext we must determine the full conditional distribution for the spatial random vector. There are two approaches here, we could either derive full conditionals for \\(\\boldsymbol\\phi_i\\) and draw samples in \\(n\\) sub-blocks of parameters, or we could derive the full conditional for \\(\\boldsymbol\\phi'\\) and update all \\(\\boldsymbol\\phi_i\\) in a single block. It is hard to say which would be computationally preferable. The latter seems more interesting, though, so let’s try to tackle that first, then derive the full conditionals for \\(\\boldsymbol\\phi_i\\).\n\nJoint FC approach - \\(\\boldsymbol\\phi'\\)\nThe trickiness of updating \\(\\boldsymbol\\phi'\\) as a single block is due to our model specification for \\(\\boldsymbol\\delta_i\\). The likelihood for \\(\\boldsymbol\\delta_i\\) is not compatible with the prior on \\(\\boldsymbol\\phi'\\) because the spatial term in the former is specified in terms of \\(p \\times 1\\) vectors while in the latter spatial term is a \\(np \\times 1\\) vector.\nWe could either construct the joint distribution of the \\(\\boldsymbol\\delta_i\\) vectors or we could try to write \\(\\boldsymbol\\phi_i\\) in terms of \\(\\boldsymbol\\phi'\\) and substitute into the corresponding \\(\\boldsymbol\\delta_i\\) distributions.\nLet, \\[\n\\boldsymbol\\Delta =\n\\begin{pmatrix}\n\\delta_{11} & \\dots & \\delta_{1p}\\\\\n\\delta_{21} & \\dots & \\delta_{2p}\\\\\n\\vdots & \\vdots & \\vdots\\\\\n\\delta_{n1} & \\dots & \\delta_{np}\\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\boldsymbol\\delta_1^T\\\\\n\\boldsymbol\\delta_2^T\\\\\n\\vdots\\\\\n\\boldsymbol\\delta_n^T\\\\\n\\end{pmatrix}\n\\]\nThen define \\(\\boldsymbol\\delta' = \\text{vec}(\\boldsymbol\\Delta^T)\\). Since \\(\\boldsymbol\\delta_i\\) are conditionally independent, the joint distribution for \\(\\boldsymbol\\delta'\\) is\n\\[\n\\boldsymbol\\delta' \\sim \\text{Normal}_{np}\n\\left(\n\\begin{pmatrix}\n\\boldsymbol{\\mathcal{Z}}_1\\boldsymbol\\beta\\\\\n\\boldsymbol{\\mathcal{Z}}_2\\boldsymbol\\beta\\\\\n\\vdots \\\\\n\\boldsymbol{\\mathcal{Z}}_n\\boldsymbol\\beta\\\\\n\\end{pmatrix} + \\boldsymbol\\phi', \\;\n\\mathbf{I}_n \\otimes \\boldsymbol\\Omega\n\\right)\n\\] Again, the purpose of constructing the joint distribution in this way was to ensure the mean is in terms of \\(\\boldsymbol\\phi'\\). But now the fixed effects term is a bit awkward. Let’s avoid that temporarily by letting \\[\n\\mathbf{A} =\n\\begin{pmatrix}\n\\boldsymbol{\\mathcal{Z}}_1\\boldsymbol\\beta\\\\\n\\boldsymbol{\\mathcal{Z}}_2\\boldsymbol\\beta\\\\\n\\vdots \\\\\n\\boldsymbol{\\mathcal{Z}}_n\\boldsymbol\\beta\\\\\n\\end{pmatrix}.\n\\]\nThen if we proceed in deriving the full conditional for \\(\\boldsymbol\\phi'\\), we will have\n\\[\n\\begin{align*}\np(\\boldsymbol\\phi' \\,|\\, \\text{rest}) &\\propto p(\\boldsymbol\\delta', \\boldsymbol\\phi', \\boldsymbol\\beta, \\boldsymbol\\Omega, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto p(\\{\\boldsymbol\\delta' \\,|\\, \\boldsymbol\\beta, \\boldsymbol\\phi', \\boldsymbol\\Omega)p(\\boldsymbol\\phi')\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol\\delta'-(\\mathbf{A} + \\boldsymbol\\phi')\\right]^T\\left(\\mathbf{I}_n \\otimes \\boldsymbol\\Omega\\right)^{-1}\\left[\\boldsymbol\\delta_i-(\\mathbf{A} + \\boldsymbol\\phi')\\right]\\right\\}\\\\\n&\\quad \\, \\times \\exp\\left\\{-\\frac{1}{2}(\\boldsymbol\\phi')^T\\left((\\mathbf{D-\\rho\\mathbf{W}}) \\otimes \\boldsymbol\\Lambda^{-1}\\right)\\boldsymbol\\phi'\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[(\\boldsymbol\\phi')^T\\left(\\mathbf{I}_n \\otimes \\boldsymbol\\Omega\\right)^{-1}\\boldsymbol\\phi' - 2\\left((\\boldsymbol\\delta'-\\mathbf{A})^T\\left(\\mathbf{I}_n \\otimes \\boldsymbol\\Omega\\right)^{-1}\\right)\\boldsymbol\\phi'\\right]\\right\\}\\\\\n&\\quad \\, \\times \\exp\\left\\{-\\frac{1}{2}(\\boldsymbol\\phi')^T\\left((\\mathbf{D-\\rho\\mathbf{W}}) \\otimes \\boldsymbol\\Lambda^{-1}\\right)\\boldsymbol\\phi'\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[(\\boldsymbol\\phi')^T\\left(\\left(\\mathbf{I}_n \\otimes \\boldsymbol\\Omega\\right)^{-1} + \\left((\\mathbf{D-\\rho\\mathbf{W}}) \\otimes \\boldsymbol\\Lambda^{-1}\\right)\\right)\\boldsymbol\\phi' - 2\\left((\\boldsymbol\\delta'-\\mathbf{A})^T\\left(\\mathbf{I}_n \\otimes \\boldsymbol\\Omega\\right)^{-1}\\right)\\boldsymbol\\phi'\\right]\\right\\}\n\\end{align*}\n\\] Thus,\n\\[\n\\begin{align*}\n\\boldsymbol\\phi'\\,|\\,\\text{rest} &\\sim \\text{Normal}_{np}(\\mathbf{V}_{\\phi'}^{-1}\\mathbf{M}_{\\phi'}, \\mathbf{V}_{\\phi'}^{-1})\\\\\n\\mathbf{V}_{\\phi'} &= \\left(\\mathbf{I}_n \\otimes \\boldsymbol\\Omega\\right)^{-1} + \\left((\\mathbf{D-\\rho\\mathbf{W}}) \\otimes \\boldsymbol\\Lambda^{-1}\\right)\\\\\n\\mathbf{M}_{\\phi'} &= \\left(\\mathbf{I}_n \\otimes \\boldsymbol\\Omega\\right)^{-1}(\\boldsymbol\\delta'-\\mathbf{A})\n\\end{align*}\n\\]\nThis full conditional looks to be pretty clean, though we will need to consider efficient ways to compute \\(\\mathbf{A}\\). Before moving on, let’s explore an alternative derivation for the full conditional of \\(\\boldsymbol\\phi'\\).\nIf we instead wrote \\(\\boldsymbol\\phi_i\\) in terms of \\(\\boldsymbol\\phi'\\) when describing the distribution of \\(\\boldsymbol\\delta_i\\)? Consider the way in which one “extracts” a sub-vector of elements from a vector. We know there exists a \\(p \\times np\\) matrix \\(\\mathbf{U}_i\\) which will “extract” the \\(p \\times 1\\) dimension \\(\\boldsymbol\\phi_i\\) vector from the \\(np \\times 1\\) dimenion \\(\\boldsymbol\\phi'\\) vector. This \\(\\mathbf{U}_i\\) matrix will have all elements equal to \\(0\\) except for \\(p\\) columns forming a \\(p \\times p\\) identity matrix. The position of this \\(p \\times p\\) identity matrix in \\(\\mathbf{U}_i\\) determines which \\(\\boldsymbol\\phi_i\\) to “extract” from \\(\\boldsymbol\\phi'\\). In order to extract \\(\\boldsymbol\\phi_i\\), then the \\(p \\times p\\) identity matrix must begin at column \\((i-1)p + 1\\) in \\(\\mathbf{U}_i\\)\n\\[\n\\begin{align*}\n\\boldsymbol\\phi_i &=\n\\mathbf{U}_i\\boldsymbol\\phi'\\\\\n&= \\begin{pmatrix}\n\\boldsymbol{0}_p & \\dots & \\boldsymbol{0}_p & \\mathbf{I}_p & \\boldsymbol{0}_p & \\dots & \\boldsymbol{0}_p\n\\end{pmatrix}\n\\boldsymbol\\phi'\n\\end{align*}\n\\] where \\(\\boldsymbol 0_p\\) is a \\(p \\times p\\) matrix of \\(0\\)’s. Now we can write\n\\[\n\\boldsymbol\\delta_i \\sim \\text{Normal}_p\\left(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\mathbf{U}_i\\boldsymbol\\phi', \\;  \\boldsymbol\\Omega\\right)\n\\] which will facilitate the derivation of the full conditional for \\(\\boldsymbol\\phi'\\).\n\\[\n\\begin{align*}\np(\\boldsymbol\\phi' \\,|\\, \\text{rest}) &\\propto p(\\{\\boldsymbol\\delta_i\\}, \\boldsymbol\\phi', \\boldsymbol\\beta, \\boldsymbol\\Omega, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto p(\\{\\boldsymbol\\delta_i\\} \\,|\\, \\boldsymbol\\beta, \\boldsymbol\\phi', \\boldsymbol\\Omega)p(\\boldsymbol\\phi')\\\\\n&= \\left[\\prod_{i=1}^n p(\\boldsymbol\\delta_i \\,|\\, \\boldsymbol\\beta,\\boldsymbol\\phi',\\boldsymbol\\Omega)\\right]p(\\boldsymbol\\phi')\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^n\\left[\\boldsymbol\\delta_i-(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\mathbf{U}_i\\boldsymbol\\phi')\\right]^T\\boldsymbol\\Omega^{-1}\\left[\\boldsymbol\\delta_i-(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\mathbf{U}_i\\boldsymbol\\phi')\\right]\\right\\}\\\\\n&\\quad \\, \\times \\exp\\left\\{-\\frac{1}{2}(\\boldsymbol\\phi')^T\\left((\\mathbf{D-\\rho\\mathbf{W}}) \\otimes \\boldsymbol\\Lambda^{-1}\\right)\\boldsymbol\\phi'\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[(\\boldsymbol\\phi')^T\\left(\\sum_{i=1}^n\\mathbf{U}_i^T\\boldsymbol\\Omega^{-1}\\mathbf{U}_i\\right)\\boldsymbol\\phi' - 2\\left(\\sum_{i=1}^n(\\boldsymbol\\delta_i-\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta)^T\\boldsymbol\\Omega^{-1}\\mathbf{U}_i\\right)\\boldsymbol\\phi'\\right]\\right\\}\\\\\n&\\quad \\, \\times \\exp\\left\\{-\\frac{1}{2}(\\boldsymbol\\phi')^T\\left((\\mathbf{D-\\rho\\mathbf{W}}) \\otimes \\boldsymbol\\Lambda^{-1}\\right)\\boldsymbol\\phi'\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[(\\boldsymbol\\phi')^T\\left(\\sum_{i=1}^n\\mathbf{U}_i^T\\boldsymbol\\Omega^{-1}\\mathbf{U}_i + \\left((\\mathbf{D-\\rho\\mathbf{W}}) \\otimes \\boldsymbol\\Lambda^{-1}\\right)\\right)\\boldsymbol\\phi' - 2\\left(\\sum_{i=1}^n(\\boldsymbol\\delta_i-\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta)^T\\boldsymbol\\Omega^{-1}\\mathbf{U}_i\\right)\\boldsymbol\\phi'\\right]\\right\\}\n\\end{align*}\n\\] Thus, \\[\n\\begin{align*}\n\\boldsymbol\\phi'\\,|\\,\\text{rest} &\\sim \\text{Normal}_{np}(\\mathbf{V}_{\\phi'}^{-1}\\mathbf{M}_{\\phi'}, \\mathbf{V}_{\\phi'}^{-1})\\\\\n\\mathbf{V}_{\\phi'} &= \\sum_{i=1}^n\\mathbf{U}_i^T\\boldsymbol\\Omega^{-1}\\mathbf{U}_i + \\left((\\mathbf{D-\\rho\\mathbf{W}}) \\otimes \\boldsymbol\\Lambda^{-1}\\right)\\\\\n\\mathbf{M}_{\\phi'} &= \\sum_{i=1}^n\\mathbf{U}^T_i\\mathbf{\\Omega}^{-1}(\\boldsymbol\\delta_i-\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta)\n\\end{align*}\n\\]\nThese two full conditionals for \\(\\boldsymbol\\phi'\\) are equivalent. To see this we need to apply an identity previously described in Step 3 during the discussion of the full conditional for \\(\\boldsymbol\\beta\\). In Step 3, we considered an organization of the covariates \\(\\mathbf{z}_i\\), into a matrix \\(\\mathbf{Z} = (\\mathbf{z}_1^T, \\dots,\\mathbf{z}_N^T)^T\\) \\[\n\\begin{equation}\n\\tag{3}\n\\mathbf{Z} =\n\\begin{bmatrix}\n1 & z_{12} & \\dots & z_{1q}\\\\\n\\vdots & & \\vdots &\\\\\n1 & z_{N2} & \\dots & z_{Nq}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{z}_1^T\\\\\n\\vdots\\\\\n\\mathbf{z}_N^T\\\\\n\\end{bmatrix}.\n\\end{equation}\n\\] Notice, \\(\\boldsymbol{\\mathcal{Z}}_i = \\mathbf{z}^T_i \\otimes \\mathbf{I}_p\\). Then the following relate the sums to Kronecker products, (make this more general?)\n\\[\n\\begin{equation}\n\\tag{4}\n\\sum_{i=1}^N\\boldsymbol{\\mathcal{Z}}^T_i\\boldsymbol\\Omega^{-1}\\boldsymbol{\\mathcal{Z}}_i = \\mathbf{Z}^T\\mathbf{Z} \\otimes \\boldsymbol\\Omega^{-1}\n\\end{equation}\n\\]\n\\[\n\\begin{equation}\n\\tag{5}\n\\sum_{i=1}^N\\boldsymbol{\\mathcal{Z}}^T_i\\mathbf{\\Omega}^{-1}\\boldsymbol\\delta_i =  \\text{vec}(\\Omega^{-1}\\boldsymbol\\Delta\\mathbf{Z}) = (\\mathbf{Z}^T \\otimes \\, \\boldsymbol\\Omega^{-1})\\text{vec}(\\boldsymbol\\Delta).\n\\end{equation}\n\\] In the case of the full conditional for \\(\\boldsymbol\\phi'\\), we have \\(\\mathbf{U}_i = \\mathbf{u}^T_i \\otimes I_n\\) where \\(\\mathbf{u}_i\\) is an elementary vector with \\(1\\) in position \\(i\\) and \\(0\\) elsewhere. Stacking rows of \\(\\mathbf{u}_i^T\\) into a matrix constructs the identity matrix \\(\\mathbf{I}_n\\). So, substituting \\(\\mathbf{U}_i\\) for \\(\\boldsymbol{\\mathcal{Z}}_i\\) and \\(\\mathbf{I}_n\\) for \\(\\mathbf{Z}\\) yields\n\\[\n\\begin{equation}\n\\tag{4}\n\\sum_{i=1}^N\\mathbf{U}_i^T\\boldsymbol\\Omega^{-1}\\mathbf{U}_i = \\mathbf{I}_n^T\\mathbf{I}_n \\otimes \\boldsymbol\\Omega^{-1} = \\mathbf{I}_n \\otimes \\boldsymbol\\Omega^{-1}\n\\end{equation}\n\\]\nAlso, substituting \\(\\mathbf{U}_i\\) for \\(\\boldsymbol{\\mathcal{Z}}_i\\) and \\(\\boldsymbol\\delta_i - \\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta\\) for \\(\\boldsymbol\\delta_i\\) yields \\[\n\\begin{equation}\n\\tag{5}\n\\sum_{i=1}^N\\mathbf{U}_i\\mathbf{\\Omega}^{-1}(\\boldsymbol\\delta_i - \\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta) =  \\text{vec}(\\Omega^{-1}\\boldsymbol\\Delta\\mathbf{I}_n) = (\\mathbf{I}_n^T \\otimes \\, \\boldsymbol\\Omega^{-1})(\\boldsymbol\\delta'-\\mathbf{A}).\n\\end{equation}\n\\] This shows that the two approaches to deriving the full conditional for \\(\\boldsymbol\\phi'\\) yield the same results.\nReturning to \\(\\mathbf{A}\\), which is necessary to compute in the block full conditional for \\(\\boldsymbol\\phi'\\). We recognize that\n\\[\n\\begin{align*}\n\\mathbf{A} &=\n\\begin{pmatrix}\n\\boldsymbol{\\mathcal{Z}}_1\\boldsymbol\\beta\\\\\n\\vdots \\\\\n\\boldsymbol{\\mathcal{Z}}_n\\boldsymbol\\beta\\\\\n\\end{pmatrix}\\\\\n&=\n\\begin{pmatrix}\n\\mathbf{B}\\mathbf{z_1}\\\\\n\\vdots \\\\\n\\mathbf{B}\\mathbf{z_n}\\\\\n\\end{pmatrix}\\\\\n&= \\text{vec}(\\mathbf{B}\\mathbf{Z^T})\n\\end{align*}\n\\] So we can substitute, \\[\n(\\boldsymbol\\delta' - \\mathbf{A}) = \\text{vec}(\\boldsymbol\\Delta-\\mathbf{B}\\mathbf{Z}^T)\n\\] in the full conditional for \\(\\boldsymbol\\phi'\\).\n\n\nMarginal (site-by-site) FC approach - \\(\\boldsymbol\\phi_i\\)\nWe might desire to sample full conditionals for \\(\\boldsymbol\\phi_i\\) rather than the full joint distribution for \\(\\boldsymbol\\phi'\\).\n\\[\n\\begin{align*}\np(\\boldsymbol\\phi_i \\,|\\, \\text{rest})\n&\\propto\np(\\{\\boldsymbol\\delta_i\\}, \\{\\boldsymbol\\phi_i\\}, \\boldsymbol\\beta, \\boldsymbol\\Omega, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto\np(\\boldsymbol\\delta_i \\,|\\, \\boldsymbol\\phi_i, \\boldsymbol\\beta, \\boldsymbol\\Omega)p(\\boldsymbol\\phi_i \\,|\\, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto\n\\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol\\delta_i-(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\boldsymbol\\phi_i)\\right]^T\\boldsymbol\\Omega^{-1}\\left[\\boldsymbol\\delta_i-(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta + \\boldsymbol\\phi_i)\\right]\\right\\}\\\\\n&\\quad \\, \\times\n\\exp\\left\\{-\\frac{w_{i+}}{2}\\left(\\boldsymbol\\phi_i - \\rho\\sum_{j \\in \\mathcal{N}_i}\\mathbf{B}_{ij}\\boldsymbol\\phi_j\\right)^T\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol\\phi_i - \\rho\\sum_{j \\in \\mathcal{N}_i}\\mathbf{B}_{ij}\\boldsymbol\\phi_j\\right)\\right\\}\\\\\n&\\propto\n\\exp\\left\\{-\\frac{1}{2}\\left[(\\boldsymbol\\phi_i)^T\\boldsymbol\\Omega^{-1}\\boldsymbol\\phi_i - 2\\left(\\boldsymbol\\delta_i^T -\\boldsymbol\\beta^T\\boldsymbol{\\mathcal{Z}}^T_i\\right)\\boldsymbol\\Omega^{-1}\\boldsymbol\\phi_i\\right]\\right\\}\\\\\n&\\quad \\, \\times\n\\exp\\left\\{-\\frac{1}{2}\\left[(\\boldsymbol\\phi_i)^Tw_{i+}\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_i - 2\\rho w_{i+}\\left(\\sum_{j \\in \\mathcal{N}_i}\\boldsymbol\\phi_j^T\\mathbf{B}_{ij}^T\\right)\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_i\\right]\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{1}{2}\\left[(\\boldsymbol\\phi_i)^T\\left(\\boldsymbol\\Omega^{-1} + w_{i+}\\boldsymbol\\Lambda^{-1}\\right)\\boldsymbol\\phi_i - 2\\left[\\left(\\boldsymbol\\delta_i^T -\\boldsymbol\\beta^T\\boldsymbol{\\mathcal{Z}}^T_i\\right)\\boldsymbol\\Omega^{-1} + \\rho w_{i+}\\left(\\sum_{j \\in \\mathcal{N}_i}\\boldsymbol\\phi_j^T\\mathbf{B}_{ij}^T\\right)\\boldsymbol\\Lambda^{-1}\\right]\\boldsymbol\\phi_i\\right]\\right\\}\\\\\n\\end{align*}\n\\]\nThus,\n\\[\n\\begin{align*}\n\\boldsymbol\\phi_i\\,|\\,\\text{rest} &\\sim \\text{Normal}_{p}(\\mathbf{V}_{\\phi_i}^{-1}\\mathbf{M}_{\\phi_i}, \\mathbf{V}_{\\phi_i}^{-1})\\\\\n\\mathbf{V}_{\\phi_i} &= \\boldsymbol\\Omega^{-1} + w_{i+}\\boldsymbol\\Lambda^{-1}\\\\\n\\mathbf{M}_{\\phi_i} &= \\boldsymbol\\Omega^{-1}\\left(\\boldsymbol\\delta_i-\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta\\right) + \\rho w_{i+}\\boldsymbol\\Lambda^{-1}\\sum_{j \\in \\mathcal{N}_i}\\mathbf{B}_{ij}\\boldsymbol\\phi_j\n\\end{align*}\n\\] taking \\(\\mathbf{B}_{ij} = \\frac{w_{ij}}{w_{i+}}\\mathbf{I}_p\\), we will have \\(\\mathbf{M}_{\\phi_i} = \\boldsymbol\\Omega^{-1}\\left(\\boldsymbol\\delta_i-\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta\\right) + \\rho \\boldsymbol\\Lambda^{-1}\\sum_{j \\in \\mathcal{N}_i}w_{ij}\\boldsymbol\\phi_j\\)."
  },
  {
    "objectID": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#fc-for-covariance-of-the-spatial-random-vector---boldsymbollambda",
    "href": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#fc-for-covariance-of-the-spatial-random-vector---boldsymbollambda",
    "title": "Step 11 - A Bayesian HM for multivariate Guassian data and MCAR prior - derivation of full conditionals and comments on computing",
    "section": "FC for Covariance of the Spatial Random Vector - \\(\\boldsymbol\\Lambda\\)",
    "text": "FC for Covariance of the Spatial Random Vector - \\(\\boldsymbol\\Lambda\\)\n\nUsing joint distribution for \\(\\boldsymbol\\phi'\\)\n\\[\n\\begin{align*}\np(\\boldsymbol\\Lambda \\,|\\, \\text{rest}) &\\propto p(\\{\\boldsymbol\\delta_i\\}, \\boldsymbol\\phi', \\boldsymbol\\beta, \\boldsymbol\\Omega, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto\np(\\boldsymbol\\phi'|\\boldsymbol\\Lambda,\\rho)p(\\boldsymbol\\Lambda)\\\\\n&\\propto\n\\big|(\\mathbf{D} - \\rho\\mathbf{W})^{-1} \\otimes \\boldsymbol\\Lambda\\big|^{-1/2} \\exp\\left\\{-\\frac{1}{2}\\left(\\boldsymbol\\phi'\\right)^T\\left((\\mathbf{D} - \\rho\\mathbf{W})^{-1}\\otimes\\boldsymbol\\Lambda\\right)^{-1}\\boldsymbol\\phi'\\right\\}\\\\\n&\\quad \\;\n\\times \\big|\\boldsymbol\\Lambda\\big|^{-(\\nu + p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}(\\boldsymbol\\Lambda^{-1}\\mathbf{R})\\right\\}\\\\\n&\\propto\n\\big|\\boldsymbol\\Lambda\\big|^{-p/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left(\\left(\\boldsymbol\\phi'\\right)^T\\left((\\mathbf{D} - \\rho\\mathbf{W})\\otimes\\boldsymbol\\Lambda^{-1}\\right)\\boldsymbol\\phi'\\right)\\right\\}\\\\\n&\\quad \\;\n\\times \\big|\\boldsymbol\\Lambda\\big|^{-(\\nu + p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}(\\boldsymbol\\Lambda^{-1}\\mathbf{R})\\right\\}\\\\\n&\\propto\n\\big|\\boldsymbol\\Lambda\\big|^{-p/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left(\\boldsymbol\\Lambda^{-1} \\boldsymbol\\Phi^T(\\mathbf{D}-\\rho\\mathbf{W})\\boldsymbol\\Phi\\right)\\right\\}\\\\\n&\\quad \\;\n\\times \\big|\\boldsymbol\\Lambda\\big|^{-(\\nu + p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}(\\boldsymbol\\Lambda^{-1}\\mathbf{R})\\right\\}\\\\\n&= \\big|\\boldsymbol\\Lambda\\big|^{-(p +\\nu + p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}(\\boldsymbol\\Lambda^{-1}\\left(\\mathbf{R} + \\boldsymbol\\Phi^T(\\mathbf{D}-\\rho\\mathbf{W})\\boldsymbol\\Phi \\right)\\right\\}\n\\end{align*}\n\\] Thus, \\[\n\\begin{align*}\n\\boldsymbol\\Lambda \\,|\\, \\text{rest} &\\sim \\text{InvWish}\\left(p + \\nu, \\mathbf{R}+\\mathbf{S}\\right)\\\\\np &= \\text{dim}(\\boldsymbol\\phi_i)\\\\\n\\mathbf{S} &= \\boldsymbol\\Phi^T(\\mathbf{D}-\\rho\\mathbf{W})\\boldsymbol\\Phi\n\\end{align*}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nI believe I’ve completed this derivation. The key trick is found in Aspects of Multivariate Statistical Theory by Robb J. Muirhead on page 76, Lemma 2.2.3. Interestingly it is not immediately obvious that it agrees with the following derivation using Brook’s Lemma. Perhaps that is something worth checking. The route using Brook’s Lemma was tedious. I would not be surprised if I made some errors there.\n\n\n\n\nUsing conditional distributions for \\(\\boldsymbol\\phi_i\\) and Brook’s Lemma\nAn alternative path, let’s attempt to derive the full conditional for \\(\\boldsymbol\\Lambda\\) using the conditional distributions for \\(\\boldsymbol\\phi_i\\).\n\\[\n\\begin{align*}\np(\\boldsymbol\\Lambda \\,|\\, \\text{rest}) &\\propto p(\\{\\boldsymbol\\delta_i\\}, \\{\\boldsymbol\\phi_i\\}, \\boldsymbol\\beta, \\boldsymbol\\Omega, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto p(\\boldsymbol\\phi'|\\boldsymbol\\Lambda,\\rho)p(\\boldsymbol\\Lambda)\\\\\n&= \\frac{p(\\boldsymbol\\phi_1 \\,|\\, \\{\\boldsymbol\\phi_{i}\\}_{i=2}^n)}{p(\\boldsymbol\\phi_{1,0} \\,|\\, \\{\\boldsymbol\\phi_{i}\\}_{i=2}^n)} \\frac{p(\\boldsymbol\\phi_2 \\,|\\, \\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^1,\\{\\boldsymbol\\phi_{i}\\}_{i=3}^n)}{p(\\boldsymbol\\phi_{2,0} \\,|\\, \\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^1,\\{\\boldsymbol\\phi_{i}\\}_{i=3}^n)}\n\\times \\\\\n&\\quad \\quad \\vdots \\\\\n&\\quad \\;\\times\\frac{p(\\boldsymbol\\phi_k \\,|\\, \\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^{k-1},\\{\\boldsymbol\\phi_{i}\\}_{i={k+1}}^n)}{p(\\boldsymbol\\phi_{k,0} \\,|\\, \\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^{k-1},\\{\\boldsymbol\\phi_{i}\\}_{i={k+1}}^n)} \\times\\\\\n&\\quad \\quad \\vdots \\\\\n&\\quad \\;\\times\\frac{p(\\boldsymbol\\phi_n \\,| \\,\n\\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^{n-1})}{p(\\boldsymbol\\phi_{n,0} \\,| \\,\n\\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^{n-1})} p(\\boldsymbol\\phi' = \\boldsymbol\\phi'_0 \\,|\\, \\boldsymbol\\Lambda,\\rho)p(\\boldsymbol\\Lambda)\n\\end{align*}\n\\] where the last line follows by application of Brook’s Lemma to \\(p(\\boldsymbol\\phi'|\\boldsymbol\\Lambda,\\rho)\\). We’ve dropped \\(\\boldsymbol\\Lambda\\) and \\(\\rho\\) from conditioning notation in the resulting product of ratios, but the conditioning is very much still present. We choose \\(\\boldsymbol\\phi'_0 = \\boldsymbol{0}\\) for the fixed point in the support of \\(\\boldsymbol\\phi'\\). This will simplify things later and allow us to avoid the situation we ran into ealier.\n\n\n\n\n\n\nNote\n\n\n\nCould I have derived the full conditionals incorrectly for the univariate case? I think I just used the product of the conditionals there and not Brook’s Lemma. . . this might explain why my MCMC had issues with convergence. I should go back and check.\n\n\nWe have a particularly tedious product of ratios. Let’s focus on the \\(k\\)th term first, then consider the entire product to understand the form of the full conditional for \\(\\boldsymbol\\Lambda\\). Let \\[\n\\mathbf{m}_k = \\rho\\sum_{\\substack{j \\in \\mathcal{N}_k \\\\ j \\, > \\, k}}\\mathbf{B}_{kj}\\boldsymbol\\phi_j\n\\] Note that at the \\(k\\)th term we are considering the density as a function of some non-zero conditioned variates. The \\(\\boldsymbol\\phi_j\\) for \\(j < k\\) have been set to \\(\\mathbf{0}\\). This may affect the sum in the conditional mean, so we restrict the sum to terms that are non-zero, that is \\(\\boldsymbol\\phi_j\\) where \\(j \\in \\mathcal{N}_k\\) and \\(j > k\\).\nThen the \\(k\\)th term, \\[\n\\begin{align*}\n\\frac{p(\\boldsymbol\\phi_k \\,|\\, \\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^{k-1},\\{\\boldsymbol\\phi_{i}\\}_{i={k+1}}^n)}{p(\\boldsymbol\\phi_{k,0} \\,|\\, \\{\\boldsymbol\\phi_{j,0}\\}_{j=1}^{k-1},\\{\\boldsymbol\\phi_{i}\\}_{i={k+1}}^n)}\n&=\n\\frac{\\exp\\left\\{-\\frac{w_{k+}}{2}\\left(\\boldsymbol\\phi_k-\\mathbf{m}_k\\right)^T\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol\\phi_k-\\mathbf{m}_k\\right)\\right\\}}{\\exp\\left\\{-\\frac{w_{k+}}{2}\\left(\\boldsymbol{0}-\\mathbf{m}_k\\right)^T\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol{0}-\\mathbf{m}_k\\right)\\right\\}}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\left(\\boldsymbol\\phi_k-\\mathbf{m}_k\\right)^T\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol\\phi_k-\\mathbf{m}_k\\right) + \\frac{w_{k+}}{2}\\mathbf{m}_k^T\\boldsymbol\\Lambda^{-1}\\mathbf{m}_k\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\left[\\left(\\boldsymbol\\phi_k-\\mathbf{m}_k\\right)^T\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol\\phi_k-\\mathbf{m}_k\\right) -\\mathbf{m}_k^T\\boldsymbol\\Lambda^{-1}\\mathbf{m}_k\\right]\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\left[\\boldsymbol\\phi^T_k\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k - 2\\mathbf{m}_k^T\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi^T_k\\right]\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\text{Tr}\\left[\\boldsymbol\\phi^T_k\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k - 2\\mathbf{m}_k^T\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k\\right]\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\left(\\text{Tr}\\left[\\boldsymbol\\phi^T_1\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k\\right] - \\text{Tr}\\left[ 2\\mathbf{m}_k^T\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k\\right]\\right)\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\left(\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k\\right] - \\text{Tr}\\left[ 2\\boldsymbol\\Lambda^{-1}\\boldsymbol\\phi_k\\mathbf{m}_k^T\\right]\\right)\\right\\}\\\\\n&=\n\\exp\\left\\{-\\frac{w_{k+}}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k -  2\\boldsymbol\\phi_k\\mathbf{m}_k^T\\right)\\right]\\right\\}\\\\\n\\end{align*}\n\\]\nThen the entire product will be \\[\n\\begin{align*}\np(\\boldsymbol\\phi'|\\boldsymbol\\Lambda,\\rho)\n&=\n\\left[\\prod_{k=1}^n \\frac{p(\\boldsymbol\\phi_k \\,|\\, \\{\\boldsymbol\\phi_{j,0} = \\mathbf{0}\\}_{j=1}^{k-1},\\{\\boldsymbol\\phi_{i}\\}_{i={k+1}}^n, \\boldsymbol\\Lambda, \\rho)}{p(\\boldsymbol\\phi_{k,0} = \\mathbf{0} \\,|\\, \\{\\boldsymbol\\phi_{j,0} = \\mathbf{0}\\}_{j=1}^{k-1},\\{\\boldsymbol\\phi_{i}\\}_{i={k+1}}^n, \\boldsymbol\\Lambda, \\rho)}\\right]p(\\boldsymbol\\phi' = \\mathbf{0} \\,|\\, \\boldsymbol\\Lambda,\\rho)\\\\\n&=\n\\left[\\prod_{k=1}^n \\exp\\left\\{-\\frac{w_{k+}}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1}\\left(\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k -  2\\boldsymbol\\phi_k\\mathbf{m}_k^T\\right)\\right]\\right\\}\\right]p(\\boldsymbol\\phi' = \\mathbf{0} \\,|\\, \\boldsymbol\\Lambda,\\rho)\\\\\n&=\n\\exp\\left\\{-\\frac{1}{2}\\sum_{k=1}^n\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1}w_{k+}\\left(\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k -  2\\boldsymbol\\phi_k\\mathbf{m}_k^T\\right)\\right]\\right\\}p(\\boldsymbol\\phi' = \\mathbf{0} \\,|\\, \\boldsymbol\\Lambda,\\rho)\\\\\n&=\n\\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1} \\sum_{k=1}^n \\mathbf{S}_k\\right]\\right\\} \\times \\big|(\\mathbf{D} - \\rho\\mathbf{W})^{-1} \\otimes \\boldsymbol\\Lambda\\big|^{-1/2} \\exp\\left\\{-\\frac{1}{2}\\left(\\mathbf{0}\\right)^T\\left((\\mathbf{D} - \\rho\\mathbf{W})^{-1}\\otimes\\boldsymbol\\Lambda\\right)^{-1}\\mathbf{0}\\right\\}\\\\\n&= \\big|(\\mathbf{D} - \\rho\\mathbf{W})^{-1} \\otimes \\boldsymbol\\Lambda\\big|^{-1/2}\n\\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1} \\sum_{k=1}^n \\mathbf{S}_k\\right]\\right\\}\n\\end{align*}\n\\]\nwhere \\(\\mathbf{S}_k = w_{k+}\\left(\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k - 2\\boldsymbol\\phi_k\\mathbf{m}_k^T\\right)\\).\nFinally, the full conditional for \\(\\boldsymbol\\Lambda\\) is \\[\n\\begin{align*}\np(\\boldsymbol\\Lambda \\,|\\, \\text{rest}) &\\propto p(\\{\\boldsymbol\\delta_i\\}, \\{\\boldsymbol\\phi_i\\}, \\boldsymbol\\beta, \\boldsymbol\\Omega, \\boldsymbol\\Lambda, \\rho)\\\\\n&\\propto p(\\boldsymbol\\phi'|\\boldsymbol\\Lambda,\\rho)p(\\boldsymbol\\Lambda)\\\\\n&=\n\\big|(\\mathbf{D} - \\rho\\mathbf{W})^{-1} \\otimes \\boldsymbol\\Lambda\\big|^{-1/2}\n\\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1} \\sum_{k=1}^n \\mathbf{S}_k\\right]\\right\\}\\\\\n&\\quad \\;\\times\n\\big|\\boldsymbol\\Lambda\\big|^{-(\\nu + p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}(\\boldsymbol\\Lambda^{-1}\\mathbf{R})\\right\\}\\\\\n&\\propto\n\\big|\\boldsymbol\\Lambda\\big|^{-p/2}\n\\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1} \\sum_{k=1}^n \\mathbf{S}_k\\right]\\right\\}\\\\\n&\\quad \\;\\times\n\\big|\\boldsymbol\\Lambda\\big|^{-(\\nu + p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1}\\mathbf{R}\\right]\\right\\}\\\\\n&=\n\\big|\\boldsymbol\\Lambda\\big|^{-(\\nu + 2p + 1)/2} \\exp\\left\\{-\\frac{1}{2}\\text{Tr}\\left[\\boldsymbol\\Lambda^{-1}\\left(\\mathbf{R} + \\sum_{k=1}^n \\mathbf{S}_k\\right)\\right]\\right\\}\\\\\n\\end{align*}\n\\] and we conclude \\(\\boldsymbol\\Lambda \\,|\\, \\text{rest} \\sim \\text{InvWish}\\left(p + \\nu,\\, \\mathbf{R} + \\sum_{k=1}^n \\mathbf{S}_k\\right)\\)\nThis full conditional seems reasonable, but computing \\(\\sum_{k=1}^n \\mathbf{S}_k\\) may prove a bit tricky. Perhaps there are some simplifications to be made for this term in particular.\n\\[\n\\begin{align*}\n\\sum_{k=1}^n \\mathbf{S}_k &= \\sum_{k=1}^nw_{k+}\\left(\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k -  2\\boldsymbol\\phi_k\\mathbf{m}_k^T\\right)\\\\\n&= \\sum_{k=1}^nw_{k+}\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k - \\sum_{k=1}^nw_{k+}2\\boldsymbol\\phi_k\\mathbf{m}_k^T\\\\\n&= \\sum_{k=1}^nw_{k+}\\boldsymbol\\phi_k\\boldsymbol\\phi^T_k - \\sum_{k=1}^nw_{k+}2\\boldsymbol\\phi_k\\rho\\sum_{\\substack{j \\in \\mathcal{N}_k \\\\ j \\, > \\, k}}\\boldsymbol\\phi_j^T\\mathbf{B}^T_{kj}\\\\\n&= \\mathbf{D}\\boldsymbol\\Phi^T\\boldsymbol\\Phi - 2\\rho\\sum_{k=1}^nw_{k+}\\boldsymbol\\phi_k\\sum_{\\substack{j \\in \\mathcal{N}_k \\\\ j \\, > \\, k}}\\boldsymbol\\phi_j^T\\mathbf{B}^T_{kj}\\\\\n&= \\mathbf{D}\\boldsymbol\\Phi^T\\boldsymbol\\Phi - 2\\rho\\sum_{k=1}^nw_{k+}\\boldsymbol\\phi_k\\sum_{\\substack{j \\in \\mathcal{N}_k \\\\ j \\, > \\, k}}\\frac{w_{kj}}{w_{k+}}\\boldsymbol\\phi_j^T\\mathbf{I}_p \\quad \\quad \\text{set } \\mathbf{B}_{kj} = \\frac{w_{kj}}{w_{k+}}\\mathbf{I}_p\\\\\n&= \\mathbf{D}\\boldsymbol\\Phi^T\\boldsymbol\\Phi - 2\\rho\\sum_{k=1}^n\\boldsymbol\\phi_k\\sum_{\\substack{j \\in \\mathcal{N}_k \\\\ j \\, > \\, k}}w_{kj}\\boldsymbol\\phi_j^T\\\\\n&= \\mathbf{D}\\boldsymbol\\Phi^T\\boldsymbol\\Phi - 2\\rho\\boldsymbol\\Phi^T\\mathbf{W}_{U}\\boldsymbol\\Phi\\\\\n&=\n\\end{align*}\n\\] Where \\(\\mathbf{W}_U\\) is ab \\(n \\times n\\) upper triangular matrix constructed by zeroing out the lower triangle of \\(\\mathbf{W}\\). This has the affect of setting \\(\\boldsymbol\\phi_j = \\boldsymbol 0\\) for \\(j \\le k\\) which defines the sum \\(\\sum_{\\substack{j \\in \\mathcal{N}_k \\\\ j \\, > \\, k}}w_{kj}\\boldsymbol\\phi_j^T\\).\nIt is important to note that \\(\\boldsymbol\\Phi\\) is constructed with rows equal to \\(\\boldsymbol\\phi_k\\) which results in flipping \\(\\boldsymbol\\Phi\\boldsymbol\\Phi^T\\), what we would expect, to instead be \\(\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\).\n\n\n\n\n\n\nNote\n\n\n\nThis is suspiciously close to \\((\\mathbf{D}-\\rho\\mathbf{W})\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\). Perhaps \\(2\\boldsymbol\\Phi^T\\mathbf{W}_U\\boldsymbol\\Phi = \\boldsymbol\\Phi^T\\mathbf{W}\\boldsymbol\\Phi\\)? It would be nice, but actually I don’t think this is true."
  },
  {
    "objectID": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#metropolis-hastings-step-for-spatial-stength-parameter---rho",
    "href": "research/2023-03-07-Step-11-MCMC-for-MCAR-basic-example/index.html#metropolis-hastings-step-for-spatial-stength-parameter---rho",
    "title": "Step 11 - A Bayesian HM for multivariate Guassian data and MCAR prior - derivation of full conditionals and comments on computing",
    "section": "Metropolis-Hastings Step for Spatial Stength parameter - \\(\\rho\\)",
    "text": "Metropolis-Hastings Step for Spatial Stength parameter - \\(\\rho\\)\nTODO"
  }
]