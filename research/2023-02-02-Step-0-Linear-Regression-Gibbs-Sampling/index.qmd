---
title: "Step 0 - Bayesian Linear Regression with Gibbs Sampling"
description: "Starting from the basics. . ."
author:
  - name: Matthew Shisler
    affiliation: North Carloina State University - Department of Statistics
    affiliation-url: https://statistics.sciences.ncsu.edu/ 
categories: [Research, Bayesian, MCMC] # self-defined categories
draft: false 
format:
  html: 
    code-fold: true
---


```{r}
#| label: load-packages
#| output: false
#| code-summary: "Code: Load the packages"

library(tictoc)
```

We will start by verifying that our Gibbs sampler is actually working for a very simple case. First, we will simulate data from the model 

\begin{align*}
\mathbf{Y} \sim \text{Normal}_n(\mathbf{X}\mathbf{\boldsymbol\delta}, \; \sigma^2 \mathbf{I}_n)
\end{align*}

where $\mathbf{Y}$ is an $n \times 1$ response vector, $\mathbf{X}$ is an $n \times p$ design matrix, and $\boldsymbol\delta$ is a $p \times 1$ vector of parameters. The observations are mutually independent with constant variance, $\text{Cov}(\mathbf{Y}) = \sigma^2 \mathbf{I}_n$. 

Next specify priors for $\boldsymbol\delta$ and $\sigma^2$, chosen for conjugacy,
\begin{align*}
\boldsymbol\delta &\sim \text{Normal}_p(\boldsymbol\beta, \; \mathbf{\Omega})\\
\sigma^2 &\sim \text{InvGamma}(a, \; b)
\end{align*}

Where $\boldsymbol\beta = \boldsymbol0$, $\mathbf{\Omega} = \text{diag}((1000^2, 1000^2))$, and $a = b = 0.1$. The code
alternates between sampling from the full conditionals, $p(\boldsymbol\delta|\mathbf{Y},\sigma^2)$ and $p(\sigma^2|\mathbf{Y},\boldsymbol\delta)$. In this case 
\begin{align*}
  \boldsymbol\delta|\mathbf{Y},\sigma^2 &\sim \text{Normal}\left(V^{-1}M, V^{-1}\right)\\
  V &= \frac{1}{\sigma^{2}}\mathbf{X}^T\mathbf{X} + \mathbf{\Omega}^{-1}\\
  M &= \frac{1}{\sigma^{2}}\mathbf{X}^T\mathbf{Y} + \mathbf{\Omega}^{-1}\boldsymbol\beta\\
  \sigma^2|\mathbf{Y},\boldsymbol\delta &\sim \text{InvGamma}\left(\frac{n}{2} + a, \frac{1}{2}(\mathbf{Y}-\mathbf{X}\boldsymbol\delta)^T(\mathbf{Y}-\mathbf{X}\boldsymbol\delta) + b \right)
\end{align*}

Let's do a simple example with $n=100$ and $p = 2$. I will start the parameter index at $1$, i.e. $\delta_1$ is the intercept.

```{r}
#| code-summary: "Simulate some data"

n  <- 100
p  <- 2    #including intercept

# Generate some fake data (with intercept)
X      <- matrix(c(rep(1,n), rnorm(n*(p-1))), nrow = n, ncol = p)
delta0 <- rnorm(p, mean = 0, sd = 3)
Y      <- matrix(rnorm(n, X%*%delta0, sd = 1))

```

```{r}
#| code-summary: "Run the Gibbs sampler"

# set-up
niter <- 5000
keep_delta  <- matrix(NA, nrow = niter, ncol = p)
keep_sigma2 <- rep(NA,niter)

# initial values (chosen to be intentionally poor)
delta  <- c(-10,10)
sigma2 <- 10
keep_delta[1,] <- delta
keep_sigma2[1] <- sigma2

# prior parameters
a  <- 0.1
b  <- 0.1
beta <- rep(0,p)
Omega_inv <- diag(rep(1e-06,p))

# pre-computes
XtY <- t(X)%*%Y
XtX <- t(X)%*%X
Obeta <- Omega_inv%*%beta # not really necessary since beta = 0

# Gibbs Loop
tic()
for (i in 2:niter){
  
  # Sample from delta full conditional
  M     <- (1/sigma2)*XtY + Obeta
  V_inv <- solve((1/sigma2)*XtX + Omega_inv)
  delta <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)
    
  # Sample from sigma2 full conditional
  A      <- n/2 + a
  B      <- (1/2)*sum((Y-X%*%delta)^2) + b
  sigma2 <- 1/rgamma(1, A, B)
  
  # store the results
  keep_delta[i,]  <- delta
  keep_sigma2[i] <- sigma2
  
}
toc()
```

Let's inspect the resulting trace plots as a quick visual check. The true value of the respective parameter is represented by a red horizontal line. To make the convergence more obvious, I've included the poor initial guess.
```{r}
#| code-summary: "Code: Generate trace plots"

win <- 1:niter

par(mfrow = c(2,2))

plot(win, keep_delta[win,1], type = "l",
     ylab = expression(delta[1]),
     xlab = "iter")
abline(h = delta0[1], col = "red")

plot(win, keep_delta[win,2], type = "l",
     ylab = expression(delta[2]),
     xlab = "iter")
abline(h = delta0[2], col = "red")

plot(win, keep_sigma2[win],   type = "l",
     ylab = expression(sigma^2),
     xlab = "iter")
abline(h = 1, col = "red")
```

We can "burn" the first 100 iterations to see the behavior after convergence. Looks good to me!
```{r}
#| code-summary: "Code: Generate trace plots"

win <- 100:niter

par(mfrow = c(2,2))

plot(win, keep_delta[win,1], type = "l",
     ylab = expression(delta[1]),
     xlab = "iter")
abline(h = delta0[1], col = "red")

plot(win, keep_delta[win,2], type = "l",
     ylab = expression(delta[2]),
     xlab = "iter")
abline(h = delta0[2], col = "red")

plot(win, keep_sigma2[win],   type = "l",
     ylab = expression(sigma^2),
     xlab = "iter")
abline(h = 1, col = "red")
```