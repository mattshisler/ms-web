---
title: "Step 7 - Areal data and the spatial CAR model - Basic Example"
description: "A basic description and example of the spatial conditionally autoregressive model for areal data."
author:
  - name: Matthew Shisler
    affiliation: North Carloina State University - Department of Statistics
    affiliation-url: https://statistics.sciences.ncsu.edu/ 
categories: [Bayesian, MCMC, Spatial, CAR] # self-defined categories
draft: false 
format:
  html: 
    code-fold: true
execute: 
  cache: false
---


```{r}
#| label: load-packages
#| output: false
#| code-summary: "Code: Load the packages"

library(tidyverse)
library(igraph)
library(viridis)
library(MASS)
library(Matrix)
library(tictoc)
library(extraDistr)
library(CARBayes)
```

## A Brief Introduction

Here we're going to examine the spatial CAR model. CAR stands for Conditionally Autoregressive. The *spatial* CAR model is in a way an extension of autoregressive models for time series data. Time series data is typically 1-dimensional (in time) and the observations have a natural ordering in the sense that observations can be ordered by the time they were observed. Spatial data can be any dimension and do not necessarily have a natural ordering.

## What is areal data?

A CAR model is commonly applied to areal data. That is data where the spatial domain $D$ is partitioned into a finite number of blocks or *areas*. A common example is the partition of the United States of America into states, census tracts, or counties. A measurement is then collected for each areal unit.

The spatial domain is $D$.

The areal units are $B_i$ for $i = 1,\dots n$.

The measurements are $Z_i \equiv Z(B_i)$ for $i = 1, \dots, n$.

Before we dive into the distributional assumptions related to the CAR model, something must be said about the structure of the blocks. Namely we must define some notion of *proximity* from one block to the next. It's difficult in general to do this, particularly for an irregular partition of the spatial domain. The easiest approach is to define and *adjacency* matrix which captures which blocks are bordering other blocks. If there are $n$ blocks, then this adjacency matrix $\mathbf{W}$ is $n \times n$ and

$$
w_{ij} = 
\begin{cases}
1 \quad \text{if } B_i \text{ shares a border with } B_j,\\
0 \quad \text{otherwise}.
\end{cases}
$$
By convention we say that an areal unit does not share a border with itself hence $w_{ii} = 0$ for all $i = 1, \dots, n$.

Let's define our own spatial domain and partition it into some very basic units.

```{r}
n <- 4
spat_domain <- expand.grid(x = 1:n, y = 1:n)
spat_domain$label <- 1:(n*n)
```

Here is the spatial domain. It is a regular lattice with `r n^2` areal units.

```{r}
#| label: fig-lattice-example
#| fig-align: "center"
#| fig-cap: "A spatial domain partitioned into a regular lattice with areal units labeled $1,...,n$."
ggplot(spat_domain) +
  geom_tile(aes(x, y), linewidth = 2, color = "grey50", fill="white") +
  geom_text(aes(x, y, label=label), size = 15) +
  coord_fixed() + 
  theme_void()
```

Next we want to define a neighborhood matrix for this regular lattice. A small digression, the convention used to label the areal units will impact the structure of this matrix. Is there a "best" structure? That remains to be seen. For now, let's stick with the adjacency matrix that arises from the ordering in figure above. Adjacency matrices are abundant in graph theory. We'll use the package `igraph` to construct the adjacency matrix for the areal units above. This is accomplished by first using `igraph` to create a `r n` $\times$ `r n` lattice graph, then using the `as_adjacency_matrix` function to convert the graph object to a sparse matrix.

```{r}
#| label: fig-adjacent_mat
#| fig-cap: "does this work"
spat_domain_g <- make_lattice(c(n,n), mutual = TRUE)
W <- as_adjacency_matrix(spat_domain_g, sparse=1)
W
```


With the spatial domain defined and partitioned, we can continue by simulating spatial data. The most basic case assumes spatial independence.Let's also make it a bit more interesting by bumping up the number of areal units. Now it will probably be a lot easier to spot spatial dependence by a plot of the data alone.

```{r}
n <- 70
spat_domain <- expand.grid(x = 1:n, y = 1:n)
spat_domain$z <- rnorm(n^2, mean = 0, sd = 1)
ggplot(spat_domain) +
  geom_tile(aes(x, y, fill=z)) +
  scale_y_reverse() +
  scale_fill_gradientn(colors = viridis(10)) +
  coord_fixed() + 
  theme_void()
```

We want to simulate data *with* spatial dependence. We can do this from the CAR model.

Let's turn to working with the CAR model through a simple example. Let 

$$
Z_i =  \mathbf{x}^T_i\beta + \phi_i + \varepsilon_i
$$
Here we have a covariate vector $\mathbf{x}_i$ indexed by spatial location $i$, $\phi_i$ is a spatial random effect and $\varepsilon_i$ is a random error associated with the measurement at location $i$ (later we assume to be normal with zero mean and constant variance). The defining characteristic of the CAR model is to specify a spatial structure through the conditional distributions of $\phi_i$ accordingly

$$
\phi_i|\phi_{j, \; j \ne i} \sim \text{N}\left(\textstyle{\frac{1}{n-1}\sum}_{j \ne i} \phi_j, \tau^2_i\right)
$$
That is the conditional mean of $\phi_i$ is just the average of the spatial random effects across all other locations. That being said, it's probably not reasonable to condition on ALL other locations. Paraphrasing Tobler's First Law of Geography, "everything is related to everything else, but near things are more related than distant things."

Perhaps we don't need to condition on "distant things." Instead we'll condition on only the locations we've defined as the neighbors of location $i$. Let $\mathcal{N}_i$ be the set of locations that are considered neighbors with location $i$. Then we specify the conditional distribution as

$$
\phi_i|\phi_{j, j \in \mathcal{N}_i} \sim \text{N}\left(\textstyle{\frac{1}{|\mathcal{N}_i|}\sum}_{j \in \mathcal{N}_i} \phi_j, \tau^2_i\right)
$$
Practically, there are too many parameters in this model. Namely we have specified a location-specific variance, $\tau^2_i$, in each conditional distribution. We can simplify the model by specifying the conditional variance as a function of a parameter shared across locations and the number of neighbors of a given location. This structure is intuitive because we would expect the conditional variance to decrease as the number of neighbors increases. The new conditional distributions are specified as

$$
\phi_i|\phi_{j, j \in \mathcal{N}_i} \sim \text{N}\left(\textstyle{\frac{1}{|\mathcal{N}_i|}\sum}_{j \in \mathcal{N}_i} \phi_j, \frac{\tau^2}{|\mathcal{N}_i|}\right)
$$
At the end of the day it is possible to write the joint distribution of the spatial random effects from the conditional distributions. This is called *compatibility* and note that it is not guaranteed!  Let $\boldsymbol\phi = (\phi_i,\dots,\phi_n)$, $\mathbf{M}$ be an $n \times n$ diagonal matrix containing the number of neighbors for each spatial location on its diagonal, and again $\mathbf{W}$ is our proximity matrix we defined earlier. Finally, we need to introduce another parameter $\rho$ to ensure that the distribution is proper (details in the BCG 2003).

$$
\boldsymbol\phi \sim \text{N}\left(\boldsymbol{0}, \tau^2(\mathbf{M} - \rho \mathbf{W})^{-1}\right)
$$
The matrix $\mathbf{M}$ is fairly easy to obtain. Its diagonal is just the row sums from $\mathbf{W}$ and all other terms set to 0.

Let's try to simulate some data from this model. First we'll define the spatial domain.

```{r}
n      <- 70
nsites <- n^2
spat_domain <- expand.grid(x = 1:n, y = 1:n)
spat_domain$label <- 1:nsites
spat_domain_g <- make_lattice(c(n,n), mutual = TRUE)
W <- as_adjacency_matrix(spat_domain_g, sparse=1)
```

Next define some parameters and draw from the spatial random effects distribution.
```{r}
tau2 <- 5
rho <- 0.99
M   <- diag(rowSums(W))
spat_prec <- (1/tau2)*(M-rho*W) # swap this with something else. . .
spat_domain$phi <- backsolve(chol(spat_prec), matrix(rnorm(nsites), nrow = nsites))
# spat_cov <- tau^{-2}*solve(M-rho*W)
# 
# phi <- MASS::mvrnorm(1, mu = rep(0,n^2), Sigma = spat_cov)
```

Next sample the observations from the data distribution.
```{r}
X    <- rep(1, nsites)
beta <- matrix(c(2), nrow = 1)
sigma2 <- 5
spat_domain$z  <- rnorm(nsites, mean = X%*%beta + spat_domain$phi, sd = sqrt(sigma2))
```

Let's generate some plots. First, the spatial random effects, then the observations.


```{r}
#| fig-align: center
#| 
ggplot(spat_domain) +
  geom_tile(aes(x, y, fill=phi)) +
  scale_y_reverse() +
  scale_fill_gradientn(colors = viridis(10)) +
  coord_fixed() + 
  theme_void()
```

```{r}
#| fig-align: center
ggplot(spat_domain) +
  geom_tile(aes(x, y, fill=z)) +
  scale_y_reverse() +
  scale_fill_gradientn(colors = viridis(10)) +
  coord_fixed() + 
  theme_void()
```


So we've simulated some data from the CAR model and it seems fairly clear that the measurements are spatially correlated. Now we want to fit a CAR model to this data and estimate the parameters, $\boldsymbol\theta = (\beta, \sigma^2, \tau^2, \rho)$.

We can do this using MCMC. First, let's summarize the hierarchical model.


\begin{align*}
Z_i &\sim \text{N}\left(x^T_i\boldsymbol\beta + \phi_i, \; \sigma^2\right)\\
\mu &\sim \text{N}\left(0, \lambda^2\right)\\
\phi_i|\phi_{j \in \mathcal{N}_i} &\sim \text{N}\left(\frac{\rho}{m_i}\sum_{j\in \mathcal{N}_i} \phi_j, \frac{\tau^2}{m_i} \right)\\
\sigma^2 &\sim \text{IG}\left(a, b\right)\\
\tau^2 &\sim \text{IG}\left(a,b\right)\\
\rho &\sim \text{Unif}\left(0,1\right)
\end{align*}


We've stated the model using conditional distributions of $\phi_i$, though we learned earlier that it is possible to write the joint distribution of $\boldsymbol\phi$. If we do this, we will at some point to need invert a very large matrix in order to sample from the full conditional for $\boldsymbol\phi$. Instead it might be faster to cycle through the full conditionals of $\phi_i$ for each $i$.

Most of this model can be implemented using Gibbs sampling, except when sampling the $\rho$ parameter. We'll need to use a Metropolis-Hastings step for that. The full conditionals are as follows,


\begin{align*}
\boldsymbol\beta|\text{rest} &\sim \text{N}\left(\mathbf{B}^{-1}\mathbf{A}, \mathbf{B}^{-1}\right)\\
\mathbf{A} &= \sigma^{-2} \mathbf{X}^T(Z - \boldsymbol\phi)\\
\mathbf{B} &= \sigma^{-2}\mathbf{X}^T\mathbf{X} + \lambda^{-2}\mathbf{I}\\
\\\\
\phi_i|\text{rest} &\sim \text{N}\left(\frac{A}{B}, \frac{1}{B}\right)\\
A &= \frac{\rho}{\tau^2}\sum_{j \in \mathcal{N}_i}\phi_j + \frac{1}{\sigma^2}(Z_i - x_i^T\boldsymbol\beta)\\
B &= \frac{m_i}{\tau^2} + \frac{1}{\sigma^2}\\
\\\\
\sigma^2|\text{rest} &\sim \text{IG}\left(A, B \right)\\
A &= a + \frac{n}{2}\\
B &= b + \frac{1}{2}(Z-\mathbf{X}\boldsymbol\beta-\phi)^T(Z-\mathbf{X}\boldsymbol\beta-\boldsymbol\phi)\\
\\\\
\tau^2|\text{rest} &\sim \text{IG} \left(A, B\right)\\
A &= a + \frac{n}{2}\\
B &= b + \frac{1}{2}\sum_{i=1}^n m_i\left(\phi_i - \frac{\rho}{m_i}\sum_{j \in \mathcal{N}_i}\phi_j\right)^2\\
\\\\
p(\rho|\text{rest}) &\propto \left[\prod_{i=1}^n p(\phi_i|\phi_{j, j\in\mathcal{N}_i}, \rho)\right]p(\rho)\\
&\propto \exp\left\{-\frac{1}{2\tau^2} \sum_{i=1}^n m_i \left(\phi_i - \frac{\rho}{m_i}\sum_{j \in \mathcal{N}_i} \phi_j \right)^2\right\} \mathbf{1}\left\{\rho \in [0,1]\right\}
\end{align*} 


Clearly we are unable to sample directly from the full conditional for $\rho$. Instead we will need to implement a Metropolis-Hasting step. We will use a truncated normal proposal distribution from the pacakge `extraDistr` to match the support of $\rho$. This also let's us "ignore" the indicator function in the uniform prior because we will never propose a candidate value outside of $[0,1]$.

As an aside, we will need to routinely compute averages of neighboring spatial random effects at each location. Rather than extract neighbor information from a large neighbor matrix, we define `ineighbors` as a list of vectors containing the neighbor indices for each location. Additionally we create `nneighbors` as a vector containing the number of neighbors for each location. These two objects together should give us all we need to efficiently sample full conditionals.
```{r}
n <- 10
nsites <- n^2
spat_domain <- expand.grid(x = 1:n, y = 1:n)
spat_domain$label <- 1:(n*n)
spat_domain_g <- make_lattice(c(n,n), mutual = TRUE)
W <- as_adjacency_matrix(spat_domain_g, sparse=1)
ineighbors <- apply(W, MARGIN = 1, FUN = function(x) which(x==1))
nneighbors <- rowSums(W)
```

First, simulate a small data set.

Next sample the spatially independent and spatially dependent covariates.
```{r}
# spatially independent
# x1 <- rnorm(nsites, mean = 0, sd = 1)
X <- matrix(1, nrow = nsites)
# spatial random effect
tau20 <- 4
rho0  <- 0.99
M     <- diag(rowSums(W))
spat_prec <- (1/tau20)*(M-rho0*W) # swap this with something else. . .
spat_domain$phi <- backsolve(chol(spat_prec), matrix(rnorm(nsites), nrow = nsites))
phi0 <- spat_domain$phi
```

Next sample the observations from the data distribution.
```{r}
beta0   <- 2
sigma20 <- 0.25
spat_domain$z  <- rnorm(nsites, mean = X%*%beta0 + spat_domain$phi, sd = sqrt(sigma20))
z <- matrix(spat_domain$z, nrow = nsites)
```


```{r}

npars  <- length(beta)
nsites <- n^2
niters <- 5000
burn   <- 1000
keep_beta   <- matrix(NA, nrow = niters, ncol = npars)
keep_phi    <- matrix(NA, nrow = niters, ncol = nsites)
keep_sigma2 <- rep(NA, niters)
keep_tau2   <- rep(NA, niters)
keep_rho    <- rep(NA, niters)

# initial values
beta   <- keep_beta[1,]  <- 2 #beta0
phi    <- keep_phi[1,]   <- rep(10, nsites)
sigma2 <- keep_sigma2[1] <- 2   #sigma20
tau2   <- keep_tau2[1]   <- 5  # tau20
rho    <- keep_rho[1]    <- rho0

# prior parameters
a <- 0.1
b <- 0.1
lambda2 <- 10000

# Metropolis-Hastings
att <- 0
acc <- 0
MH  <- 0.1

# pre-computes
XtX <- t(X)%*%X


## TODO:
##    - review rho_loglike funtion.
# rho_loglike <- function(rho, phi, ineighbors, nneighbors){
# 
#   sneighbors <- sapply(ineighbors, FUN = function(x) sum(phi[x]))
#   aneighbors <- sneighbors/nneighbors
#   
#   t1 <- sum(nneighbors*phi^2)
#   t2 <- 2*rho*sum(phi*sneighbors)
#   t3 <- (rho^2)*sum(nneighbors*(sneighbors^2))
#   
#   return(t1 - t2 + t3)
# }


tic()
for(iter in 2:niters){
  
  # sample mu [Gibbs]
  A     <- (1/sigma2)*(t(X)%*%(z-phi))
  B_inv <- solve((1/sigma2)*XtX + (1/lambda2)*diag(npars))
  beta  <- B_inv%*%A+t(chol(B_inv))%*%rnorm(npars)
  # beta <- beta0
  
  # sample phi [Gibbs]
  for (site in 1:nsites){
    A         <- (rho/tau2)*sum(phi[ineighbors[[site]]]) + 
                 (1/sigma2)*(z[site] - X[site,]%*%beta)
    B_inv     <- 1/(nneighbors[site]/tau2 + 1/sigma2) 
    phi[site] <- rnorm(1, mean = B_inv*A, sd = sqrt(B_inv))
  }
  # phi <- phi0
  
  # sample sigma2 [Gibbs]
  A      <- a + (nsites/2)
  B      <- b + (1/2)*sum((z - X%*%beta - phi)^2)
  sigma2 <- 1/rgamma(1, A, B)
  # sigma2 <- sigma20
  
  # sample tau2 [Gibbs]
  aneighbors <- sapply(ineighbors, FUN = function(x) sum(phi[x]))/nneighbors
  A    <- a + (nsites/2)
  B    <- b + (1/2)*sum(nneighbors*(phi - rho*aneighbors)^2)
  tau2 <- 1/rgamma(1, A, B)
  # tau2 <- tau20
  
  ## TODO:
  ##    - review rho M-H step. Fix rho for now.
  ## sample rho [M-H]
  # att <- att + 1 
  # can <- rtnorm(1, rho, MH, a = 0, b = 1)
  # R   <- rho_loglike(can, phi, ineighbors, nneighbors) - # Likelihood
  #        rho_loglike(rho, phi, ineighbors, nneighbors) +
  #        dtnorm(rho, can , a = 0, b = 1, log = T) -      # M-H adjustment
  #        dtnorm(can, rho , a = 0, b = 1, log = T)
  # if(log(runif(1)) < R){
  #   acc <- acc + 1
  #   rho <- can
  # }
  # rho <- rho0
  # 
  # # tuning
  # if(iter < burn){
  #   if(att > 50){
  #     if(acc/att < 0.3){MH <- MH*0.8}
  #     if(acc/att > 0.6){MH <- MH*1.2}
  #     acc <- att <- 0
  #   }
  # }
  rho <- rho0
  
  # storage
  keep_beta[iter,]  <- beta
  keep_phi[iter,]   <- phi
  keep_sigma2[iter] <- sigma2
  keep_tau2[iter]   <- tau2
  keep_rho[iter]    <- rho
  
}
toc()
```

Now let's inspect some trace plots.
```{r}

win = 1:niters
plot(win, keep_beta[win,1], type = "l")
abline(h = beta0[1], col = "red")
plot(win, keep_sigma2[win], type = "l")
abline(h = sigma20, col = "red")
plot(win, keep_tau2[win], type = "l")
abline(h = tau20, col = "red")
plot(win, keep_rho[win], type = "l")
abline(h = rho0, col = "red")
```

Check trace plots for $\phi$. 10 random locations.
```{r}

sampled_sites <- sample(1:nsites, size = 10)
for (site in sampled_sites){
  plot(win, keep_phi[win, site] + keep_beta[win,1], type = "l")
  abline(h = phi0[site], col = "red")
}
```

There's clearly something wrong. I'll need to revisit this and verify the derivations and/or debug the code.

What if we compared to the CARBayes package? Note that CARBayes uses a modified version of the model proposed by Leroux (cite)

```{r}
# convert neighbord matrix from sparse dgCMatrix format.
W <- as.matrix(W)
# fit model from CARBayes.
cb.model <- S.CARleroux(z~1, family = "gaussian", 
                        W=W, burnin = 1000, n.sample = 5000, 
                        rho = rho0, verbose = FALSE)
cb.model
```

Let's look at the CARbayes samples...
```{r}
plot(cb.model$samples$beta)
plot(cb.model$samples$nu2)
plot(cb.model$samples$tau2)
# plot(cb.model$samples$phi)
```