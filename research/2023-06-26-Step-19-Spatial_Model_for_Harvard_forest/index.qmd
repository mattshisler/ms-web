---
title: "Step 19 - Applying the spatial model to a test region of Harvard Forest - w/ bug fixes"
description: "Fixed likelihood for MCAR prop parameter. Handle missing data."
author:
  - name: Matthew Shisler
    affiliation: North Carloina State University - Department of Statistics
    affiliation-url: https://statistics.sciences.ncsu.edu/ 
date: "7/5/2023"
categories: [Bayesian, MCMC, Spatial, MCAR] # self-defined categories
draft: false 
format:
  html: 
    code-fold: false
execute: 
  eval: true
  freeze: false
---


```{r}
#| label: load-packages
#| output: false
#| code-summary: "Code: Load the packages"
#| code-fold: true
#| message: false
#| warning: false

library(tidyverse)
library(igraph)
library(viridis)
library(matrixsampling)
library(MCMCpack)
library(gridExtra)
library(extraDistr)
library(tictoc)
library(minpack.lm)
library(terra)
library(ncdf4)
library(numDeriv)
library(MASS)
library(lubridate)
```

## Intro

We will attempt to fit the spatial model to a 50x50 pixel region of the Harvard Forest Region. Here are the steps:

```{r, layer-filt-id}
#| eval: true
#| code-fold: true
#| code-summary: Satellite snapshop filter

# from a previous exploratory analysis. Used to identify the layers to filter out from the Harvard Forest spatraster object.
layer_agg_hf <- read_csv("~/ms-web/research/data/harvard_forest/layer_agg_hf.csv", show_col_types = FALSE)

bad_layers_year <- c(1984, 1984,
                     1986, 
                     1988, 
                     1990, 
                     1992, 
                     1995, 1995,
                     1999, 1999,
                     2001,
                     2003,
                     2005, 2005,
                     2008,
                     2009, 2009,
                     2011, 2011,
                     2013,
                     2014,
                     2015, 2015,
                     2016,
                     2017, 2017,
                     2019, 2019
                     ) 
bad_layers_doy  <- c(171, 155,
                     167,
                     310,
                     315,
                     145,
                       9, 16,
                       4, 332,
                     345,
                     334,
                     300, 364,
                     316,
                      31, 183,
                      36,  84,
                     154,
                     341,
                      15, 151,
                     339,
                      21,  68,
                     314,  66)


bad_layer_year_days <- paste(bad_layers_year, bad_layers_doy, sep="-")

filter_ind <- which(paste(layer_agg_hf$year, layer_agg_hf$doy, sep="-") %in% bad_layer_year_days) 
NA_ind     <- which(is.na(layer_agg_hf$mean))
```


<!-- For now, let's do this for a smaller test region. Let's read-in the hf data, remove bad layers, crop it to a small region, aggregate into a mean value for each layer, fit the average model, linearize the double logistic function around the average model parameter estimates, compute the residuals of the average model, construct the basis function matrix from the gradient of the double logistic function evaluated at the avg model parameter estimates, compute estimates of the delta parameter vectors using the basis function matrix and the avg model residuals, compute sample covariance of the delta parameter vectors using the estimated delta parameter vectors, treat these results as inputs for the spatial Bayesian hierarchical model, estimate the covariate effects at each site. -->

```{r, read-and-crop-hf}
#| eval: true
#| code-fold: true
#| code-summary: Read and prep raster data

ReadNcTs <- function(ncfile) {
    r <- rast(ncfile)
    # Get time
    nc_in <- nc_open(ncfile)
    dates <- ncvar_get(nc_in, "time")
    nc_close(nc_in)
    # Assign time to the original image object
    if (nchar(dates[1]) == 8) {
        fm <- "%Y%m%d"
    } else if (nchar(dates[1] == 7)) {
        fm <- "%Y%j"
    }
    time(r) <- as.Date(as.character(dates), tryFormats = fm)

    # Reorder the layers by time
    r <- r[[order(time(r))]]

    # Get map projection
    tif <- rast(list.files(dirname(ncfile), ".tif$", full.names = TRUE)[1])
    crs(r) <- crs(tif)

    return(r)
}

# Complete Data
hf <- ReadNcTs("~/ms-web/research/data/harvard_forest/harvard_forest_evi2.nc")
hf


# filtered data - no NA layers or layers previously identified to be erroneous.
hf <- hf[[-c(filter_ind, NA_ind)]]
hf
# plot(hf[[100]])


# there are 12 extra pixels with NA values for everything. Let's crop those from the spatraster.
# hf <- crop(hf, ext(1923375, 1926405, 2412500, 2415165))
# hf
# plot(hf[[100]], range = c(0,1))

lpix <- 50
# we will crop again to create a small test region.
# hf_test <- crop(hf, ext(1926405 - lpix*30, 1926405, 2412500, 2412500 + lpix*30))
hf_test <- crop(hf, ext(1924500, 1924500+lpix*30, 2413000, 2413000 + lpix*30))
hf_test

plot(hf[[100]], range = c(0,1))
plot(hf_test[[100]], range = c(0,1))

```

```{r, compute-layer-summary-stats}
#| eval: true
#| code-fold: true
#| code-summary: Compute raster layer stats for region model

layer_agg <- global(hf_test, fun = c("mean", "notNA"), na.rm = T)

hf_name_date <- data.frame(name = names(hf_test), date = time(hf_test))
layer_agg$name <- row.names(layer_agg)

layer_agg <- layer_agg %>%
  inner_join(hf_name_date, by = "name") %>%
  mutate(doy = yday(date)) %>%
  mutate(year = year(date)) %>%
  mutate(perc_valid = notNA/ncell(hf_test))

```


```{r, plot-layer-summary-stats}
#| eval: true
#| code-fold: true
#| code-summary: Plots of raster summary data

region <- "Harvard Forest"


ggplot(data = layer_agg) + 
  geom_point(aes(x = date, y = mean, col = perc_valid)) +
    scale_color_gradientn(colors = viridis(10),
                             limits = c(0,1)) + 
  scale_y_continuous(limits = c(0,1)) +
  labs(title = region, x = "Date") +
  theme_bw()

snap_filt <- 0.00

ggplot(data = subset(layer_agg, perc_valid >= snap_filt)) + 
  geom_point(aes(x = doy, y = mean, color = perc_valid)) +
  scale_color_gradientn(colors = viridis(10),
                             limits = c(0,1)) + 
  scale_y_continuous(limits = c(0,1)) +
  labs(title = region, x = "Day of Year") +
  theme_bw()

ggplot(data = subset(layer_agg, perc_valid >= snap_filt)) + 
  geom_point(aes(x = doy, y = mean, color = year)) +
  scale_color_gradientn(colors = viridis(10)) + 
  scale_y_continuous(limits = c(0,1)) +
  labs(title = region, x = "Day of Year") +
  theme_bw()

ggplot(data = subset(layer_agg, perc_valid >= snap_filt)) + 
  geom_point(aes(x = doy, y = year, color = perc_valid)) +
  scale_color_gradientn(colors = viridis(10)) + 
  scale_y_reverse() +
  labs(title = region, x = "Day of Year") +
  theme_bw()
```

```{r, avg-model-fit}
#| eval: true
#| code-fold: true
#| code-summary: Fit region average model

# model_equ6_orig <- as.formula("y ~ theta1 + 
#                          theta2 * 
#                          ((1 / (1 + exp((theta3 - t) / theta4))) - 
#                          (1 / (1 + exp((theta5 - t) / theta6))))")


model_equ6_alt <- as.formula("y ~ 1/(1 + exp(-theta1)) + 
                              exp(theta2) * 
                              ((1 / (1 + exp((theta3 - t) / exp(theta4)))) - 
                              (1 / (1 + exp((theta5 - t) / exp(theta6)))))")


region_agg_fit <- function(doy, vi, 
                           model_form, 
                           init_val = c(-1.25, -0.5, 145, 2.5, 270, 2.5), 
                           lower_bound = c(-10, -5, 1, -5, 1, -5), 
                           upper_bound = c(10, 5, 370, 5, 370, 5)){
  
  avg_fit <- nlsLM(model_form,
             data = list(y = vi, 
                         t = doy),
             weights = rep(1, length(t)),
             start = list(theta1 = init_val[1],
                          theta2 = init_val[2],
                          theta3 = init_val[3],
                          theta4 = init_val[4],
                          theta5 = init_val[5],
                          theta6 = init_val[6]),
             lower = lower_bound[1:6],
             upper = upper_bound[1:6],
             control = list("maxiter" = 1000))
  
  return(avg_fit)
  
}

double_logis <- function(t, theta){
  # double logistic function.
  # theta1 is transformed using the logistic function.
  # theta2 is transformed using the 
  # This allows for all parameters to follow a gaussian distribution
  
  theta[1] <- plogis(theta[1])
  theta[2] <- exp(theta[2])
  theta[4] <- exp(theta[4])
  theta[6] <- exp(theta[6])
  
  n1 <- 1
  d1 <- 1 + exp((theta[3] - t)/theta[4])
    
  n2 <- 1
  d2 <- 1 + exp((theta[5] - t)/theta[6])
    
  out <- theta[1] + (theta[2])*(n1/d1 - n2/d2)
  
  return(out)
}

hf_avg_fit6 <- region_agg_fit(doy = layer_agg$doy, vi = layer_agg_hf$mean, model_form = model_equ6_alt)


ggplot() + 
  geom_point(data = subset(layer_agg, perc_valid >= 0), aes(x = doy, y = mean, color = year)) +
  geom_line(aes(x = 1:366, y = double_logis(1:366, coef(hf_avg_fit6))), color = "blue", linewidth = 1.5) +
  scale_color_gradientn(colors = viridis(10)) + 
  scale_y_continuous(limits = c(0,1)) +
  labs(title = "Harvard Forest") +
  theme_bw()

```

Compute the residuals that result from the average model.

```{r, avg-model-residuals}
#| code-fold: true
#| eval: true
#| code-summary: Compute residuals from average model.

# compute residuals
hf_test_residuals <- sapp(hf_test, function(x) x - double_logis(yday(time(x)), coef(hf_avg_fit6)))

```

Construct basis function matrix from the gradient of the double-logistic function.

```{r, construct-basis-matrix}
#| code-fold: true
#| eval: true
#| code-summary: Construct basis function matrix

# grad_basis_functions <- function(t, theta, dl_param = 6){
#   t <- t%%366
#   
#   e34 <- exp((theta[3] - t) / theta[4])
#   e56 <- exp((theta[5] - t) / theta[6])
#   
#   B1 <- 1
#   B2 <- ((1 / (1 + e34)) - (1 / (1 + e56)))
#   B3 <- -(theta[2]*e34)/(theta[4]*(1+e34)^2)
#   B4 <- (theta[2]*e34*(theta[3]-t))/((theta[4]*(1+e34))^2)
#   B5 <- (theta[2]*e56)/(theta[6]*(1+e56)^2)
#   B6 <- -(theta[2]*e56*(theta[5]-t))/((theta[6]*(1+e56))^2)
# 
#   B <- unname(cbind(B1, B2, B3, B4, B5, B6))
#   
#   return(B)
# }
# 
# 
# X <- grad_basis_functions(1:366, coef(hf_avg_fit6), dl_param = 6)
# 
# matplot(X, type="l", main = "Analytic Gradient", xlab = "t")


gradinput <- function(x,t){
  return(double_logis(t,x))
}

X <- matrix(0, nrow = 366, ncol = 6)
for (i in 1:366){
  X[i,] <- numDeriv::grad(gradinput, x = coef(hf_avg_fit6), t=i)
}

# X <- apply(X, MARGIN = 2, scale)
# X[,1] <- plogis(coef(hf_avg_fit6)[1])

matplot(X, type="l", main = "Numerical Gradient", xlab = "t")

```

Using these basis functions and the residuals, compute point estimates of the $\boldsymbol\delta_{s,t}$ vectors.

```{r, least-squares-delta-est-prep}
#| eval: true
#| code-fold: true
#| code-summary: Prep for delta estimates

p <- 6
k <- length(unique(layer_agg$year))
n <- ncell(hf_test_residuals)

# XtX_inv_Xt 

years <- unique(layer_agg$year)

layers_for_year <- vector(mode = "list", length = k)
doy_for_year    <- vector(mode = "list", length = k)

dates <- time(hf_test)

for (t in 1:k){
  layers_for_year[[t]] <- which(year(dates) == years[t])
  doy_for_year[[t]]    <- yday(dates[layers_for_year[[t]]])
}
names(layers_for_year) <- years
names(doy_for_year) <- years


# n <- ncell(hf_test_residuals)
delta <- array(NA, dim = c(p,k,n))

invert_issue <- rep(0,n)
```

```{r, least-squares-delta-est}
#| eval: false
#| code-fold: true
#| code-summary: Compute delta estimates

tic()
for(t in 1:length(years)){
  
  save_XtXinvXt <- list()
  
  for(s in 1:n){
    
    resid_temp <- as.numeric(terra::extract(hf_test_residuals[[layers_for_year[[t]]]], s))
    good_ind   <- !is.na(resid_temp)
    
    if (sum(good_ind) > p){
      invert_issue[s] <- 1
      row_temp   <- doy_for_year[[t]][good_ind]
      day_seq    <- paste(row_temp, collapse = "-")
      
      # save calc if not already created.
      if (day_seq %in% names(save_XtXinvXt)){
        delta[,t,s] <- save_XtXinvXt[[day_seq]]%*%resid_temp[good_ind]
      } else {
        X_temp     <- X[row_temp,]
        try({
          save_XtXinvXt[[day_seq]] <- solve(t(X_temp)%*%X_temp)%*%t(X_temp)
          delta[,t,s] <- save_XtXinvXt[[day_seq]]%*%resid_temp[good_ind]
        }, silent = T)
      }
    }
  }
}
toc()

```
<!-- ran in about 42 min for 89x101 hf full plot. -->
<!-- ran in about 2.4 min for 25x25 hf test plot. -->
ran in about 10.25 min for 50x50 test plot.

Using the point estimates of $\widehat{\boldsymbol{\delta}}_{s,t}$ and the region average $\widehat{\boldsymbol{\theta}}_0$, we can recover $\widehat{\boldsymbol{\theta}}_{s,t}$ using $\widehat{\boldsymbol{\theta}}_{s,t} = \widehat{\boldsymbol{\delta}}_{s,t} - \widehat{\boldsymbol{\theta}}_0$.

We can also apply back-transformations to recover values corresponding to the original parameterization of the double-logistic function.
```{r, plot-thetas}
#| code-fold: true
#| eval: true
#| code-summary: Plot theta estimates.

delta <- readRDS("~/ms-web/research/2023-06-26-Step-19-Spatial_Model_for_Harvard_forest/hf_50x50_delta_est.RDS")

spat_domain <- expand.grid(x = 1:sqrt(n), y = 1:sqrt(n))
spat_domain$label <- 1:n

display_ls <- vector(mode = "list", length = k)

for (t in 1:k){
  temp <- spat_domain
  temp$delta1 <- plogis(delta[1,t,] + coef(hf_avg_fit6)[1])
  temp$delta2 <- exp(delta[2,t,] + coef(hf_avg_fit6)[2])
  temp$delta3 <- (delta[3,t,] + coef(hf_avg_fit6)[3])
  temp$delta4 <- exp(delta[4,t,] + coef(hf_avg_fit6)[4])
  temp$delta5 <- (delta[5,t,] + coef(hf_avg_fit6)[5])
  temp$delta6 <- exp(delta[6,t,] + coef(hf_avg_fit6)[6])
  temp$year <- rep(years[t], n)
  display_ls[[t]] <- temp 
}

display_df <- do.call(rbind, display_ls)

panel_names <- as_labeller(function(x) paste('t =', x))

ggplot(display_df) +
    geom_tile(aes(x, y, fill=delta1)) +
    scale_y_reverse() +
    scale_fill_gradientn(colors = viridis(10),
                         limits = c(0,0.5),
                         name = bquote(theta["s,t,1"])) +
    coord_fixed() + 
    theme_void() + 
    facet_wrap(~year, labeller = panel_names)


ggplot(display_df) +
    geom_tile(aes(x, y, fill=delta2)) +
    scale_y_reverse() +
    scale_fill_gradientn(colors = viridis(10),
                         limits = c(0,1.25),
                         name = bquote(theta["s,t,2"])) +
    coord_fixed() + 
    theme_void() + 
    facet_wrap(~year, labeller = panel_names)

ggplot(display_df) +
    geom_tile(aes(x, y, fill=delta3)) +
    scale_y_reverse() +
    scale_fill_gradientn(colors = viridis(10),
                         limits = c(100, 180),
                         name = bquote(theta["s,t,3"])) +
    coord_fixed() + 
    theme_void() + 
    facet_wrap(~year, labeller = panel_names)

ggplot(display_df) +
            geom_tile(aes(x, y, fill=delta4)) +
            scale_y_reverse() +
            scale_fill_gradientn(colors = viridis(10),
                                 limits = c(0,30),
                                 name = bquote(theta["s,t,4"])) +
            coord_fixed() + 
            theme_void() + 
            facet_wrap(~year, labeller = panel_names)

ggplot(display_df) +
            geom_tile(aes(x, y, fill=delta5)) +
            scale_y_reverse() +
            scale_fill_gradientn(colors = viridis(10),
                                 limits = c(220,365),
                                 name = bquote(theta["s,t,5"])) +
            coord_fixed() + 
            theme_void() + 
            facet_wrap(~year, labeller = panel_names)

ggplot(display_df) +
            geom_tile(aes(x, y, fill=delta6)) +
            scale_y_reverse() +
            scale_fill_gradientn(colors = viridis(10),
                                 limits = c(0,40),
                                 name = bquote(theta["s,t,6"])) +
            coord_fixed() + 
            theme_void() + 
            facet_wrap(~year, labeller = panel_names)

```

Computing the least squares estimate of $\boldsymbol\delta_{s,t}$ sometimes fails due to too few satellite snapshots in a particular year. We need to identify those cases and handle them as missing data in the follow-on spatial hierarchical model.
```{r, filter-and-id-missing-val}
#| eval: true
#| code-fold: true
#| code-summary: Identify missing values


years <- years[-1]
delta <- delta[,-1,]
k     <- 36

missing_id <- list()

idx <- 1
for (s in 1:n){
  for (t in 1:k){
    
    if (is.na(delta[1,t,s])){
      missing_id[[idx]] <- c(t, s)
      idx <- idx + 1
    }
    
  }
}

missing_id <- as.data.frame(do.call(rbind, missing_id))
names(missing_id) <- c("year", "site")
table(missing_id$year)

# missing_id <- missing_id[missing_id$year != 1,]
```

Compute the sample covariance $\widehat{\boldsymbol\Omega}_t$ for each year using the least squares estimates of $\boldsymbol\delta_{s,t}$.
```{r, compute-sample-covar}
#| eval: true
#| code-fold: true
#| code-summary: Compute Sample Covariances for each year

#initialize array
Omega_arr     <- array(rep(c(diag(p)), k), dim = c(p,p,k))
inv_Omega_arr <- array(rep(c(diag(p)), k), dim = c(p,p,k))
for (t in 1:k){
  Omega_arr[,,t]     <- cov(t(matrix(delta[,t,1:n], ncol = n)), use = "complete.obs")
  inv_Omega_arr[,,t] <- solve(Omega_arr[,,t])
}

```

Now to set-up the MCMC for the spatial hierachical model.

Number of spatial locations  - 2500
Number of covariates         - 2
Dimension of response vector - 6x1
Number of MCMC iterations    - 2000

Runtime: A little over 3 hours.

```{r, mcmc-set-up}
#| code-fold: true
#| eval: true
#| code-summary: MCMC set-up

p <- 6
q <- 2
n <- ncell(hf_test)

# For now, the only covariate is time in years, centered and scaled.
# Consider changing in the future.
# Z array is 3 dim - left to right: time, covariate vector length, spatial index
Z <- array(NA, dim=c(k,q,n))
for (s in 1:n){
  #Z[,,s] <- matrix(c(rep(1,k),rnorm((q-1)*k,0,1)), ncol = q)
  Z[,,s] <- matrix(c(rep(1,k), 1:k), ncol = q)
}

niters <- 2000
burn   <- 2000
# burn   <- 0.1*niters

# storage
keep_beta_mat <- array(0, dim = c(p*q, n, niters))
keep_Lambda   <- array(0, dim = c(p*q, p*q, niters))
keep_rho_eps  <- rep(0, niters) 
keep_rho_beta <- rep(0, niters)

# initial values
mc_beta_mat <- matrix(0, nrow = p*q, ncol = n)
mc_beta_arr <- array(NA, dim = c(p, q, n))
mc_beta_arr[1:p,1:q,] <- mc_beta_mat
mc_Lambda   <- diag(p*q)
mc_rho_eps  <- 0.95
mc_rho_beta <- 0.95
keep_beta_mat[,,1] <- mc_beta_mat
keep_Lambda[,,1] <- mc_Lambda
keep_rho_eps[1] <- mc_rho_eps
keep_rho_beta[1] <- mc_rho_beta

# prior parameters
nu <- p*q - 1 + 0.1 
G  <- diag(p*q)

# M-H tuning parameters
MH_beta <- 0.05
att_beta <- 0
acc_beta <- 0

MH_eps  <- 0.05
att_eps <- 0
acc_eps <- 0

# initialize missing values to 0 (no difference from region average model)
n_miss <- nrow(missing_id)
for (i in 1:n_miss){
  delta[ , missing_id$year[i],  missing_id$site[i]] <- 0
}

```

Some pre-computes for the MCMC.
```{r, mcmc-pre-computes}
#| code-fold: true
#| eval: true
#| code-summary: MCMC pre-computes

# tic()

# Pre-compute the first summation term for V in the full conditional for beta.
sumt_ZT_Omega_inv_Z <- array(0, dim = c(p*q, p*q, n))
for (s in 1:n){
  for (j in 1:k){
    sumt_ZT_Omega_inv_Z[,,s] <- sumt_ZT_Omega_inv_Z[,,s] +
                                t(kronecker(t(Z[j,,s]),diag(p)))%*%inv_Omega_arr[,,j]%*%kronecker(t(Z[j,,s]), diag(p))
  }
}

# collect the neighbor indices for use within the sampling loop. 
# probably a better way to do this withough creating a sparse matrix.
spat_domain_g <- make_lattice(c(sqrt(n),sqrt(n)), mutual = TRUE)
W <- as_adjacency_matrix(spat_domain_g, sparse=0)
neighbor_idx     <- apply(W, MARGIN = 1, function(x) which(x==1))
neighbor_weights <- lapply(1:n, function(x) W[x, neighbor_idx[[x]]])


# Pre-compute the number of neighbors for each pixel.
w_plus <- rowSums(W)
D <- diag(w_plus)

# pre-compute residuals for the first iteration.
Bz <- delta
for (j in 1:k){
  for (s in 1:n){
    Bz[,j,s] <- mc_beta_arr[,,s]%*%Z[j,,s]
  }
}
resid <- delta - Bz

```

Now for the sampling loop. Not executed here. Results loaded next.
```{r, mcmc-sampling-loop}
#| eval: false
#| code-fold: true
#| code-summary: MCMC sampling loop

tic()
# profvis({
for (iter in 2:niters){
  
  # computing residuals needed in full conditional for beta - M.
  # probably a better way to do this.
  
  inv_mc_Lambda <- solve(mc_Lambda)
  
  # sample beta_s
  for (s in 1:n){
    
    V <- w_plus[s]*sumt_ZT_Omega_inv_Z[,,s] + w_plus[s]*inv_mc_Lambda
    
    # revisit for efficiency
    temp <- 0
    for (j in 1:k){
      temp <- temp + t(kronecker(t(Z[j,,s]),diag(p)))%*%inv_Omega_arr[,,j]%*%(mc_rho_eps*resid[,j,neighbor_idx[[s]]]%*%neighbor_weights[[s]] +w_plus[s]*delta[,j,s])
    }
    M <- temp + mc_rho_beta*(inv_mc_Lambda%*%(mc_beta_mat[,neighbor_idx[[s]]]%*%neighbor_weights[[s]]))
      
    V_inv <- chol2inv(chol(V))
    mc_beta_mat[,s]  <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p*q)
    # mc_beta_mat[,s]  <- beta_mat[,s]
    
  }
  mc_beta_arr[1:p,1:q,] <- mc_beta_mat
  
  # sample Lambda
  H <- mc_beta_mat%*%(D - mc_rho_beta*W)%*%t(mc_beta_mat)
  mc_Lambda <- MCMCpack::riwish(nu + n, G + H)
  # mc_Lambda <- Lambda
  
  # compute residuals
  Bz <- delta
  for (j in 1:k){
    for (s in 1:n){
      Bz[,j,s] <- mc_beta_arr[,,s]%*%Z[j,,s]
    }
  }
  resid <- delta - Bz
  
  # compute sums for rho_beta & rho_eps likelihood in one loop.
  sum1_rho_beta <- 0
  sum2_rho_beta <- 0
  sum1_rho_eps  <- 0
  sum2_rho_eps  <- 0
  
  for (s in 1:n){
    
    beta_tilde    <- (mc_beta_mat[,neighbor_idx[[s]]]%*%neighbor_weights[[s]])/w_plus[s]
    sum1_rho_beta <- sum1_rho_beta + w_plus[s]*(t(beta_tilde)%*%inv_mc_Lambda%*%beta_tilde) 
    sum2_rho_beta <- sum2_rho_beta + w_plus[s]*(t(mc_beta_mat[,s])%*%inv_mc_Lambda%*%beta_tilde)
    
    for (t in 1:k){
    
      r <- resid[,t,neighbor_idx[[s]]]%*%neighbor_weights[[s]]
      sum1_rho_eps <- sum1_rho_eps + (t(r)%*%inv_Omega_arr[,,t]%*%r)/w_plus[s]
      sum2_rho_eps <- sum2_rho_eps + (t(delta[,t,s]) - t(Bz[,t,s]))%*%inv_Omega_arr[,,t]%*%r
      
    }
  }
  
  # sample rho_eps
  att_eps <- att_eps + 1
  can <- rtnorm(1, mc_rho_eps, MH_eps, a = 0, b = 1)
  R   <- (-0.5*((can^2)*sum1_rho_eps - 2*can*sum2_rho_eps)) -
         (-0.5*((mc_rho_eps^2)*sum1_rho_eps - 2*mc_rho_eps*sum2_rho_eps)) + 
         dtnorm(mc_rho_eps, mean = can        , sd = MH_eps, a = 0, b = 1, log = T) -      # M-H adjustment
         dtnorm(can,        mean = mc_rho_eps , sd = MH_eps, a = 0, b = 1, log = T)
  if(log(runif(1)) < R){
    acc_eps <- acc_eps + 1
    mc_rho_eps  <- can
  }
  # mc_rho_eps <- rho_eps
  
  # sample rho_beta
  att_beta <- att_beta + 1
  can <- rtnorm(1, mc_rho_beta, MH_beta, a = 0, b = 1)
  R   <- (-0.5*((can^2)*sum1_rho_beta - 2*can*sum2_rho_beta)) -
         (-0.5*((mc_rho_beta^2)*sum1_rho_beta - 2*mc_rho_beta*sum2_rho_beta)) + 
         dtnorm(mc_rho_beta, mean = can         , sd = MH_beta, a = 0, b = 1, log = T) -      # M-H adjustment
         dtnorm(can,         mean = mc_rho_beta , sd = MH_beta, a = 0, b = 1, log = T)
  if(log(runif(1)) < R){
    acc_beta <- acc_beta + 1
    mc_rho_beta  <- can
  }
  # mc_rho_beta <- rho_beta
  
  # tuning
  if(iter < burn){
    if(att_eps > 100){
      if(acc_eps/att_eps < 0.3){MH_eps <- MH_eps*0.8}
      if(acc_eps/att_eps > 0.5){MH_eps <- MH_eps*1.2}
      acc_eps <- att_eps <- 0
    }
  }
  
  if(iter < burn){
    if(att_beta > 100){
      if(acc_beta/att_beta < 0.3){MH_beta <- MH_beta*0.8}
      if(acc_beta/att_beta > 0.5){MH_beta <- MH_beta*1.2}
      acc_beta <- att_beta <- 0
    }
  }
  
  keep_beta_mat[,,iter] <- mc_beta_mat 
  keep_Lambda[,,iter]   <- mc_Lambda
  keep_rho_eps[iter]    <- mc_rho_eps
  keep_rho_beta[iter]   <- mc_rho_beta
  
  # sample missing values
  n_miss <- nrow(missing_id)
  for (i in 1:n_miss){
      
      s <- missing_id$site[i]
      t <- missing_id$year[i]
    
          V <- Omega_arr[,,t]/w_plus[s]
          M <- Bz[,t,s] + mc_rho_eps*resid[,t,neighbor_idx[[s]]]%*%neighbor_weights[[s]]/w_plus[s]
    
      delta[ , t, s] <- M+t(chol(V))%*%rnorm(p)
      
  }
  
}
# })
toc()
```

Load the MCMC results from a previous code execution.
```{r, read-mcmc-results}
#| eval: true
#| code-fold: true
#| code-summary: Read MCMC result from pre-ran code.

mcmc_result <- readRDS("~/ms-web/research/2023-06-26-Step-19-Spatial_Model_for_Harvard_forest/mcmc_result_hf_50x50_2000iter.RDS")

keep_beta_mat <- mcmc_result$keep_beta_mat
keep_Lambda <- mcmc_result$keep_Lambda
keep_rho_eps <- mcmc_result$keep_rho_eps
keep_rho_beta <- mcmc_result$keep_rho_beta

```


Now time for a lot of trace plots. First for $\boldsymbol\beta_s$. In most cases convergence appears to occur almost immediately. Sometimes it can take up to 1000 iterations where the chain is slowly moving towards a stable mean value.
```{r, traceplots-beta}
#| eval: true
#| code-fold: true
#| code-summary: Trace plots for beta parameters

burn <- 1

location_sample <- sample(1:n, size = 5)

beta_subscripts <- paste(rep(1:q, each = p), rep(1:p, times = q), sep = ",")

for (s in location_sample){
  par(mfcol = c(3,2))
  for (i in 1:(p*q)){
    tsub <- beta_subscripts[i]
    plot(burn:niters, 
         keep_beta_mat[i,s,burn:niters], 
         type = "l",
         xlab = "iteration",
         ylab = bquote(beta[.(tsub)]))
    mtext(paste("Pixel: ",s), side = 3, line = -3, outer = TRUE)
  }
  # mtext(paste("Pixel: ",s), side = 3, line = -3, outer = TRUE)
}

```


```{r, traceplots-lambda}
#| eval: true
#| code-fold: true
#| code-summary: Trace plots for Lambda matrix

burn <- 100
par(mfrow = c(3,2))

# off-diagonal
for (i in 1:(p*q)){
  for (j in i:(p*q)){
    tsub <- paste(i,j, sep = ", ")
    plot(burn:niters, 
         keep_Lambda[i,j,burn:niters], 
         type = "l",
         xlab = "iteration",
         ylab = bquote(Lambda[.(tsub)]))
  }
  mtext(bquote(Lambda ~ "Matrix"), side = 3, line = -3, outer = TRUE)
}

```
Focusing on just the diagonal of $\boldsymbol\Lambda$.
```{r, traceplots-lambda-diag}
#| eval: true
#| code-fold: true
#| code-summary: Trace plots for Lambda matrix diagonal.

burn <- 100
par(mfrow = c(3,2))

# off-diagonal
for (i in 1:(p*q)){
    tsub <- paste(i,i, sep = ", ")
    plot(burn:niters, 
         keep_Lambda[i,i,burn:niters], 
         type = "l",
         xlab = "iteration",
         ylab = bquote(Lambda[.(tsub)]))
    mtext(bquote(Lambda ~ "Matrix"), side = 3, line = -3, outer = TRUE)
}


```

Traceplots of $\boldsymbol\Lambda$ transformed to correlations.
```{r, traceplots-lambda-corr}
#| eval: true
#| code-fold: true
#| code-summary: Trace plots for Lambda transformed to correlations.

keep_Lambda_corr <- keep_Lambda

for (iter in 1:niters){
  keep_Lambda_corr[,,iter] <- cov2cor(keep_Lambda[,,iter])
}

burn <- 100
par(mfrow = c(3,2))

# off-diagonal
for (i in 1:(p*q)){
  for (j in i:(p*q)){
    tsub <- paste(i,j, sep = ", ")
    plot(burn:niters, 
         keep_Lambda_corr[i,j,burn:niters], 
         type = "l",
         xlab = "iteration",
         ylab = bquote(Lambda[.(tsub)]))
  }
  mtext(bquote(Lambda ~ "Matrix"), side = 3, line = -3, outer = TRUE)
}

```

Traceplots for $\rho_{\beta}$ and $\rho_{\epsilon}$ appear less than ideal. These values were initialized at 0.8 - swiftly moved to an $\varepsilon > 0$ below 1.
```{r, traceplots-rho}
#| eval: true
#| code-fold: true
#| code-summary: Traceplots for CAR propriety params

burn <- 100
par(mfrow=c(1,2))

plot(burn:niters, 
         keep_rho_beta[burn:niters], 
         type = "l",
         xlab = "iteration",
         ylab = bquote(rho[beta]))

plot(burn:niters, 
         keep_rho_eps[burn:niters], 
         type = "l",
         xlab = "iteration",
         ylab = bquote(rho["\u03F5"]))

```

Mapping the estimates of the intercept and time effect on $\boldsymbol\delta$.
```{r, plot-beta-est}
#| eval: true
#| code-fold: true
#| code-summary: Map of beta elements 3 and 5 (effects for SOS and EOS)

spat_domain$beta13 <- rowMeans(keep_beta_mat[3, 1:n, 1001:niters])
spat_domain$beta23 <- rowMeans(keep_beta_mat[9, 1:n, 1001:niters])

ggplot(spat_domain) +
  geom_tile(aes(x, y, fill=beta13)) +
  scale_y_reverse() +
  scale_fill_gradientn(colors = viridis(10),
                       name = bquote(beta["s,1,3"])) +
  coord_fixed() + 
  theme_void()

ggplot(spat_domain) +
  geom_tile(aes(x, y, fill=beta23)) +
  scale_y_reverse() +
  scale_fill_gradientn(colors = viridis(10),
                       name = bquote(beta["s,2,3"])) +
  coord_fixed() + 
  theme_void()


spat_domain$beta15 <- rowMeans(keep_beta_mat[5, 1:n, 501:niters])
spat_domain$beta25 <- rowMeans(keep_beta_mat[11, 1:n, 501:niters])

ggplot(spat_domain) +
  geom_tile(aes(x, y, fill=beta15)) +
  scale_y_reverse() +
  scale_fill_gradientn(colors = viridis(10),
                       name = bquote(beta["s,1,5"])) +
  coord_fixed() + 
  theme_void()

ggplot(spat_domain) +
  geom_tile(aes(x, y, fill=beta25)) +
  scale_y_reverse() +
  scale_fill_gradientn(colors = viridis(10),
                       name = bquote(beta["s,2,5"])) +
  coord_fixed() + 
  theme_void()


```

Compute the fitted values and residuals.
```{r, fitted-values-and-residuals}
#| eval: true
#| code-fold: true
#| code-summary: Compute the fitted values and residuals

delta_fitted <- delta

burn  <- 1001
beta1 <- matrix(0, nrow = 6, ncol = n)
beta2 <- beta1

for (i in 1:p){
  beta1[i,] <- rowMeans(keep_beta_mat[i, 1:n, burn:niters])
  beta2[i,] <- rowMeans(keep_beta_mat[(i+p), 1:n, burn:niters])
}



for (s in 1:n){
  for (t in 1:k){
    delta_fitted[,t,s] <- beta1[,s] + beta2[,s]*Z[t,2,s]
  }
}

delta_resid <- delta_fitted - delta
```

Mapping the residuals.
```{r, plot-residuals-map}
#| eval: true
#| code-fold: true
#| code-summary: Plot delta residuals by year for each component of delta

resid_display_ls <- vector(mode = "list", length = k)

for (t in 1:k){
  temp <- spat_domain
  temp$resid1 <- delta_resid[1,t,]
  temp$resid2 <- delta_resid[2,t,]
  temp$resid3 <- delta_resid[3,t,]
  temp$resid4 <- delta_resid[4,t,]
  temp$resid5 <- delta_resid[5,t,]
  temp$resid6 <- delta_resid[6,t,]
  temp$year <- rep(years[t], n)
  resid_display_ls[[t]] <- temp 
}

resid_display_df <- do.call(rbind, resid_display_ls)

panel_names <- as_labeller(function(x) paste('t =', x))

ggplot(resid_display_df) +
    geom_tile(aes(x, y, fill=resid1)) +
    scale_y_reverse() +
    scale_fill_gradientn(colors = viridis(10),
                         name = bquote(delta_resid["s,t,1"])) +
    coord_fixed() + 
    theme_void() + 
    facet_wrap(~year, labeller = panel_names)


ggplot(resid_display_df) +
    geom_tile(aes(x, y, fill=resid2)) +
    scale_y_reverse() +
    scale_fill_gradientn(colors = viridis(10),
                         name = bquote(delta_resid["s,t,2"])) +
    coord_fixed() + 
    theme_void() + 
    facet_wrap(~year, labeller = panel_names)

ggplot(resid_display_df) +
    geom_tile(aes(x, y, fill=resid3)) +
    scale_y_reverse() +
    scale_fill_gradientn(colors = viridis(10),
                         limits = c(-100,200),
                         name = bquote(delta_resid["s,t,3"])) +
    coord_fixed() + 
    theme_void() + 
    facet_wrap(~year, labeller = panel_names)

ggplot(resid_display_df) +
    geom_tile(aes(x, y, fill=resid4)) +
    scale_y_reverse() +
    scale_fill_gradientn(colors = viridis(10),
                         name = bquote(delta_resid["s,t,4"])) +
    coord_fixed() + 
    theme_void() + 
    facet_wrap(~year, labeller = panel_names)

ggplot(resid_display_df) +
    geom_tile(aes(x, y, fill=resid5)) +
    scale_y_reverse() +
    scale_fill_gradientn(colors = viridis(10),
                         name = bquote(delta_resid["s,t,5"])) +
    coord_fixed() + 
    theme_void() + 
    facet_wrap(~year, labeller = panel_names)

ggplot(resid_display_df) +
    geom_tile(aes(x, y, fill=resid6)) +
    scale_y_reverse() +
    scale_fill_gradientn(colors = viridis(10),
                         name = bquote(delta_resid["s,t,6"])) +
    coord_fixed() + 
    theme_void() + 
    facet_wrap(~year, labeller = panel_names)

```

There are some extreme residuals which affect the gradient scale of the plots above. We will set limits using the 2.5 and 97.5 quantiles as limits. Any pixel residual falling outside these limits are highlighted in red.
```{r}
#| eval: true
#| code-fold: true
#| code-summary: Residual quantiles

resid_quantiles <- apply(resid_display_df[,8:13],2,quantile,probs=c(0.025,0.975))
resid_quantiles

```

```{r}
#| eval: true
#| code-fold: true
#| code-summary: Plot residuals again with limits from quantiles.

ggplot(resid_display_df) +
    geom_tile(aes(x, y, fill=resid1)) +
    scale_y_reverse() +
    scale_fill_gradientn(colors = viridis(10),
                         # limits = c(-1,1),
                         limits = resid_quantiles[,1],
                         name = bquote(delta_resid["s,t,1"]),
                         na.value = "red") +
    coord_fixed() + 
    theme_void() + 
    facet_wrap(~year, labeller = panel_names)

ggplot(resid_display_df) +
    geom_tile(aes(x, y, fill=resid2)) +
    scale_y_reverse() +
    scale_fill_gradientn(colors = viridis(10),
                         # limits = c(-0.5,0.5),
                         limits = resid_quantiles[,2],
                         name = bquote(delta_resid["s,t,2"]),
                         na.value = "red") +
    coord_fixed() + 
    theme_void() + 
    facet_wrap(~year, labeller = panel_names)

ggplot(resid_display_df) +
    geom_tile(aes(x, y, fill=resid3)) +
    scale_y_reverse() +
    scale_fill_gradientn(colors = viridis(10),
                         # limits = c(-20,20),
                         limits = resid_quantiles[,3],
                         name = bquote(delta_resid["s,t,3"]),
                         na.value = "red") +
    coord_fixed() + 
    theme_void() + 
    facet_wrap(~year, labeller = panel_names)

ggplot(resid_display_df) +
    geom_tile(aes(x, y, fill=resid4)) +
    scale_y_reverse() +
    scale_fill_gradientn(colors = viridis(10),
                         # limits = c(-2,2),
                         limits = resid_quantiles[,4],
                         name = bquote(delta_resid["s,t,4"]),
                         na.value = "red") +
    coord_fixed() + 
    theme_void() + 
    facet_wrap(~year, labeller = panel_names)

ggplot(resid_display_df) +
    geom_tile(aes(x, y, fill=resid5)) +
    scale_y_reverse() +
    scale_fill_gradientn(colors = viridis(10),
                         # limits = c(-20,20),
                         limits = resid_quantiles[,5],
                         name = bquote(delta_resid["s,t,5"]),
                         na.value = "red") +
    coord_fixed() + 
    theme_void() + 
    facet_wrap(~year, labeller = panel_names)

ggplot(resid_display_df) +
    geom_tile(aes(x, y, fill=resid6)) +
    scale_y_reverse() +
    scale_fill_gradientn(colors = viridis(10),
                         # limits = c(-1.5,1.5),
                         limits = resid_quantiles[,6],
                         name = bquote(delta_resid["s,t,6"]),
                         na.value = "red") +
    coord_fixed() + 
    theme_void() + 
    facet_wrap(~year, labeller = panel_names)

```

Plot histograms of residuals for each component of $\delta_{+,t}$. Some residuals are clearly not centered on 0. Perhaps this is due to a mean model misspecification?
```{r, plot-residuals-histogram}
#| eval: true
#| code-fold: true
#| code-summary: Histograms of residuals for each year

par(mfrow = c(2,3))   
for (t in 1:k){
  for (i in 1:p){
    subscript <- paste(c("+",years[t],i), collapse = ",")
    hist(delta_fitted[i,t,] - delta[i,t,], main = bquote(delta_resid[.(subscript)]))
  }     
}
```


Store MCMC results for publishing.
```{r}
#| eval: false

#| code-fold: true
#| code-summary: Store MCMC results for publishing.
# 
# mcmc_result <- list("keep_beta_mat" = keep_beta_mat,
#                     "keep_Lambda" = keep_Lambda,
#                     "keep_rho_eps" = keep_rho_eps,
#                     "keep_rho_beta" = keep_rho_beta)
# 
# saveRDS(mcmc_result, "~/ms-web/research/2023-06-26-Step-19-Spatial_Model_for_Harvard_forest/mcmc_result_hf_50x50_2000iter.RDS")


```











