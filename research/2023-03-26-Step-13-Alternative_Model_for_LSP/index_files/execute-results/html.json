{
  "hash": "0801f1650c3a957c80a166ad604b744b",
  "result": {
    "markdown": "---\ntitle: \"Step 13 - An alternative Heirarchical Model for Spatial Land Surface Phenology - Pixel Climate Trends\"\ndescription: \"Develop a streamlined analysis workflow to identify climate trends.\"\nauthor:\n  - name: Matthew Shisler\n    affiliation: North Carloina State University - Department of Statistics\n    affiliation-url: https://statistics.sciences.ncsu.edu/ \ndate: \"3/29/2023\"\ncategories: [Bayesian, MCMC, Spatial, MCAR] # self-defined categories\ndraft: false \nformat:\n  html: \n    code-fold: false\nexecute: \n  eval: true\n  freeze: false\n---\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code: Load the packages\"}\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(viridis)\nlibrary(matrixsampling)\nlibrary(MCMCpack)\nlibrary(gridExtra)\nlibrary(extraDistr)\nlibrary(tictoc)\n```\n:::\n\n\n## Intro\n\nHere we will develop a workflow for a spatial analysis of land-surface phenology (LSP) using a vegetation index (VI) constructed from remotely-sensed surface reflectance and emmitance.\n\nLet $\\mathcal{D}$ be the spatial domain partitioned into a regular lattice of $n$ areal units which we will refer to as \"locations\" or \"pixels\". Index these units by $s \\in S,\\, S = \\{1,2,\\dots,n\\}$. \n\nFor year $t \\in T$, a satellite captures imagery from the earth's surface on a subset of days $\\mathbf{d}_t \\subset \\{1, 2, \\dots, m\\}$ where $m$ may take on a value of 365 or 366 - the choice of does not appear to be consequential. \n\n\nLet $Y_{s,t,j} \\in (0,1)$ be the vegetation index observed on day $d_{t,j}$ of year $t$ at pixel $s$. Though the VI is restricted to the interval $(0,1)$, we nevertheless adopt a Gaussian model with mean function $v(d;\\boldsymbol\\theta)$, parameterized by the $p \\times 1$ vector $\\boldsymbol\\theta$, and variance $\\sigma^2$ which corresponds to the random noise in the satellite measurements. The assumption is that the true mean is bounded sufficiently far from the physical limits of $0$ and $1$ and the random noise is small so that there is practically no scenario in which we would be required to model values of $0$ or $1$. Notationally,\n\n$$\nY_{s,t,j} \\sim \\text{Normal}\\left(v(d_{t,j};\\boldsymbol\\theta_{s,t}), \\sigma^2\\right)\n$$\nwhere $v(d;\\boldsymbol{\\theta}) : \\mathbb{R} \\rightarrow \\mathbb{R}$ is the so-called double-logistic function,\n$$\nv(d; \\boldsymbol{\\theta}) = \\theta_1 + (\\theta_2 - \\theta_7d)\\left(\\frac{1}{1 +\\exp\\left\\{\\frac{\\theta_3 - d}{\\theta_5}\\right\\}} - \\frac{1}{1 +\\exp\\left\\{\\frac{\\theta_4 - d}{\\theta_6}\\right\\}}\\right)\n$$\nThe parameters of the double-logistic function are readily interpretable in a way that will be explained later.\n\nAt the first step in the analysis we aggregate the data over some pre-defined space and time domain whose \"typical\" or \"average\" phenological characteristics are to be estimated using a frequentist non-linear least squares regression. This amounts to estimating $\\boldsymbol\\theta$ for the aggregated data. Denote the estimate as $\\widehat{\\boldsymbol\\theta}_{0}$. We then linearize the double-logistic function centered on $\\widehat{\\boldsymbol\\theta}_{0}$,\n$$\nv(d;\\boldsymbol\\theta) \\approx v(d; \\widehat{\\boldsymbol\\theta}_0) + \\nabla_{\\boldsymbol\\theta} v(d; \\boldsymbol\\theta)|_{\\boldsymbol\\theta = \\widehat{\\boldsymbol\\theta}_0}(\\boldsymbol\\theta -\\widehat{\\boldsymbol\\theta}_0)\n$$\n\nFrom this linearization we define the following,\n$$\n\\begin{align*}\nr_{s,t,j} &= Y_{s,t,j} - v(d_{t,j}, \\widehat{\\boldsymbol\\theta}_0),\\\\\nX_0(d) &=  \\nabla_{\\boldsymbol\\theta} v(d; \\boldsymbol\\theta)|_{\\boldsymbol\\theta = \\widehat{\\boldsymbol\\theta}_0},\\\\\n\\boldsymbol\\delta_{s,t} &= \\boldsymbol\\theta_{s,t} - \\widehat{\\boldsymbol\\theta}_0.\n\\end{align*}\n$$\nHere, $r_{s,t,j}$ are the residuals from the model of the domain's typical phenological characteristics, the gradient $X_0(d) : \\mathbb{R} \\rightarrow \\mathbb{R}^p$ is regarded as a set of basis functions, and $\\boldsymbol\\delta_{s,t}$ represents the deviation in the phenological characteristics of pixel $s$ in year $t$ from the domain's typical characteristics, $\\widehat{\\boldsymbol\\theta}_0$.\n\nNext, we construct the $m \\times p$ \"parent\" design matrix $\\mathbf{X}_0$ with rows $X_0(d)$, $d = 1,2,\\dots,m$. Then define the \"child\" design matrix $\\mathbf{X}_{s,t}$ for each pixel-year pair $(s,t)$ by sub-setting the rows of $\\mathbf{X}_0$ that correspond to the days $\\mathbf{d}_t$ for which VI measurements were collected at pixel $s$.\n\nThis facilitates modeling through the residuals,\n$$\n\\mathbf{r}_{s,t} \\sim \\text{Normal}\\left(\\mathbf{X}_{s,t}\\boldsymbol\\delta_{s,t}, \\sigma^2\\mathbf{I}\\right)\n$$\nwhere $\\mathbf{r}_{s,t}$ is the vector of residuals at pixel-year pair $(s,t)$ and $\\mathbf{I}$ is an identity matrix of sufficient dimension, unspecified because the number of VI measurements vary across pixel-year pairs.\n\nWe have transformed the task of modeling a non-linear mean function, $v(d;\\boldsymbol\\theta)$, to modeling a linear mean function $\\mathbf{X}\\boldsymbol\\delta$. A key assumption is that any specific pixel-year pair does not deviate substantially from the domain's typical phenological characteristics. Otherwise the linearization will produce a poor approximation to the original non-linear mean function.\n\nIf the goal of the analysis is to identify climate trends across pixels in the spatial domain as a function of other climatological factors, then we may consider collapsing this model into a more computationally manageable scheme.\n\nWe can construct the usual estimates for $\\boldsymbol\\delta_{s,t}$ and their corresponding variance, $\\text{Var}(\\widehat{\\boldsymbol\\delta}_{s,t})$, from our model of the residuals. These are of course,\n$$\n\\widehat{\\boldsymbol\\delta}_{s,t} = \\left(\\mathbf{X}^T_{s,t}\\mathbf{X}_{s,t}\\right)^{-1}\\mathbf{X}^T_{s,t}\\mathbf{r}_{s,t} \\quad \\text{and} \\quad \\text{Var}(\\widehat{\\boldsymbol\\delta}_{s,t}) = \\sigma^2\\left(\\mathbf{X}^T_{s,t}\\mathbf{X}_{s,t}\\right)^{-1} = \\sigma^2\\boldsymbol{\\mathcal{X}}^{-1}_{s,t}\n$$\nleaving $\\sigma^2$ to be estimated later.\n\nAs a parenthetical note, we could go so far as to consider $\\sigma^2$ fixed by using the results of research on the measurement noise associated with atmospheric attenuation and the optical limitations of the satellite sensors.\n\nWe may from time to time use an emperical estimate for the covariance of the deviation vectors. We construct these by collecting $\\widehat{\\boldsymbol\\delta}_{s,t}$ over all spatial locations $s \\in \\mathcal{D}$ for each year $t \\in T$, then computing the sample covariance matrix\n$$\n\\widehat{\\boldsymbol\\Omega}_t = \\frac{1}{n-1}\\sum_{s=1}^n(\\widehat{\\boldsymbol\\delta}_{s,t} - \\bar{\\widehat{\\boldsymbol\\delta}}_{\\cdot,t})(\\widehat{\\boldsymbol\\delta}_{s,t} - \\bar{\\widehat{\\boldsymbol\\delta}}_{\\cdot,t})^T\n$$\nnote: but we only assume conditional independence of $\\widehat{\\boldsymbol\\delta}_{s,t}$, is this still okay to do?\n\n\nFrom here we will consider a Bayesian hierarchical model incorporating climatological covariates, $\\boldsymbol{\\mathcal{Z}}_{s,t}$ for pixel-year pair $(s,t)$, and the associated spatial effects $\\boldsymbol\\beta_s$ on which we place a multivariate conditionally autoregressive (MCAR) prior with common propriety parameter $\\rho$. \n\nWe define $\\mathbf{W}$ as the first-order neighborhood matrix for the spatial domain $\\mathcal{D}$. The $s,s'$ entry of $\\mathbf{W}$, $w_{s,s'}$ is $1$ if spatial locations $s$ and $s'$ are adjacent to one another and $0$ otherwise. The number of neighbors for location $s$ is the corresponding row sum of $\\mathbf{W}$, $w_{s+}$.\n\nThe model is,\n$$\n\\begin{align*}\n\\widehat{\\boldsymbol{\\delta}}_{s,t} &= \\mathcal{Z}_{s,t}\\boldsymbol\\beta_{s} + \\boldsymbol\\epsilon_{s,t}\\\\\n\\boldsymbol\\epsilon_{s,t} &\\sim \\text{MCAR}(\\rho_\\epsilon, \\widehat{\\boldsymbol\\Omega}_t)\\\\\n\\\\\n\\boldsymbol\\beta_{s} &\\sim \\text{MCAR}(\\rho_\\beta, \\boldsymbol\\Lambda)\\\\\n\\boldsymbol\\Lambda &\\sim \\text{InvWishart}(\\nu, \\mathbf{G})\\\\\n\\rho_\\beta &\\sim \\text{Unif}(0,1)\\\\\n\\rho_\\epsilon &\\sim \\text{Unif}(0,1)\n\\end{align*}\n$$\nwhere $\\text{MCAR}(\\rho, \\boldsymbol\\Lambda)$ represents the typical proper MCAR prior,\n$$\n\\boldsymbol\\beta_s \\,|\\, \\boldsymbol\\beta_{s' \\ne s}, \\boldsymbol\\Lambda, \\rho_\\beta \\sim \\text{Normal}\\left(\\rho_\\beta\\sum_{s' \\in \\mathcal{N}(s)}\\frac{w_{s,s'}}{w_{s+}}\\boldsymbol\\beta_{s'}, \\frac{\\boldsymbol\\Lambda}{w_{s+}}\\right)\n$$\n$$\n\\widehat{\\boldsymbol\\delta}_{s,t} \\,|\\, \\widehat{\\boldsymbol\\delta}_{s' \\ne s,t}, \\, \\widehat{\\boldsymbol\\Omega}_t, \\rho_{\\epsilon} \\sim \\text{Normal}\\left(\\boldsymbol{\\mathcal{Z}}_{s,t}\\boldsymbol\\beta_s +\\rho_{\\epsilon}\\sum_{s' \\in \\mathcal{N}(s)} \\frac{w_{s,s'}}{w_{s+}}\\left(\\widehat{\\boldsymbol{\\delta}}_{s',t} - \\boldsymbol{\\mathcal{Z}}_{s',t}\\boldsymbol\\beta_{s'}\\right), \\frac{\\widehat{\\boldsymbol{\\Omega}}_t}{w_{s+}} \\right)\n$$\n\nThe full conditionals for this model are:\n\n$$\n\\begin{align*}\n\\boldsymbol\\beta_s \\,|\\, \\text{rest} &\\sim \\text{Normal}\\left(\\mathbf{V}^{-1}_{\\beta_s}\\mathbf{M}_{\\beta_s}, \\mathbf{V}^{-1}_{\\beta_s}\\right)\\\\\n\\mathbf{V}_{\\beta_s} &= w_{s+}\\left(\\sum_{t\\in T}\\boldsymbol{\\mathcal{Z}}^T_{s,t}\\widehat{\\boldsymbol\\Omega}^{-1}_{t}\\boldsymbol{\\mathcal{Z}}_{s,t} + \\boldsymbol\\Lambda^{-1}\\right)\\\\\n\\mathbf{M}_{\\beta_s} &= \\sum_{t\\in T} \\left[\\ \\boldsymbol{\\mathcal{Z}}^T_{s,t}\\widehat{\\boldsymbol\\Omega}^{-1}_t\\left(\\rho_{\\epsilon}\\sum_{s' \\in \\mathcal{N}(s)}w_{s,s'}\\left(\\widehat{\\boldsymbol\\delta}_{s',t} - \\boldsymbol{\\mathcal{Z}}_{s',t}\\boldsymbol\\beta_{s'}\\right) + w_{s+}\\widehat{\\boldsymbol\\delta}_{s,t}\\right)\\right] + \\rho_{\\beta}\\boldsymbol\\Lambda^{-1}\\sum_{s' \\in \\mathcal{N}(s)} w_{s,s'}\\boldsymbol\\beta_{s'}\\\\\n\\\\\\\\\n\\boldsymbol\\Lambda \\,|\\, \\text{rest} &\\sim \\text{InvWishart}\\left(\\nu + n, \\mathbf{G} + \\mathbf{H}\\right)\\\\\nn &= \\text{number of pixels}\\\\\n\\mathbf{H} &= \\boldsymbol{\\mathcal{B}}^T(\\mathbf{D}-\\rho\\mathbf{W})\\boldsymbol{\\mathcal{B}}\\\\\n\\end{align*}\n$$\nHere $\\boldsymbol{\\mathcal{B}}$ is a $n \\times pq$ matrix with rows equal to $\\boldsymbol\\beta^T_s$  and $\\mathbf{D}$ is a diagonal matrix of the row sums from $\\mathbf{W}$.\n\nFurther, $n = |S|$, the number of pixels, and $k = |T|$, the number of years.\n\nThough we defined the elements of the neighborhood matrix, $\\mathbf{W}$, to be either $0$ or $1$, we've elected to keep them in the derivation of the full conditional\n\n\nNext will simulate data from this model. What follows is some scratch code for the simulation. Here we set $p = 2$, the dimension of the response vectors, $q = 2$, the number of covariates, and $k=20$ years.\n\nWe start by constructing the spatial domain. In this case a regular lattice partitioned into $n = 25^2$ areal units. The neighborhood matrix is the 1st order neighborhood structure.\n\n::: {.cell}\n\n```{.r .cell-code}\n# specify spatial domain\n\nn <- 25^2\nspat_domain <- expand.grid(x = 1:sqrt(n), y = 1:sqrt(n))\nspat_domain$label <- 1:n\n\nspat_domain_g <- make_lattice(c(sqrt(n),sqrt(n)), mutual = TRUE)\nW <- as_adjacency_matrix(spat_domain_g, sparse=0)\nD <- diag(rowSums(W))\n```\n:::\n\n\n\nWe sample the spatial random noise vectors for each pixel-year pair. At least initially, we specify these errors with no spatial variation by setting the $\\rho_\\epsilon$ parameter equal to 0.\n\n::: {.cell}\n\n```{.r .cell-code}\np = 2 \nq = 2  \nk = 20\n\n# propriety parameter for epsilon MCAR\nrho_eps <- 0\n\n# Conditional Covariance Matrix.\n# Assume Omega_t is constant across time for now.\n# first, specify disired correlation and std deviation, then construct covariance matrix.\ncOmega <- matrix(c(1, 0,\n                   0, 1), byrow = T, ncol = p)\n\nstddev_eps <- diag(c(1,1))\n\nOmega <- stddev_eps%*%cOmega%*%stddev_eps\n\n# Construct MCAR joint distribution covariance\ninv_Sigma_eps <- kronecker((D-rho_eps*W), solve(Omega))\n\n# Draw from eps ~ MCAR(rho_eps, Omega_t = Omega)\n# eps array is 3-dim - left to right: response length, time, spatial index.\neps <- array(NA, dim = c(p,k,n))\nfor (t in 1:k){  \n  eps[,t,] <- t(matrix(backsolve(chol(inv_Sigma_eps), rnorm(n*p)), byrow=T, ncol=p))\n} \n```\n:::\n\n\n\n\nWe can plot a sequence by year of the 1st and 2nd element in the random noise vector, $\\boldsymbol\\epsilon_{s,t}$\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndisplay_ls <- vector(mode = \"list\", length = k)\n\nfor (t in 1:k){\n  temp <- spat_domain\n  temp$eps1 <- eps[1,t,]\n  temp$eps2 <- eps[2,t,]\n  temp$year <- rep(t, n)\n  display_ls[[t]] <- temp \n}\n\ndisplay_df <- do.call(rbind, display_ls)\n\npanel_names <- as_labeller(function(x) paste('t =', x))\n\nggplot(display_df) +\n            geom_tile(aes(x, y, fill=eps1)) +\n            scale_y_reverse() +\n            scale_fill_gradientn(colors = viridis(10),\n                                 limits = c(min(display_df$eps1),max(display_df$eps1)),\n                                 name = bquote(\"\\u03F5\"[\"s,t,1\"])) +\n            coord_fixed() + \n            theme_void() + \n            facet_wrap(~year, labeller = panel_names)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(display_df) +\n            geom_tile(aes(x, y, fill=eps2)) +\n            scale_y_reverse() +\n            scale_fill_gradientn(colors = viridis(10),\n                                 limits = c(min(display_df$eps2),max(display_df$eps2)),\n                                 name = bquote(\"\\u03F5\"[\"s,t,2\"])) +\n            coord_fixed() + \n            theme_void() + \n            facet_wrap(~year, labeller = panel_names)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n:::\n\n\nWith $p = q = 2$, we will need to sample $\\boldsymbol\\beta_{s}$ as a $4 \\times 1$ of stacked $2 \\times 1$ $\\boldsymbol\\beta_{s,q}$ vectors for each site from $\\text{MCAR}(\\rho_\\beta, \\Lambda)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# propriety parameter for beta MCAR\nrho_beta <- 0.99\n\n# Conditional Covariance Matrix\n# beta_s is regarded as a vector of stacked beta_1, beta_2 vectors\n# Assume beta_1 and beta_2 vectors are independent for now.\n\ncLambda <- diag(p*q)\n\ncLambda <- matrix(c(  1,   0,   0,   0,\n                      0,   1,   0,   0,\n                      0,   0,   1, 0.7,\n                      0,   0, 0.7,   1 ), byrow = T, ncol = p*q)\n\nstddev_beta <- diag(c(2,2,4,8))\n\nLambda <- stddev_beta%*%cLambda%*%stddev_beta\n\n# Construct MCAR joint distribution covariance\ninv_Sigma_beta <- kronecker((D-rho_beta*W), solve(Lambda))\n\n# Draw from beta ~ MCAR(rho_beta, Lambda)\n# beta array is 3-dim - left to right: p, q, spatial index\n# note: the beta_s vectors are drawn in joint vectorized form.\n#       first, split the joint vector according to spatial index\n#       second, collapse the (p*q x 1) vectors into (p x q) matrices\n\n\nbeta_mat <- t(matrix(backsolve(chol(inv_Sigma_beta), rnorm(n*p*q)), byrow=T, ncol=p*q))\n# For now assume zero mean.\n# beta_mat[1,] <- 0\n# beta_mat[2,] <- 0\n\n# construct array version of beta\nbeta_arr <- array(NA, dim = c(p,q,n))\nbeta_arr[1:p,1:q,] <- beta_mat\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndisplay_ls <- vector(mode = \"list\", length = (p*q))\n\nidx <- 1\nfor (i in 1:p){\n  for (j in 1:q){\n    temp <- spat_domain\n    temp$beta <- beta_arr[i,j,]\n    temp$p <- rep(i, n)\n    temp$q <- rep(j, n)\n    display_ls[[idx]] <- temp\n    idx <- idx + 1\n  }\n}\n\ndisplay_df <- do.call(rbind, display_ls)\n\nmy_plot <- function(data) {\n  \n  ggplot(data) +\n  geom_tile(aes(x, y, fill=beta)) +\n  scale_y_reverse() +\n  scale_fill_gradientn(colors = viridis(10), name = bquote(beta[.(unique(data$q)) * \",\" * .(unique(data$p))])) +\n  coord_fixed() + \n  theme_void()\n  \n}\n\ndo.call(grid.arrange, \n        args=list(grobs=by(display_df, list(display_df$p, display_df$q), my_plot), \n                  ncol=q,\n                  top=\"\",\n                  as.table = F))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nNext construct a covariate matrix, $\\boldsymbol{Z}_{s,t}$ for the simulation. In this example we include an intercept column of $1$s and a centered sequence $1$ to $k$ and hold this constant across space. We could experiment with spatially varying covariate data at a later time. Also in this next block we sample the response vectors, $\\boldsymbol\\delta_{s,t}$.\n\n::: {.cell}\n\n```{.r .cell-code}\n# sample covariates\n# For now, the only covariate is time in years, centered and scaled.\n# Consider changing in the future.\n# Z array is 3 dim - left to right: time, covariate vector length, spatial index\nZ <- array(NA, dim=c(k,q,n))\nfor (s in 1:n){\n  #Z[,,s] <- matrix(c(rep(1,k),rnorm((q-1)*k,0,1)), ncol = q)\n  Z[,,s] <- matrix(c(rep(1,k), scale(1:k, scale = F)), ncol = q)\n}\n\n# Rather than sampling covariates from a normal distribution, let's\n# let's just focus on an intercept term and centered equally spaced term.\n\n\n# Now draw delta (naively via loops)\n# delta array is 3-dim - left to right: response vector length, time, space\ndelta <- array(NA, dim = c(p,k,n))\nfor (s in 1:n){\n  for (t in 1:k){\n    delta[,t,s] <- beta_arr[,,s]%*%Z[t,,s] + eps[,t,s]\n  }\n}\n```\n:::\n\n\nPlot first and second elements of $\\boldsymbol\\delta_{s,t}$ across space by year, $t$.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndisplay_ls <- vector(mode = \"list\", length = k)\n\nfor (t in 1:k){\n  temp <- spat_domain\n  temp$eps1 <- eps[1,t,]\n  temp$eps2 <- eps[2,t,]\n  temp$delta1 <- delta[1,t,] \n  temp$delta2 <- delta[2,t,]\n  temp$year <- rep(t, n)\n  display_ls[[t]] <- temp \n}\n\ndisplay_df <- do.call(rbind, display_ls)\n\npanel_names <- as_labeller(function(x) paste('t =', x))\n\nggplot(display_df) +\n            geom_tile(aes(x, y, fill=delta1)) +\n            scale_y_reverse() +\n            scale_fill_gradientn(colors = viridis(10),\n                                 limits = c(min(display_df$delta1),max(display_df$delta1)),\n                                 name = bquote(delta[\"s,t,1\"])) +\n            coord_fixed() + \n            theme_void() + \n            facet_wrap(~year, labeller = panel_names)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(display_df) +\n            geom_tile(aes(x, y, fill=delta2)) +\n            scale_y_reverse() +\n            scale_fill_gradientn(colors = viridis(10),\n                                 limits = c(min(display_df$delta2),max(display_df$delta2)),\n                                 name = bquote(delta[\"s,t,2\"])) +\n            coord_fixed() + \n            theme_void() + \n            facet_wrap(~year, labeller = panel_names)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n:::\n\n\nWe can go back and simulate more complex data sets at a later time with relative ease. For now, let's move on to the MCMC. \n\nThe MCMC will be a combination of Metropolis and Gibbs steps. Gibbs steps using the full conditionals for $\\boldsymbol\\beta_s$ and $\\boldsymbol\\Lambda$ and Metropolis steps for the MCAR propriety parameters $\\rho_\\epsilon$ and $\\rho_\\beta$.\n\nThe sampler is intialized with values that are much different than the values used to generate the simulated data. \n\nThis naive implementation is not particularly computationally efficient nor have we tested how it scales with the dimension of the response vector, number of covariates, or spatial extent. These will be tackled at a later time. All we are doing here is verifying convergence through some cursory checks of trace plots. Formal diagnostics will be examined later as well.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nniters <- 600\nburn   <- 200\n# burn   <- 0.1*niters\n\n# storage\nkeep_beta_mat <- array(0, dim = c(p*q, n, niters))\nkeep_Lambda   <- array(0, dim = c(p*q, p*q, niters))\nkeep_rho_eps  <- rep(0, niters) \nkeep_rho_beta <- rep(0, niters)\n\n# initial values\nmc_beta_mat <- beta_mat\nmc_beta_mat <- matrix(10, nrow = p*q, ncol = n)\nmc_beta_arr <- beta_arr\nmc_beta_arr[1:p,1:q,] <- mc_beta_mat\nmc_Lambda   <- diag(p*q)\nmc_rho_eps  <- 0.8\nmc_rho_beta <- 0.2\nkeep_beta_mat[,,1] <- mc_beta_mat\nkeep_Lambda[,,1] <- mc_Lambda\nkeep_rho_eps[1] <- mc_rho_eps\nkeep_rho_beta[1] <- mc_rho_beta\n\n\n# prior parameters\nnu <- p*q - 1 + 0.1 # come back to check this\nG  <- diag(p*q)\n\n# M-H tuning parameters\nMH_beta <- 0.1\natt_beta <- 0\nacc_beta <- 0\n\nMH_eps  <- 0.1\natt_eps <- 0\nacc_eps <- 0\n```\n:::\n\n\nWork in progress MCMC loop. At the moment both $\\boldsymbol\\beta_s$ and $\\Lambda$ are updated. The MCAR propriety parameters, $\\rho_\\epsilon$ and $\\rho_\\beta$ are held fixed.\n\nSome pre-computes and function definitions.\n\n::: {.cell}\n\n```{.r .cell-code}\n# tic()\n# Omega is constant in time for now. May need to update this to\n# consider different Omegas for each year.\ninv_Omega <- solve(Omega)\ninv_Omega_arr <- array(rep(c(inv_Omega), k), dim = c(p,p,k))\n\n# Pre-compute the first summation term for V in the full conditional for beta.\nsumt_ZT_Omega_inv_Z <- array(0, dim = c(p*q, p*q, n))\nfor (s in 1:n){\n  for (j in 1:k){\n    sumt_ZT_Omega_inv_Z[,,s] <- sumt_ZT_Omega_inv_Z[,,s] +\n                                t(kronecker(t(Z[j,,s]),diag(p)))%*%inv_Omega_arr[,,j]%*%kronecker(t(Z[j,,s]), diag(p))\n  }\n}\n\n# collect the neighbor indices for use within the sampling loop\nneighbor_idx     <- apply(W, MARGIN = 1, function(x) which(x==1))\nneighbor_weights <- lapply(1:n, function(x) W[x, neighbor_idx[[x]]])\n\n\n# Pre-compute the number of neighbors for each pixel.\nw_plus <- rowSums(W)\nD <- diag(w_plus)\n\n# pre-compute residuals for the first iteration.\nBz <- delta\nfor (j in 1:k){\n  for (s in 1:n){\n    Bz[,j,s] <- mc_beta_arr[,,s]%*%Z[j,,s]\n  }\n}\nresid <- delta - Bz\n\n# loglike functions for Metropolis-Hastings steps\nrho_beta_loglike <- function(rho, beta_mat, neighbor_idx, neighbor_weights, w_plus, inv_Lambda){\n  \n  temp1 <- 0\n  temp2 <- 0\n  for (s in 1:n){\n    beta_tilde <- (beta_mat[,neighbor_idx[[s]]]%*%neighbor_weights[[s]])/w_plus[s]\n    temp1 <- temp1 + w_plus[s]*(t(beta_tilde)%*%inv_Lambda%*%beta_tilde) \n    temp2 <- temp2 + w_plus[s]*(t(beta_mat[,s])%*%inv_Lambda%*%beta_tilde)\n  }\n  return(-0.5*((rho^2)*temp1 - 2*rho*temp2))\n  \n}\n\nrho_eps_loglike <- function(rho, delta, Bz, resid, inv_Omega_arr, neighbor_idx, neighbor_weights, w_plus){\n\n  dbz <- delta + Bz\n  \n  temp1 <- 0\n  temp2 <- 0\n  for (s in 1:n){\n    for (t in 1:k){\n      r <- resid[,t,neighbor_idx[[s]]]%*%neighbor_weights[[s]]\n      \n      temp1 <- temp1 + (t(r)%*%inv_Omega_arr[,,t]%*%r)*w_plus[s]\n      temp2 <- temp2 + (t(delta[,t,s]) + t(Bz[,t,s]))%*%inv_Omega_arr[,,t]%*%r\n    }\n  }\n  \n  return(-0.5*((rho^2)*temp1 - 2*rho*temp2))\n}\n```\n:::\n\n\nSampling loop.\n\n::: {.cell hash='index_cache/html/unnamed-chunk-11_565b78819c8f8137bbe5eef08ad6c691'}\n\n```{.r .cell-code}\ntic()\n# profvis({\nfor (iter in 2:niters){\n  \n  # computing residuals needed in full conditional for beta - M.\n  # probably a better way to do this.\n  \n  inv_mc_Lambda <- solve(mc_Lambda)\n  \n  # sample beta_s\n  for (s in 1:n){\n    \n    V <- w_plus[s]*sumt_ZT_Omega_inv_Z[,,s] + w_plus[s]*inv_mc_Lambda\n    \n    # revisit for efficiency\n    temp <- 0\n    for (j in 1:k){\n      temp <- temp + t(kronecker(t(Z[j,,s]),diag(p)))%*%inv_Omega_arr[,,j]%*%(mc_rho_eps*resid[,j,neighbor_idx[[s]]]%*%neighbor_weights[[s]] +w_plus[s]*delta[,j,s])\n       \n    }\n    M <- temp + mc_rho_beta*(inv_mc_Lambda%*%(mc_beta_mat[,neighbor_idx[[s]]]%*%neighbor_weights[[s]]))\n      \n    V_inv <- chol2inv(chol(V))\n    mc_beta_mat[,s]  <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p*q)\n    \n    \n  }\n  mc_beta_arr[1:p,1:q,] <- mc_beta_mat\n  \n  # sample Lambda\n  H <- beta_mat%*%(D - rho_beta*W)%*%t(beta_mat)\n  mc_Lambda <- MCMCpack::riwish(nu + n, G + H)\n  \n  # compute residuals\n  Bz <- delta\n  for (j in 1:k){\n    for (s in 1:n){\n      Bz[,j,s] <- mc_beta_arr[,,s]%*%Z[j,,s]\n    }\n  }\n  resid <- delta - Bz\n  \n  \n  # sample rho_eps\n  att_eps <- att_eps + 1\n  can <- rtnorm(1, mc_rho_eps, MH_eps, a = 0, b = 1)\n  R   <- rho_eps_loglike(can,        delta, Bz, resid, inv_Omega_arr, neighbor_idx, neighbor_weights, w_plus) - # Likelihood\n         rho_eps_loglike(mc_rho_eps, delta, Bz, resid, inv_Omega_arr, neighbor_idx, neighbor_weights, w_plus) +\n         dtnorm(mc_rho_eps, mean = can        , sd = MH_eps, a = 0, b = 1, log = T) -      # M-H adjustment\n         dtnorm(can,        mean = mc_rho_eps , sd = MH_eps, a = 0, b = 1, log = T)\n  if(log(runif(1)) < R){\n    acc_eps <- acc_eps + 1\n    mc_rho_eps  <- can\n  }\n  \n  # sample rho_beta\n  att_beta <- att_beta + 1\n  can <- rtnorm(1, mc_rho_beta, MH_beta, a = 0, b = 1)\n  R   <- rho_beta_loglike(can,         mc_beta_mat, neighbor_idx, neighbor_weights, w_plus, inv_mc_Lambda) - # Likelihood\n         rho_beta_loglike(mc_rho_beta, mc_beta_mat, neighbor_idx, neighbor_weights, w_plus, inv_mc_Lambda) +\n         dtnorm(mc_rho_beta, mean = can         , sd = MH_beta, a = 0, b = 1, log = T) -      # M-H adjustment\n         dtnorm(can,         mean = mc_rho_beta , sd = MH_beta, a = 0, b = 1, log = T)\n  if(log(runif(1)) < R){\n    acc_beta <- acc_beta + 1\n    mc_rho_beta  <- can\n  }\n\n  # tuning\n  if(iter < burn){\n    if(att_eps > 25){\n      if(acc_eps/att_eps < 0.3){MH_eps <- MH_eps*0.8}\n      if(acc_eps/att_eps > 0.6){MH_eps <- MH_eps*1.2}\n      acc_eps <- att_eps <- 0\n    }\n  }\n  \n  if(iter < burn){\n    if(att_beta > 50){\n      if(acc_beta/att_beta < 0.3){MH_beta <- MH_beta*0.8}\n      if(acc_beta/att_beta > 0.6){MH_beta <- MH_beta*1.2}\n      acc_beta <- att_beta <- 0\n    }\n  }\n  \n  \n  keep_beta_mat[,,iter] <- mc_beta_mat \n  keep_Lambda[,,iter]   <- mc_Lambda\n  keep_rho_eps[iter]    <- mc_rho_eps\n  keep_rho_beta[iter]   <- mc_rho_beta\n}\n# })\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n534.267 sec elapsed\n```\n:::\n:::\n\n\nAt this point in the model fit we evaluate some trace plots of the parameters $\\boldsymbol\\beta_{s}$, $\\boldsymbol\\Lambda$, $\\rho_\\beta$, and $\\rho_\\varepsilon$.\n\nFirst, we'll look at the entire trace plot a single location without \n\n\n::: {.cell}\n\n```{.r .cell-code}\ns <- 1\nburn <- 1\nbeta_subscripts <- c(\"1,1\", \"1,2\", \"2,1\", \"2,2\")\n\npar(mfcol = c(2,2))\nfor (i in 1:(p*q)){\n  tsub <- beta_subscripts[i]\n  plot(burn:niters, \n       keep_beta_mat[i,s,burn:niters], \n       type = \"l\",\n       xlab = \"iteration\",\n       ylab = bquote(beta[.(tsub)]),\n       ylim = c(min(min(keep_beta_mat[i,s,burn:niters]),beta_mat[i,s]), max(max(keep_beta_mat[i,s,burn:niters]),beta_mat[i,s])))\n  abline(h = beta_mat[i,s], col = \"red\")\n}\nmtext(paste(\"Pixel: \",s), side = 3, line = -3, outer = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\nIt's clear it takes the model a few iterations to converge. How do the trace plots without the burn-in iterations look? We sample a handful of locations and constuct trace plots of $\\boldsymbol\\beta_{s}$ without burn-in.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nburn <- 50\n\nlocation_sample <- sample(1:n, size = 5)\n\nbeta_subscripts <- c(\"1,1\", \"1,2\", \"2,1\", \"2,2\")\n\nfor (s in location_sample){\n  par(mfcol = c(2,2))\n  for (i in 1:(p*q)){\n    tsub <- beta_subscripts[i]\n    plot(burn:niters, \n         keep_beta_mat[i,s,burn:niters], \n         type = \"l\",\n         xlab = \"iteration\",\n         ylab = bquote(beta[.(tsub)]),\n         ylim = c(min(min(keep_beta_mat[i,s,burn:niters]),beta_mat[i,s]), max(max(keep_beta_mat[i,s,burn:niters]),beta_mat[i,s])))\n    abline(h = beta_mat[i,s], col = \"red\")\n  }\n  mtext(paste(\"Pixel: \",s), side = 3, line = -3, outer = TRUE)\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-5.png){width=672}\n:::\n:::\n\n\nConstruct trace plots for $\\boldsymbol\\Lambda$ (upper triangle).\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nburn <- 1\npar(mfrow = c(2,2))\n\n# off-diagonal\nfor (i in 1:(p*q)){\n  for (j in i:(p*q)){\n    tsub <- paste0(i,j)\n    plot(burn:niters, \n         keep_Lambda[i,j,burn:niters], \n         type = \"l\",\n         xlab = \"iteration\",\n         ylab = bquote(Lambda[.(tsub)]))\n    abline(h = Lambda[i,j], col = \"red\")\n  }\n  mtext(bquote(Lambda ~ \"Matrix\"), side = 3, line = -3, outer = TRUE)\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-3.png){width=672}\n:::\n:::\n\nTrace plots for $\\rho_\\beta$ and $\\rho_\\varepsilon$.\n\n::: {.cell}\n\n```{.r .cell-code}\nburn <- 1\npar(mfrow=c(1,2))\n\nplot(burn:niters, \n         keep_rho_beta[burn:niters], \n         type = \"l\",\n         xlab = \"iteration\",\n         ylab = bquote(rho[beta]))\n    abline(h = rho_beta, col = \"red\")\n\nplot(burn:niters, \n         keep_rho_eps[burn:niters], \n         type = \"l\",\n         xlab = \"iteration\",\n         ylab = bquote(rho[\"\\u03F5\"]))\n    abline(h = rho_eps, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n<!-- Let's do a quick visual check of the estimates $\\widehat{\\boldsymbol\\beta}_{s}$ vs $\\boldsymbol\\beta_{s}$. We will plot $\\widehat{\\boldsymbol\\beta}_{s} - \\boldsymbol\\beta_{s}$ across the spatial domain. -->\n\n<!-- ```{r} -->\n\n<!-- elem <- 1 -->\n\n<!-- hat_beta_mat <- rowMeans(keep_beta_mat[,,burn:niters], dims = 2) -->\n\n<!-- ggplot(spat_domain) + -->\n<!--   geom_tile(aes(x, y, fill=beta_mat[elem,])) + -->\n<!--   scale_y_reverse() + -->\n<!--   scale_fill_gradientn(colors = viridis(10)) + -->\n<!--   coord_fixed() +  -->\n<!--   theme_void() -->\n\n<!-- ggplot(spat_domain) + -->\n<!--   geom_tile(aes(x, y, fill=hat_beta_mat[elem,])) + -->\n<!--   scale_y_reverse() + -->\n<!--   scale_fill_gradientn(colors = viridis(10)) + -->\n<!--   coord_fixed() +  -->\n<!--   theme_void() -->\n\n<!-- ggplot(spat_domain) + -->\n<!--   geom_tile(aes(x, y, fill=hat_beta_mat[elem,]-beta_mat[elem,])) + -->\n<!--   scale_y_reverse() + -->\n<!--   scale_fill_gradientn(colors = viridis(10), limits = c(min(beta_mat[elem,]),max(beta_mat[elem,]))) + -->\n<!--   coord_fixed() +  -->\n<!--   theme_void() -->\n\n<!-- ``` -->\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}