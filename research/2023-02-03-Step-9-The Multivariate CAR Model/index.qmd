---
title: "Step 9 - The multivariate CAR model"
description: "We explore extensions of the univariate CAR model described in Hierarchical Modeling and Analysis for Spatial data."
author:
  - name: Matthew Shisler
    affiliation: North Carloina State University - Department of Statistics
    affiliation-url: https://statistics.sciences.ncsu.edu/ 
categories: [Bayesian, MCMC, Spatial, CAR] # self-defined categories
draft: false 
format:
  html: 
    code-fold: true
execute: 
  cache: false
  freeze: auto
---


```{r}
#| label: load-packages
#| output: false
#| code-summary: "Code: Load the packages"

library(tidyverse)
library(igraph)
library(viridis)
```

## Intro

This is another test. This is a third test.

Here we will extend the univariate CAR model to the multivariate case. 

Consider a spatial domain $\mathcal{D} \in \mathbb{R}^2$ that is partitioned into $n$ areal units. In the univariate case we specified a spatial random effect $\boldsymbol\phi = (\phi_1,\dots,\phi_n)$ meant to capture the spatial dependence. In the multivariate case, say of dimension $p$, we specify a spatial random vector $\boldsymbol\phi^T = (\boldsymbol\phi_1, \boldsymbol\phi_2,\dots,\boldsymbol\phi_3)$ which is $np \times 1$ and each $\boldsymbol\phi_i = (\phi_{i1}, \phi_{i2},\dots, \phi_{ip})$ is $p \times 1$.

The joint distribution of $\boldsymbol\phi$ is derived from the full conditional distributions of $\boldsymbol\phi_i$.
Under the Markov Random Field (MRF) assumption, the conditional distributions of $\boldsymbol\phi_i$ can be specified as
$$
p(\boldsymbol\phi_i|\boldsymbol\phi_{j\ne i}, \Gamma_i) = N\left(\sum_{i \sim j} \mathbf{B}_{ij}\boldsymbol\phi_j, \Gamma_i\right), \quad i,j = 1,\dots,n.
$$

where $\Gamma_i$ and $\mathbf{B}_{ij}$ are $p \times p$ matrices. Here we $i \tilde j$ to say that $j$ is in the neighborhood of $i$, i.e. $j \in \mathcal{N}(i)$.

The role of $\Gamma_i$ and $\mathbf{B}_{ij}$ are analogous to the roles of $\tau_i^2$ and $b_{ij}$, respectively, in the univariate CAR models. The matrix $\Gamma_i$ is the within unit covariance matrix which describes the dependence of the variables in the vector $\boldsymbol\phi_i$. The matrix $\mathbf{B}_{ij}$ is the matrix that allows us to weight the neighboring observations of location $i$ that are conditioned on. A convenient special case is to set $\mathbf{B}_{ij} = b_{ij}\mathbf{I}_p$ where $b_{ij} = w_{ij}/w_{i+}$. Recall, for a neighborhood matrix $\mathbf{W} = \{w_{ij}\}$,
$$
w_{ij} = 
\begin{cases}
1 \quad \text{if} \quad j \in \mathcal{N}(i),\\
0 \quad \text{otherwise}.
\end{cases}
$$
and $w_{i+} = \sum_{j=1}^pw_{ij}$, i.e. the number of neighbors of location $i$, $|\mathcal{N}(i)|$.


Specifying the conditional distributions in this way implies the unique joint distribution, via Brook's Lemma, to be
$$
p(\boldsymbol\phi \;| \;\{\Gamma_i\}) \propto \exp\left\{-\frac{1}{2}\boldsymbol\phi^T\Gamma^{-1}(\mathbf{I}_{np} - \tilde{\mathbf{B}})\phi\right\}
$$
where $\Gamma$ is block-diagonal with block $\Gamma_i$, and $\tilde{\mathbf{B}}$ is $np \times np$ with the $(i,j)$-th block $\mathbf{B}_{ij}$.

Of course, a requirement for the multivariate normal distribution is symmetry of $\Gamma^{-1}(\mathbf{I}_{np} - \tilde{\mathbf{B}})$. Setting $\mathbf{B}_{ij} = b_{ij}\mathbf{I}_p$ where $b_{ij} = w_{ij}/w_{i+}$ leads to the symmetry condition $b_{ij}\Gamma_j = b_{ji}\Gamma_i$. Further, a common simplifying assumption would be consider the within unit covariance equal. That is, set $\Gamma_i = w_{i+}^{-1}\Sigma$ where $\Sigma$ is the $p \times p$ within unit covariance matrix for $\boldsymbol\phi_i$ and common across spatial locations $i=1,\dots,n$.

Under these assumptions we can write $\Gamma = (\mathbf{D}^{-1}\otimes\Sigma)$ where $\mathbf{D}$ is an $n \times n$ diagonal matrix with $\mathbf{D}_{ii} = w_{i+}$. And also $\tilde{\mathbf{B}} = \mathbf{B}\otimes\mathbf{I}_p$ where $\mathbf{B}$ is a $n \times n$ matrix and elements of $\mathbf{B}$ are $\mathbf{B} = \{b_{ij}\} = \{w_{ij}/w_{i+}\}$, not to be confused with the block matrix specification $\mathbf{B}_{ij}$ given previously. 

With this in mind, we can write the precision of the above joint distribution in an alternate form,

\begin{align*}

\Gamma^{-1}(\mathbf{I}_{np} - \tilde{\mathbf{B}}) &= (\mathbf{D}^{-1} \otimes \Sigma)^{-1}(\mathbf{I}_{np} - \mathbf{B}\otimes \mathbf{I}_p)\\
&= (\mathbf{D} \otimes \Sigma^{-1})(\mathbf{I}_{np} - \mathbf{B}\otimes \mathbf{I}_p) & \text{Kronecker Prod - Inverse}\\
&= (\mathbf{D} \otimes \Sigma^{-1}) - (\mathbf{D} \otimes \Sigma^{-1})( \mathbf{B}\otimes \mathbf{I}_p) & \text{(Matrix Mult - distributive wrt matrix addition)}\\
&= (\mathbf{D} \otimes \Sigma^{-1}) - (\mathbf{D}\mathbf{B}) \otimes (\Sigma^{-1}\mathbf{I}_p) & \text{(Kronecker Prod - mixed-product property)}\\
&= (\mathbf{D} - \mathbf{D}\mathbf{B})\otimes\Sigma^{-1} & \text{(Kronecker Prod - distributive wrt matrix addition)}\\
&= (\mathbf{D} - \mathbf{W}) \otimes \Sigma^{-1}
\end{align*}

where the last step follows from the fact that $\mathbf{B} = \mathbf{D}^{-1}\mathbf{W}$.

Now there is no need to construct a large block-diagonal $\Gamma$ matrix. Also, the neighborhood matrix $\mathbf{W}$ and $\mathbf{D}$, the diagonal matrix of its row sums, are fixed and known quantities easily computed outside of any MCMC sampling loop. The joint distribution is then

$$
p(\boldsymbol\phi \;| \Sigma) \propto \exp\left\{-\frac{1}{2}\boldsymbol\phi^T\left((\mathbf{D} - \mathbf{W})\otimes \Sigma^{-1}\right)\phi\right\}
$$
However, we again encounter the issue that $(\mathbf{D} - \mathbf{W}) \otimes \Sigma^{-1}$ is singular, since $(\mathbf{D} - \mathbf{W})$ is singular. This motivates the introduction of a scalar parameter, $\rho$, also analogous to the univariate case. There are in fact more general conditions for recovering positive definiteness, but these are not explored here. More details can be found in. . .

Introducing $\rho$ results in a proper joint distribution with form

$$
p(\boldsymbol\phi \;| \Sigma) \propto \exp\left\{-\frac{1}{2}\boldsymbol\phi^T\left((\mathbf{D} - \rho\mathbf{W})\otimes \Sigma^{-1}\right)\boldsymbol\phi\right\}
$$
Even so, a model which assumes a shared $\rho$ parameter across elements of $\boldsymbol\phi_i$ may be too restrictive by assuming that the "strength of spatial dependence" is the same for each variable in the multivariate response. Further models have been developed that allow for different $\rho_k$ for variables $k=1,\dots,p$. 

This approach requires a re-arrangement of $\boldsymbol\phi$. Where previously $\boldsymbol\phi$ stacked $n$ vectors of length $p$ on top of each other, now consider $\boldsymbol\phi'$ to stack $p$ vectors of length $n$. That is we collect the $n$ instances of the $p$-th element of $\boldsymbol\phi_i$, $i=1,\dots,n$. This operation is essentially transforming the vectorization of the $n \times p$ matrix representation of $\boldsymbol\phi$ into the vectorization of its transpose and is accomplished via a *commutation* matrix.

Some things to think about. Viewing $\boldsymbol\phi$ as a vectoratization of a matrix $\Phi$. Then we can commute $\boldsymbol\phi = \text{vec}(\Phi)$ by applying the commutation matrix $\mathbf{K}^{(p,n)}$ for the result $\mathbf{K}^{(p,n)}\text{vec}(\Phi) = \text{vec}(\Phi^T)$. Given that

$$
\text{vec}(\Phi) = \boldsymbol\phi \sim N\left(\boldsymbol 0, (\mathbf{D}-\rho\mathbf{W})^{-1} \otimes \Lambda\right)
$$

then the result of left multiplying by the commutation matrix yields the distribution,
$$
\begin{align*}
\mathbf{K}^{(p,n)}\text{vec}(\Phi) = \text{vec}(\Phi^T) &\sim N\left(\boldsymbol 0, \mathbf{K}^{(p,n)}\left[(\mathbf{D}-\rho\mathbf{W})^{-1} \otimes \Lambda\right]\left(\mathbf{K}^{(p,n)}\right)^T\right)\\
&\sim N\left(\boldsymbol 0, \mathbf{K}^{(p,n)}\left[(\mathbf{D}-\rho\mathbf{W})^{-1} \otimes \Lambda\right]\mathbf{K}^{(n,p)}\right)\\
&\sim N\left(\boldsymbol 0, \left[\Lambda \otimes(\mathbf{D}-\rho\mathbf{W})^{-1}\right]\right) 
\end{align*}
$$

The effect is that the Kronecker product is commuted. When the resulting Kronecker product is expanded, the elements of the common non-spatial covariance matrix $\Lambda$ are multiplied with $(\mathbf{D}-\rho\mathbf{W})$ individually as scalars. It is in this form that we can introduce different $\rho_k$ for each of the $p$ elements. That is, different strengths of spatial correlation for each variate in the multivariate response vector.

This allows us to "break up" elements of $\Sigma$ and associate them with difference values of $\rho_k$.



Let's turn to a simple example. Consider a spatial domain partitioned into $n=4$ areal units and a multivariate response with $p=2$.


First, the spatial domain.

```{r}
n <- 4
spat_domain <- expand.grid(x = 1:sqrt(n), y = 1:sqrt(n))
spat_domain$label <- 1:n

```

```{r}
ggplot(spat_domain) +
  geom_tile(aes(x, y), linewidth = 2, color = "grey50", fill="white") +
  geom_text(aes(x, y, label=label), size = 15) +
  coord_fixed() + 
  theme_void()
```

Next, the adjacency matrix $\mathbf{W}$.

```{r}
spat_domain_g <- make_lattice(c(sqrt(n),sqrt(n)), mutual = TRUE)
W <- as.matrix(as_adjacency_matrix(spat_domain_g, sparse=1))
W
```

```{r}
B <- W/rowSums(W)
B
Btilde <- kronecker(B, diag(2))
```

```{r}
D <- diag(rowSums(W))

solve(D)
p <- 2
capSigma <- matrix(c(1,0.7,
                     0.7,1), byrow = T, ncol=p)

capLambda <- kronecker(solve(D), capSigma)
```

```{r}
A1 <- solve(capLambda)%*%(diag(p*n) - Btilde)
A2 <- kronecker((D - W), solve(capSigma))

all(A1 == A2)
```



